{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cWACPRL869I4"
   },
   "outputs": [],
   "source": [
    "# C0797202 JAY PANCHAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.60.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from memory_profiler) (5.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym) (0.0.7)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym) (1.23.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JSAnimation in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (20.4)\n",
      "Requirement already satisfied: six in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[accept-rom-license,atari] in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[accept-rom-license,atari]) (1.23.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[accept-rom-license,atari]) (2.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.24.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jay\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.25.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
      "UsageError: Line magic function `%%file` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install memory_profiler\n",
    "%load_ext memory_profiler\n",
    "%pip install gym\n",
    "%pip install JSAnimation\n",
    "%pip install matplotlib\n",
    "%pip install -U gym >= 0.21.0\n",
    "%pip install -U gym[atari,accept-rom-license]\n",
    "\n",
    "%%file training.py\n",
    "\n",
    "%%python -m memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wotUOa_e6edP"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "from matplotlib import animation\n",
    "\n",
    "from memory_profiler import profile\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R66_INeZ9nYX"
   },
   "source": [
    "## Initiating Ping Pong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtT2GyK_6edc",
    "outputId": "6ef17a84-3563-4157-caf3-2c50438190ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Importing gym library and creating environment\n",
    "import gym\n",
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRE6WmXQJ1Z0",
    "outputId": "41a4b484-4f7e-4fd6-fc69-e626adec0523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trwRXI-h6eeI",
    "outputId": "42368b9e-2477-41ef-fb42-30ced75282c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished without success, accumulated reward = -13.0\n"
     ]
    }
   ],
   "source": [
    "# Run a demo of the environment\n",
    "observation = env.reset()\n",
    "cumulated_reward = 0\n",
    "\n",
    "frames = []\n",
    "for t in range(1000):\n",
    "#     print(observation)\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    # very stupid agent, just makes a random action within the allowd action space\n",
    "    action = env.action_space.sample()\n",
    "#     print(\"Action: {}\".format(t+1))    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "#     print(reward)\n",
    "    cumulated_reward += reward\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
    "        break\n",
    "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3zZTecVWLLes"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def model_step(model, observation, prev_x):\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "  \n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, _ = policy_forward(x)\n",
    "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
    "  \n",
    "  return action, prev_x\n",
    "\n",
    "def play_game(env, model):\n",
    "  observation = env.reset()\n",
    "\n",
    "  frames = []\n",
    "  cumulated_reward = 0\n",
    "\n",
    "  prev_x = None # used in computing the difference frame\n",
    "\n",
    "  for t in range(1000):\n",
    "      frames.append(env.render(mode = 'rgb_array'))\n",
    "      action, prev_x = model_step(model, observation, prev_x)\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      cumulated_reward += reward\n",
    "      if done:\n",
    "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
    "          break\n",
    "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
    "  display_frames_as_gif(frames)\n",
    "  env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gWvZQ7AQLQt"
   },
   "source": [
    "# Logic behind the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eqFm7hqcItWl"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Init the model\n",
    "\n",
    "# number of neurons\n",
    "H = 200 \n",
    "\n",
    "# input dimensionality: 80x80 grid\n",
    "D = 80 * 80 \n",
    "\n",
    "model = {}\n",
    "def update_neurons(neurons = 200):\n",
    "    H = neurons\n",
    "    \n",
    "    # \"Xavier\" initialization\n",
    "    \n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D)\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "update_neurons(H)\n",
    "\n",
    "# import pickle\n",
    "#  model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TwjiwKisQM19"
   },
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "\n",
    "# learning_rate = 1e-4\n",
    "learning_rate = 1e-3\n",
    " \n",
    "# discount factor for reward\n",
    "gamma = 0.99 \n",
    "\n",
    "# decay factor for RMSProp leaky sum of grad^2\n",
    "decay_rate = 0.99 \n",
    "  \n",
    "# update buffers that add up gradients over a batch\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } \n",
    "\n",
    "# rmsprop memory\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } \n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_backward(epx, eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "\n",
    "def train_model(env, model, total_episodes = 100):\n",
    "  hist = []\n",
    "  hist_2 = []\n",
    "  observation = env.reset()\n",
    "\n",
    "  prev_x = None # used in computing the difference frame\n",
    "  xs,hs,dlogps,drs = [],[],[],[]\n",
    "  running_reward = None\n",
    "  reward_sum = 0\n",
    "  episode_number = 0\n",
    "\n",
    "  from datetime import datetime\n",
    "\n",
    "  now = datetime.now()\n",
    "\n",
    "  print(f'Start time: {now}')\n",
    "\n",
    "  last_export_time = now\n",
    "\n",
    "  while True:\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "      episode_number += 1\n",
    "\n",
    "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "      epx = np.vstack(xs)\n",
    "      eph = np.vstack(hs)\n",
    "      epdlogp = np.vstack(dlogps)\n",
    "      epr = np.vstack(drs)\n",
    "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "      # compute the discounted reward backwards through time\n",
    "      discounted_epr = discount_rewards(epr)\n",
    "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "      discounted_epr -= np.mean(discounted_epr)\n",
    "      discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "      grad = policy_backward(epx, eph, epdlogp)\n",
    "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "      # perform rmsprop parameter update every batch_size episodes\n",
    "      if episode_number % batch_size == 0:\n",
    "        for k,v in model.items():\n",
    "          g = grad_buffer[k] # gradient\n",
    "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "      # boring book-keeping\n",
    "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "      hist.append((episode_number, reward_sum, running_reward,datetime.now()))\n",
    "      hist_2.append((episode_number, running_reward))\n",
    "      \n",
    "      if ((datetime.now() - last_export_time).total_seconds() > 30):\n",
    "        file_name = 'hist1_'+ str(total_episodes) + '_1e3.csv'\n",
    "        np.savetxt(file_name, hist, delimiter =\",\", fmt ='% s')\n",
    "        last_export_time = datetime.now()\n",
    "        \n",
    "      print (f'resetting env. episode {episode_number}, reward total was {reward_sum}. running mean: {running_reward}, timestamp: {datetime.now()}')\n",
    "            \n",
    "      reward_sum = 0\n",
    "      observation = env.reset() # reset env\n",
    "      prev_x = None\n",
    "      if episode_number == total_episodes:\n",
    "        return hist, hist_2\n",
    "\n",
    "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6Ka_5Vl9Orm",
    "outputId": "75de5744-f8e4-4aea-c00f-2cb2977a38cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2022-08-20 08:46:02.638223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\AppData\\Local\\Temp\\ipykernel_27432\\1818632268.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return I.astype(np.float).ravel()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 1, reward total was -21.0. running mean: -21.0, timestamp: 2022-08-20 08:46:07.539125\n",
      "resetting env. episode 2, reward total was -21.0. running mean: -21.0, timestamp: 2022-08-20 08:46:13.215948\n",
      "resetting env. episode 3, reward total was -21.0. running mean: -21.0, timestamp: 2022-08-20 08:46:17.140461\n",
      "resetting env. episode 4, reward total was -21.0. running mean: -21.0, timestamp: 2022-08-20 08:46:21.288374\n",
      "resetting env. episode 5, reward total was -21.0. running mean: -21.0, timestamp: 2022-08-20 08:46:24.865808\n",
      "resetting env. episode 6, reward total was -20.0. running mean: -20.99, timestamp: 2022-08-20 08:46:29.217178\n",
      "resetting env. episode 7, reward total was -20.0. running mean: -20.980099999999997, timestamp: 2022-08-20 08:46:33.608439\n",
      "resetting env. episode 8, reward total was -20.0. running mean: -20.970298999999997, timestamp: 2022-08-20 08:46:38.405616\n",
      "resetting env. episode 9, reward total was -20.0. running mean: -20.960596009999996, timestamp: 2022-08-20 08:46:42.477733\n",
      "resetting env. episode 10, reward total was -19.0. running mean: -20.940990049899998, timestamp: 2022-08-20 08:46:47.387620\n",
      "resetting env. episode 11, reward total was -21.0. running mean: -20.941580149401, timestamp: 2022-08-20 08:46:50.973024\n",
      "resetting env. episode 12, reward total was -20.0. running mean: -20.932164347906987, timestamp: 2022-08-20 08:46:55.177783\n",
      "resetting env. episode 13, reward total was -21.0. running mean: -20.93284270442792, timestamp: 2022-08-20 08:46:58.757232\n",
      "resetting env. episode 14, reward total was -21.0. running mean: -20.93351427738364, timestamp: 2022-08-20 08:47:01.956665\n",
      "resetting env. episode 15, reward total was -20.0. running mean: -20.9241791346098, timestamp: 2022-08-20 08:47:06.623191\n",
      "resetting env. episode 16, reward total was -19.0. running mean: -20.904937343263704, timestamp: 2022-08-20 08:47:11.589916\n",
      "resetting env. episode 17, reward total was -21.0. running mean: -20.905887969831067, timestamp: 2022-08-20 08:47:14.893086\n",
      "resetting env. episode 18, reward total was -20.0. running mean: -20.896829090132755, timestamp: 2022-08-20 08:47:18.903367\n",
      "resetting env. episode 19, reward total was -21.0. running mean: -20.897860799231427, timestamp: 2022-08-20 08:47:23.326542\n",
      "resetting env. episode 20, reward total was -21.0. running mean: -20.898882191239114, timestamp: 2022-08-20 08:47:26.947865\n",
      "resetting env. episode 21, reward total was -21.0. running mean: -20.899893369326723, timestamp: 2022-08-20 08:47:31.548566\n",
      "resetting env. episode 22, reward total was -20.0. running mean: -20.890894435633456, timestamp: 2022-08-20 08:47:35.061177\n",
      "resetting env. episode 23, reward total was -21.0. running mean: -20.891985491277122, timestamp: 2022-08-20 08:47:38.642603\n",
      "resetting env. episode 24, reward total was -20.0. running mean: -20.88306563636435, timestamp: 2022-08-20 08:47:42.744638\n",
      "resetting env. episode 25, reward total was -21.0. running mean: -20.884234980000706, timestamp: 2022-08-20 08:47:46.529521\n",
      "resetting env. episode 26, reward total was -21.0. running mean: -20.8853926302007, timestamp: 2022-08-20 08:47:51.179097\n",
      "resetting env. episode 27, reward total was -20.0. running mean: -20.87653870389869, timestamp: 2022-08-20 08:47:56.317365\n",
      "resetting env. episode 28, reward total was -21.0. running mean: -20.877773316859702, timestamp: 2022-08-20 08:48:00.706626\n",
      "resetting env. episode 29, reward total was -21.0. running mean: -20.878995583691108, timestamp: 2022-08-20 08:48:03.974891\n",
      "resetting env. episode 30, reward total was -18.0. running mean: -20.850205627854198, timestamp: 2022-08-20 08:48:09.087257\n",
      "resetting env. episode 31, reward total was -20.0. running mean: -20.841703571575653, timestamp: 2022-08-20 08:48:13.857485\n",
      "resetting env. episode 32, reward total was -20.0. running mean: -20.833286535859894, timestamp: 2022-08-20 08:48:17.480799\n",
      "resetting env. episode 33, reward total was -21.0. running mean: -20.834953670501296, timestamp: 2022-08-20 08:48:20.522659\n",
      "resetting env. episode 34, reward total was -19.0. running mean: -20.816604133796282, timestamp: 2022-08-20 08:48:25.499358\n",
      "resetting env. episode 35, reward total was -18.0. running mean: -20.788438092458318, timestamp: 2022-08-20 08:48:30.871993\n",
      "resetting env. episode 36, reward total was -19.0. running mean: -20.770553711533736, timestamp: 2022-08-20 08:48:35.061798\n",
      "resetting env. episode 37, reward total was -21.0. running mean: -20.7728481744184, timestamp: 2022-08-20 08:48:40.285839\n",
      "resetting env. episode 38, reward total was -20.0. running mean: -20.765119692674215, timestamp: 2022-08-20 08:48:43.937075\n",
      "resetting env. episode 39, reward total was -21.0. running mean: -20.767468495747472, timestamp: 2022-08-20 08:48:47.568368\n",
      "resetting env. episode 40, reward total was -21.0. running mean: -20.76979381079, timestamp: 2022-08-20 08:48:50.961297\n",
      "resetting env. episode 41, reward total was -20.0. running mean: -20.7620958726821, timestamp: 2022-08-20 08:48:54.931685\n",
      "resetting env. episode 42, reward total was -21.0. running mean: -20.76447491395528, timestamp: 2022-08-20 08:48:58.600875\n",
      "resetting env. episode 43, reward total was -21.0. running mean: -20.766830164815726, timestamp: 2022-08-20 08:49:02.404721\n",
      "resetting env. episode 44, reward total was -19.0. running mean: -20.74916186316757, timestamp: 2022-08-20 08:49:07.598823\n",
      "resetting env. episode 45, reward total was -21.0. running mean: -20.751670244535894, timestamp: 2022-08-20 08:49:11.569221\n",
      "resetting env. episode 46, reward total was -20.0. running mean: -20.744153542090533, timestamp: 2022-08-20 08:49:15.540602\n",
      "resetting env. episode 47, reward total was -21.0. running mean: -20.74671200666963, timestamp: 2022-08-20 08:49:18.591443\n",
      "resetting env. episode 48, reward total was -21.0. running mean: -20.749244886602934, timestamp: 2022-08-20 08:49:23.239017\n",
      "resetting env. episode 49, reward total was -21.0. running mean: -20.751752437736904, timestamp: 2022-08-20 08:49:26.234034\n",
      "resetting env. episode 50, reward total was -21.0. running mean: -20.754234913359536, timestamp: 2022-08-20 08:49:29.510253\n",
      "resetting env. episode 51, reward total was -21.0. running mean: -20.75669256422594, timestamp: 2022-08-20 08:49:32.949062\n",
      "resetting env. episode 52, reward total was -20.0. running mean: -20.74912563858368, timestamp: 2022-08-20 08:49:36.437738\n",
      "resetting env. episode 53, reward total was -21.0. running mean: -20.751634382197846, timestamp: 2022-08-20 08:49:40.172755\n",
      "resetting env. episode 54, reward total was -21.0. running mean: -20.754118038375868, timestamp: 2022-08-20 08:49:43.452984\n",
      "resetting env. episode 55, reward total was -18.0. running mean: -20.726576857992107, timestamp: 2022-08-20 08:49:49.832935\n",
      "resetting env. episode 56, reward total was -21.0. running mean: -20.729311089412185, timestamp: 2022-08-20 08:49:53.746470\n",
      "resetting env. episode 57, reward total was -21.0. running mean: -20.732017978518066, timestamp: 2022-08-20 08:49:58.113809\n",
      "resetting env. episode 58, reward total was -21.0. running mean: -20.734697798732885, timestamp: 2022-08-20 08:50:01.838841\n",
      "resetting env. episode 59, reward total was -21.0. running mean: -20.737350820745558, timestamp: 2022-08-20 08:50:05.427248\n",
      "resetting env. episode 60, reward total was -21.0. running mean: -20.7399773125381, timestamp: 2022-08-20 08:50:08.758345\n",
      "resetting env. episode 61, reward total was -21.0. running mean: -20.74257753941272, timestamp: 2022-08-20 08:50:11.861050\n",
      "resetting env. episode 62, reward total was -21.0. running mean: -20.745151764018594, timestamp: 2022-08-20 08:50:15.210103\n",
      "resetting env. episode 63, reward total was -20.0. running mean: -20.737700246378406, timestamp: 2022-08-20 08:50:18.382632\n",
      "resetting env. episode 64, reward total was -21.0. running mean: -20.740323243914624, timestamp: 2022-08-20 08:50:20.691454\n",
      "resetting env. episode 65, reward total was -21.0. running mean: -20.742920011475476, timestamp: 2022-08-20 08:50:24.016559\n",
      "resetting env. episode 66, reward total was -20.0. running mean: -20.73549081136072, timestamp: 2022-08-20 08:50:27.432426\n",
      "resetting env. episode 67, reward total was -20.0. running mean: -20.72813590324711, timestamp: 2022-08-20 08:50:31.551417\n",
      "resetting env. episode 68, reward total was -20.0. running mean: -20.72085454421464, timestamp: 2022-08-20 08:50:36.615880\n",
      "resetting env. episode 69, reward total was -20.0. running mean: -20.713645998772492, timestamp: 2022-08-20 08:50:39.855236\n",
      "resetting env. episode 70, reward total was -21.0. running mean: -20.716509538784766, timestamp: 2022-08-20 08:50:43.578270\n",
      "resetting env. episode 71, reward total was -21.0. running mean: -20.71934444339692, timestamp: 2022-08-20 08:50:46.546336\n",
      "resetting env. episode 72, reward total was -21.0. running mean: -20.722150998962952, timestamp: 2022-08-20 08:50:49.309962\n",
      "resetting env. episode 73, reward total was -20.0. running mean: -20.714929488973322, timestamp: 2022-08-20 08:50:52.736788\n",
      "resetting env. episode 74, reward total was -21.0. running mean: -20.717780194083588, timestamp: 2022-08-20 08:50:56.770028\n",
      "resetting env. episode 75, reward total was -21.0. running mean: -20.72060239214275, timestamp: 2022-08-20 08:51:04.280935\n",
      "resetting env. episode 76, reward total was -20.0. running mean: -20.713396368221325, timestamp: 2022-08-20 08:51:09.065143\n",
      "resetting env. episode 77, reward total was -20.0. running mean: -20.70626240453911, timestamp: 2022-08-20 08:51:13.282889\n",
      "resetting env. episode 78, reward total was -21.0. running mean: -20.70919978049372, timestamp: 2022-08-20 08:51:18.068078\n",
      "resetting env. episode 79, reward total was -20.0. running mean: -20.70210778268878, timestamp: 2022-08-20 08:51:21.757237\n",
      "resetting env. episode 80, reward total was -19.0. running mean: -20.685086704861895, timestamp: 2022-08-20 08:51:25.076373\n",
      "resetting env. episode 81, reward total was -20.0. running mean: -20.678235837813276, timestamp: 2022-08-20 08:51:28.926057\n",
      "resetting env. episode 82, reward total was -21.0. running mean: -20.681453479435145, timestamp: 2022-08-20 08:51:31.991862\n",
      "resetting env. episode 83, reward total was -21.0. running mean: -20.684638944640795, timestamp: 2022-08-20 08:51:35.252144\n",
      "resetting env. episode 84, reward total was -20.0. running mean: -20.677792555194387, timestamp: 2022-08-20 08:51:38.578255\n",
      "resetting env. episode 85, reward total was -20.0. running mean: -20.671014629642443, timestamp: 2022-08-20 08:51:43.444249\n",
      "resetting env. episode 86, reward total was -21.0. running mean: -20.67430448334602, timestamp: 2022-08-20 08:51:47.082524\n",
      "resetting env. episode 87, reward total was -21.0. running mean: -20.677561438512562, timestamp: 2022-08-20 08:51:50.227117\n",
      "resetting env. episode 88, reward total was -21.0. running mean: -20.680785824127437, timestamp: 2022-08-20 08:51:54.112733\n",
      "resetting env. episode 89, reward total was -21.0. running mean: -20.683977965886164, timestamp: 2022-08-20 08:51:57.467766\n",
      "resetting env. episode 90, reward total was -21.0. running mean: -20.687138186227305, timestamp: 2022-08-20 08:52:01.259640\n",
      "resetting env. episode 91, reward total was -21.0. running mean: -20.69026680436503, timestamp: 2022-08-20 08:52:05.239988\n",
      "resetting env. episode 92, reward total was -21.0. running mean: -20.69336413632138, timestamp: 2022-08-20 08:52:11.579044\n",
      "resetting env. episode 93, reward total was -21.0. running mean: -20.696430494958168, timestamp: 2022-08-20 08:52:17.128217\n",
      "resetting env. episode 94, reward total was -21.0. running mean: -20.699466190008586, timestamp: 2022-08-20 08:52:21.975263\n",
      "resetting env. episode 95, reward total was -21.0. running mean: -20.7024715281085, timestamp: 2022-08-20 08:52:25.718248\n",
      "resetting env. episode 96, reward total was -19.0. running mean: -20.685446812827415, timestamp: 2022-08-20 08:52:28.727207\n",
      "resetting env. episode 97, reward total was -20.0. running mean: -20.67859234469914, timestamp: 2022-08-20 08:52:31.421006\n",
      "resetting env. episode 98, reward total was -21.0. running mean: -20.68180642125215, timestamp: 2022-08-20 08:52:34.168666\n",
      "resetting env. episode 99, reward total was -20.0. running mean: -20.674988357039627, timestamp: 2022-08-20 08:52:37.991443\n",
      "resetting env. episode 100, reward total was -21.0. running mean: -20.678238473469232, timestamp: 2022-08-20 08:52:41.297606\n",
      "resetting env. episode 101, reward total was -21.0. running mean: -20.681456088734542, timestamp: 2022-08-20 08:52:44.788275\n",
      "resetting env. episode 102, reward total was -21.0. running mean: -20.684641527847198, timestamp: 2022-08-20 08:52:47.436199\n",
      "resetting env. episode 103, reward total was -19.0. running mean: -20.667795112568726, timestamp: 2022-08-20 08:52:51.820493\n",
      "resetting env. episode 104, reward total was -21.0. running mean: -20.67111716144304, timestamp: 2022-08-20 08:52:55.603366\n",
      "resetting env. episode 105, reward total was -21.0. running mean: -20.67440598982861, timestamp: 2022-08-20 08:52:58.404879\n",
      "resetting env. episode 106, reward total was -21.0. running mean: -20.677661929930323, timestamp: 2022-08-20 08:53:01.913499\n",
      "resetting env. episode 107, reward total was -20.0. running mean: -20.670885310631018, timestamp: 2022-08-20 08:53:04.810756\n",
      "resetting env. episode 108, reward total was -20.0. running mean: -20.664176457524707, timestamp: 2022-08-20 08:53:08.462992\n",
      "resetting env. episode 109, reward total was -21.0. running mean: -20.66753469294946, timestamp: 2022-08-20 08:53:11.738238\n",
      "resetting env. episode 110, reward total was -21.0. running mean: -20.670859346019967, timestamp: 2022-08-20 08:53:14.863883\n",
      "resetting env. episode 111, reward total was -21.0. running mean: -20.67415075255977, timestamp: 2022-08-20 08:53:17.768121\n",
      "resetting env. episode 112, reward total was -21.0. running mean: -20.67740924503417, timestamp: 2022-08-20 08:53:20.446962\n",
      "resetting env. episode 113, reward total was -21.0. running mean: -20.68063515258383, timestamp: 2022-08-20 08:53:23.348204\n",
      "resetting env. episode 114, reward total was -21.0. running mean: -20.68382880105799, timestamp: 2022-08-20 08:53:25.776712\n",
      "resetting env. episode 115, reward total was -20.0. running mean: -20.67699051304741, timestamp: 2022-08-20 08:53:28.531350\n",
      "resetting env. episode 116, reward total was -21.0. running mean: -20.680220607916937, timestamp: 2022-08-20 08:53:32.597480\n",
      "resetting env. episode 117, reward total was -21.0. running mean: -20.683418401837766, timestamp: 2022-08-20 08:53:35.236427\n",
      "resetting env. episode 118, reward total was -19.0. running mean: -20.66658421781939, timestamp: 2022-08-20 08:53:38.911607\n",
      "resetting env. episode 119, reward total was -19.0. running mean: -20.6499183756412, timestamp: 2022-08-20 08:53:42.571821\n",
      "resetting env. episode 120, reward total was -20.0. running mean: -20.643419191884785, timestamp: 2022-08-20 08:53:47.445795\n",
      "resetting env. episode 121, reward total was -21.0. running mean: -20.646984999965937, timestamp: 2022-08-20 08:53:50.023901\n",
      "resetting env. episode 122, reward total was -18.0. running mean: -20.620515149966277, timestamp: 2022-08-20 08:53:53.445753\n",
      "resetting env. episode 123, reward total was -19.0. running mean: -20.604309998466615, timestamp: 2022-08-20 08:53:57.024192\n",
      "resetting env. episode 124, reward total was -19.0. running mean: -20.58826689848195, timestamp: 2022-08-20 08:54:02.539455\n",
      "resetting env. episode 125, reward total was -21.0. running mean: -20.59238422949713, timestamp: 2022-08-20 08:54:05.613228\n",
      "resetting env. episode 126, reward total was -21.0. running mean: -20.59646038720216, timestamp: 2022-08-20 08:54:08.337947\n",
      "resetting env. episode 127, reward total was -21.0. running mean: -20.60049578333014, timestamp: 2022-08-20 08:54:11.720906\n",
      "resetting env. episode 128, reward total was -21.0. running mean: -20.60449082549684, timestamp: 2022-08-20 08:54:15.601536\n",
      "resetting env. episode 129, reward total was -21.0. running mean: -20.60844591724187, timestamp: 2022-08-20 08:54:18.825913\n",
      "resetting env. episode 130, reward total was -21.0. running mean: -20.61236145806945, timestamp: 2022-08-20 08:54:21.699232\n",
      "resetting env. episode 131, reward total was -20.0. running mean: -20.606237843488756, timestamp: 2022-08-20 08:54:25.774340\n",
      "resetting env. episode 132, reward total was -20.0. running mean: -20.60017546505387, timestamp: 2022-08-20 08:54:29.102453\n",
      "resetting env. episode 133, reward total was -21.0. running mean: -20.60417371040333, timestamp: 2022-08-20 08:54:32.490393\n",
      "resetting env. episode 134, reward total was -21.0. running mean: -20.608131973299297, timestamp: 2022-08-20 08:54:35.993049\n",
      "resetting env. episode 135, reward total was -19.0. running mean: -20.592050653566304, timestamp: 2022-08-20 08:54:39.324119\n",
      "resetting env. episode 136, reward total was -21.0. running mean: -20.596130147030642, timestamp: 2022-08-20 08:54:43.308470\n",
      "resetting env. episode 137, reward total was -21.0. running mean: -20.600168845560336, timestamp: 2022-08-20 08:54:47.463364\n",
      "resetting env. episode 138, reward total was -21.0. running mean: -20.604167157104733, timestamp: 2022-08-20 08:54:50.912149\n",
      "resetting env. episode 139, reward total was -21.0. running mean: -20.608125485533687, timestamp: 2022-08-20 08:54:54.219319\n",
      "resetting env. episode 140, reward total was -21.0. running mean: -20.61204423067835, timestamp: 2022-08-20 08:54:57.169472\n",
      "resetting env. episode 141, reward total was -21.0. running mean: -20.615923788371568, timestamp: 2022-08-20 08:55:00.347924\n",
      "resetting env. episode 142, reward total was -21.0. running mean: -20.61976455048785, timestamp: 2022-08-20 08:55:03.373852\n",
      "resetting env. episode 143, reward total was -20.0. running mean: -20.61356690498297, timestamp: 2022-08-20 08:55:05.832266\n",
      "resetting env. episode 144, reward total was -20.0. running mean: -20.60743123593314, timestamp: 2022-08-20 08:55:09.929312\n",
      "resetting env. episode 145, reward total was -21.0. running mean: -20.611356923573812, timestamp: 2022-08-20 08:55:14.449232\n",
      "resetting env. episode 146, reward total was -21.0. running mean: -20.615243354338073, timestamp: 2022-08-20 08:55:17.362443\n",
      "resetting env. episode 147, reward total was -18.0. running mean: -20.589090920794693, timestamp: 2022-08-20 08:55:21.829531\n",
      "resetting env. episode 148, reward total was -21.0. running mean: -20.593200011586745, timestamp: 2022-08-20 08:55:24.909271\n",
      "resetting env. episode 149, reward total was -21.0. running mean: -20.597268011470877, timestamp: 2022-08-20 08:55:28.013971\n",
      "resetting env. episode 150, reward total was -20.0. running mean: -20.591295331356168, timestamp: 2022-08-20 08:55:32.142962\n",
      "resetting env. episode 151, reward total was -21.0. running mean: -20.595382378042608, timestamp: 2022-08-20 08:55:35.205748\n",
      "resetting env. episode 152, reward total was -21.0. running mean: -20.599428554262182, timestamp: 2022-08-20 08:55:39.159212\n",
      "resetting env. episode 153, reward total was -19.0. running mean: -20.58343426871956, timestamp: 2022-08-20 08:55:42.986949\n",
      "resetting env. episode 154, reward total was -21.0. running mean: -20.587599926032365, timestamp: 2022-08-20 08:55:45.905154\n",
      "resetting env. episode 155, reward total was -19.0. running mean: -20.57172392677204, timestamp: 2022-08-20 08:55:50.310398\n",
      "resetting env. episode 156, reward total was -21.0. running mean: -20.57600668750432, timestamp: 2022-08-20 08:55:54.453300\n",
      "resetting env. episode 157, reward total was -21.0. running mean: -20.58024662062928, timestamp: 2022-08-20 08:55:58.673023\n",
      "resetting env. episode 158, reward total was -21.0. running mean: -20.584444154422986, timestamp: 2022-08-20 08:56:01.910368\n",
      "resetting env. episode 159, reward total was -19.0. running mean: -20.568599712878758, timestamp: 2022-08-20 08:56:06.040336\n",
      "resetting env. episode 160, reward total was -21.0. running mean: -20.57291371574997, timestamp: 2022-08-20 08:56:09.887045\n",
      "resetting env. episode 161, reward total was -19.0. running mean: -20.55718457859247, timestamp: 2022-08-20 08:56:13.926270\n",
      "resetting env. episode 162, reward total was -20.0. running mean: -20.551612732806543, timestamp: 2022-08-20 08:56:17.621372\n",
      "resetting env. episode 163, reward total was -18.0. running mean: -20.52609660547848, timestamp: 2022-08-20 08:56:22.390635\n",
      "resetting env. episode 164, reward total was -21.0. running mean: -20.530835639423696, timestamp: 2022-08-20 08:56:25.469396\n",
      "resetting env. episode 165, reward total was -21.0. running mean: -20.53552728302946, timestamp: 2022-08-20 08:56:29.221368\n",
      "resetting env. episode 166, reward total was -19.0. running mean: -20.520172010199165, timestamp: 2022-08-20 08:56:33.541817\n",
      "resetting env. episode 167, reward total was -19.0. running mean: -20.504970290097173, timestamp: 2022-08-20 08:56:37.888199\n",
      "resetting env. episode 168, reward total was -21.0. running mean: -20.509920587196202, timestamp: 2022-08-20 08:56:41.134523\n",
      "resetting env. episode 169, reward total was -19.0. running mean: -20.49482138132424, timestamp: 2022-08-20 08:56:45.030108\n",
      "resetting env. episode 170, reward total was -19.0. running mean: -20.479873167511, timestamp: 2022-08-20 08:56:48.738196\n",
      "resetting env. episode 171, reward total was -20.0. running mean: -20.475074435835886, timestamp: 2022-08-20 08:56:52.284717\n",
      "resetting env. episode 172, reward total was -19.0. running mean: -20.460323691477527, timestamp: 2022-08-20 08:56:55.739483\n",
      "resetting env. episode 173, reward total was -21.0. running mean: -20.465720454562753, timestamp: 2022-08-20 08:56:58.940956\n",
      "resetting env. episode 174, reward total was -21.0. running mean: -20.471063250017124, timestamp: 2022-08-20 08:57:02.470490\n",
      "resetting env. episode 175, reward total was -21.0. running mean: -20.476352617516955, timestamp: 2022-08-20 08:57:05.849458\n",
      "resetting env. episode 176, reward total was -20.0. running mean: -20.471589091341784, timestamp: 2022-08-20 08:57:10.325493\n",
      "resetting env. episode 177, reward total was -20.0. running mean: -20.466873200428363, timestamp: 2022-08-20 08:57:13.604784\n",
      "resetting env. episode 178, reward total was -21.0. running mean: -20.47220446842408, timestamp: 2022-08-20 08:57:17.753646\n",
      "resetting env. episode 179, reward total was -18.0. running mean: -20.44748242373984, timestamp: 2022-08-20 08:57:22.510923\n",
      "resetting env. episode 180, reward total was -20.0. running mean: -20.44300759950244, timestamp: 2022-08-20 08:57:28.041139\n",
      "resetting env. episode 181, reward total was -20.0. running mean: -20.438577523507416, timestamp: 2022-08-20 08:57:31.621570\n",
      "resetting env. episode 182, reward total was -20.0. running mean: -20.43419174827234, timestamp: 2022-08-20 08:57:35.037440\n",
      "resetting env. episode 183, reward total was -19.0. running mean: -20.41984983078962, timestamp: 2022-08-20 08:57:39.142466\n",
      "resetting env. episode 184, reward total was -21.0. running mean: -20.425651332481724, timestamp: 2022-08-20 08:57:42.209269\n",
      "resetting env. episode 185, reward total was -21.0. running mean: -20.431394819156907, timestamp: 2022-08-20 08:57:46.244485\n",
      "resetting env. episode 186, reward total was -21.0. running mean: -20.43708087096534, timestamp: 2022-08-20 08:57:49.942597\n",
      "resetting env. episode 187, reward total was -21.0. running mean: -20.442710062255685, timestamp: 2022-08-20 08:57:53.824223\n",
      "resetting env. episode 188, reward total was -21.0. running mean: -20.44828296163313, timestamp: 2022-08-20 08:57:56.955852\n",
      "resetting env. episode 189, reward total was -21.0. running mean: -20.4538001320168, timestamp: 2022-08-20 08:57:59.776311\n",
      "resetting env. episode 190, reward total was -20.0. running mean: -20.44926213069663, timestamp: 2022-08-20 08:58:03.008671\n",
      "resetting env. episode 191, reward total was -21.0. running mean: -20.454769509389667, timestamp: 2022-08-20 08:58:06.318826\n",
      "resetting env. episode 192, reward total was -19.0. running mean: -20.44022181429577, timestamp: 2022-08-20 08:58:10.785888\n",
      "resetting env. episode 193, reward total was -20.0. running mean: -20.43581959615281, timestamp: 2022-08-20 08:58:14.177816\n",
      "resetting env. episode 194, reward total was -18.0. running mean: -20.411461400191282, timestamp: 2022-08-20 08:58:17.633578\n",
      "resetting env. episode 195, reward total was -21.0. running mean: -20.41734678618937, timestamp: 2022-08-20 08:58:20.578706\n",
      "resetting env. episode 196, reward total was -19.0. running mean: -20.40317331832748, timestamp: 2022-08-20 08:58:24.092316\n",
      "resetting env. episode 197, reward total was -21.0. running mean: -20.409141585144205, timestamp: 2022-08-20 08:58:27.609911\n",
      "resetting env. episode 198, reward total was -20.0. running mean: -20.40505016929276, timestamp: 2022-08-20 08:58:31.435689\n",
      "resetting env. episode 199, reward total was -19.0. running mean: -20.390999667599832, timestamp: 2022-08-20 08:58:34.735871\n",
      "resetting env. episode 200, reward total was -21.0. running mean: -20.397089670923833, timestamp: 2022-08-20 08:58:37.809648\n",
      "resetting env. episode 201, reward total was -21.0. running mean: -20.403118774214594, timestamp: 2022-08-20 08:58:40.596200\n",
      "resetting env. episode 202, reward total was -20.0. running mean: -20.399087586472447, timestamp: 2022-08-20 08:58:43.828558\n",
      "resetting env. episode 203, reward total was -21.0. running mean: -20.405096710607722, timestamp: 2022-08-20 08:58:46.544301\n",
      "resetting env. episode 204, reward total was -20.0. running mean: -20.401045743501644, timestamp: 2022-08-20 08:58:49.939226\n",
      "resetting env. episode 205, reward total was -21.0. running mean: -20.407035286066627, timestamp: 2022-08-20 08:58:53.333155\n",
      "resetting env. episode 206, reward total was -21.0. running mean: -20.412964933205963, timestamp: 2022-08-20 08:58:56.355087\n",
      "resetting env. episode 207, reward total was -16.0. running mean: -20.368835283873903, timestamp: 2022-08-20 08:59:01.302853\n",
      "resetting env. episode 208, reward total was -20.0. running mean: -20.365146931035163, timestamp: 2022-08-20 08:59:05.219382\n",
      "resetting env. episode 209, reward total was -20.0. running mean: -20.36149546172481, timestamp: 2022-08-20 08:59:09.385248\n",
      "resetting env. episode 210, reward total was -20.0. running mean: -20.357880507107563, timestamp: 2022-08-20 08:59:12.853974\n",
      "resetting env. episode 211, reward total was -19.0. running mean: -20.34430170203649, timestamp: 2022-08-20 08:59:16.323707\n",
      "resetting env. episode 212, reward total was -20.0. running mean: -20.340858685016123, timestamp: 2022-08-20 08:59:20.704988\n",
      "resetting env. episode 213, reward total was -21.0. running mean: -20.34745009816596, timestamp: 2022-08-20 08:59:24.086952\n",
      "resetting env. episode 214, reward total was -21.0. running mean: -20.353975597184302, timestamp: 2022-08-20 08:59:27.180682\n",
      "resetting env. episode 215, reward total was -21.0. running mean: -20.36043584121246, timestamp: 2022-08-20 08:59:30.190637\n",
      "resetting env. episode 216, reward total was -20.0. running mean: -20.356831482800334, timestamp: 2022-08-20 08:59:33.296340\n",
      "resetting env. episode 217, reward total was -20.0. running mean: -20.35326316797233, timestamp: 2022-08-20 08:59:36.493786\n",
      "resetting env. episode 218, reward total was -20.0. running mean: -20.349730536292604, timestamp: 2022-08-20 08:59:39.644364\n",
      "resetting env. episode 219, reward total was -21.0. running mean: -20.35623323092968, timestamp: 2022-08-20 08:59:43.267680\n",
      "resetting env. episode 220, reward total was -20.0. running mean: -20.35267089862038, timestamp: 2022-08-20 08:59:46.678560\n",
      "resetting env. episode 221, reward total was -19.0. running mean: -20.33914418963418, timestamp: 2022-08-20 08:59:50.472420\n",
      "resetting env. episode 222, reward total was -20.0. running mean: -20.335752747737835, timestamp: 2022-08-20 08:59:54.996328\n",
      "resetting env. episode 223, reward total was -19.0. running mean: -20.322395220260457, timestamp: 2022-08-20 08:59:59.256943\n",
      "resetting env. episode 224, reward total was -21.0. running mean: -20.329171268057852, timestamp: 2022-08-20 09:00:02.537173\n",
      "resetting env. episode 225, reward total was -21.0. running mean: -20.335879555377275, timestamp: 2022-08-20 09:00:06.110620\n",
      "resetting env. episode 226, reward total was -18.0. running mean: -20.312520759823503, timestamp: 2022-08-20 09:00:09.508552\n",
      "resetting env. episode 227, reward total was -20.0. running mean: -20.30939555222527, timestamp: 2022-08-20 09:00:13.183714\n",
      "resetting env. episode 228, reward total was -19.0. running mean: -20.296301596703017, timestamp: 2022-08-20 09:00:17.240870\n",
      "resetting env. episode 229, reward total was -19.0. running mean: -20.28333858073599, timestamp: 2022-08-20 09:00:20.827283\n",
      "resetting env. episode 230, reward total was -21.0. running mean: -20.29050519492863, timestamp: 2022-08-20 09:00:24.324934\n",
      "resetting env. episode 231, reward total was -21.0. running mean: -20.297600142979345, timestamp: 2022-08-20 09:00:27.020726\n",
      "resetting env. episode 232, reward total was -19.0. running mean: -20.284624141549553, timestamp: 2022-08-20 09:00:30.784667\n",
      "resetting env. episode 233, reward total was -21.0. running mean: -20.291777900134058, timestamp: 2022-08-20 09:00:33.428598\n",
      "resetting env. episode 234, reward total was -20.0. running mean: -20.288860121132718, timestamp: 2022-08-20 09:00:37.339146\n",
      "resetting env. episode 235, reward total was -20.0. running mean: -20.28597151992139, timestamp: 2022-08-20 09:00:40.646306\n",
      "resetting env. episode 236, reward total was -20.0. running mean: -20.283111804722175, timestamp: 2022-08-20 09:00:45.432530\n",
      "resetting env. episode 237, reward total was -20.0. running mean: -20.280280686674953, timestamp: 2022-08-20 09:00:48.901240\n",
      "resetting env. episode 238, reward total was -21.0. running mean: -20.287477879808204, timestamp: 2022-08-20 09:00:51.814455\n",
      "resetting env. episode 239, reward total was -20.0. running mean: -20.28460310101012, timestamp: 2022-08-20 09:00:54.857339\n",
      "resetting env. episode 240, reward total was -19.0. running mean: -20.27175707000002, timestamp: 2022-08-20 09:00:58.617270\n",
      "resetting env. episode 241, reward total was -19.0. running mean: -20.259039499300023, timestamp: 2022-08-20 09:01:01.847665\n",
      "resetting env. episode 242, reward total was -18.0. running mean: -20.236449104307024, timestamp: 2022-08-20 09:01:05.371215\n",
      "resetting env. episode 243, reward total was -20.0. running mean: -20.234084613263953, timestamp: 2022-08-20 09:01:08.822019\n",
      "resetting env. episode 244, reward total was -17.0. running mean: -20.201743767131315, timestamp: 2022-08-20 09:01:12.504156\n",
      "resetting env. episode 245, reward total was -19.0. running mean: -20.189726329460004, timestamp: 2022-08-20 09:01:15.567961\n",
      "resetting env. episode 246, reward total was -21.0. running mean: -20.197829066165404, timestamp: 2022-08-20 09:01:18.707567\n",
      "resetting env. episode 247, reward total was -20.0. running mean: -20.19585077550375, timestamp: 2022-08-20 09:01:21.764397\n",
      "resetting env. episode 248, reward total was -20.0. running mean: -20.193892267748712, timestamp: 2022-08-20 09:01:26.299276\n",
      "resetting env. episode 249, reward total was -19.0. running mean: -20.181953345071225, timestamp: 2022-08-20 09:01:29.271329\n",
      "resetting env. episode 250, reward total was -20.0. running mean: -20.18013381162051, timestamp: 2022-08-20 09:01:33.983762\n",
      "resetting env. episode 251, reward total was -21.0. running mean: -20.188332473504307, timestamp: 2022-08-20 09:01:37.879324\n",
      "resetting env. episode 252, reward total was -21.0. running mean: -20.196449148769265, timestamp: 2022-08-20 09:01:41.494658\n",
      "resetting env. episode 253, reward total was -21.0. running mean: -20.204484657281572, timestamp: 2022-08-20 09:01:44.503616\n",
      "resetting env. episode 254, reward total was -21.0. running mean: -20.21243981070876, timestamp: 2022-08-20 09:01:48.009244\n",
      "resetting env. episode 255, reward total was -19.0. running mean: -20.20031541260167, timestamp: 2022-08-20 09:01:52.060416\n",
      "resetting env. episode 256, reward total was -18.0. running mean: -20.178312258475653, timestamp: 2022-08-20 09:01:55.382536\n",
      "resetting env. episode 257, reward total was -21.0. running mean: -20.186529135890897, timestamp: 2022-08-20 09:01:58.938032\n",
      "resetting env. episode 258, reward total was -20.0. running mean: -20.184663844531986, timestamp: 2022-08-20 09:02:02.169403\n",
      "resetting env. episode 259, reward total was -21.0. running mean: -20.192817206086666, timestamp: 2022-08-20 09:02:05.893440\n",
      "resetting env. episode 260, reward total was -21.0. running mean: -20.2008890340258, timestamp: 2022-08-20 09:02:10.276722\n",
      "resetting env. episode 261, reward total was -21.0. running mean: -20.20888014368554, timestamp: 2022-08-20 09:02:14.555288\n",
      "resetting env. episode 262, reward total was -21.0. running mean: -20.216791342248687, timestamp: 2022-08-20 09:02:17.583193\n",
      "resetting env. episode 263, reward total was -19.0. running mean: -20.204623428826203, timestamp: 2022-08-20 09:02:22.448189\n",
      "resetting env. episode 264, reward total was -21.0. running mean: -20.212577194537943, timestamp: 2022-08-20 09:02:25.597770\n",
      "resetting env. episode 265, reward total was -21.0. running mean: -20.220451422592564, timestamp: 2022-08-20 09:02:29.017629\n",
      "resetting env. episode 266, reward total was -21.0. running mean: -20.22824690836664, timestamp: 2022-08-20 09:02:32.494350\n",
      "resetting env. episode 267, reward total was -21.0. running mean: -20.235964439282974, timestamp: 2022-08-20 09:02:35.633943\n",
      "resetting env. episode 268, reward total was -21.0. running mean: -20.243604794890146, timestamp: 2022-08-20 09:02:38.996953\n",
      "resetting env. episode 269, reward total was -20.0. running mean: -20.241168746941245, timestamp: 2022-08-20 09:02:43.325393\n",
      "resetting env. episode 270, reward total was -21.0. running mean: -20.24875705947183, timestamp: 2022-08-20 09:02:47.415451\n",
      "resetting env. episode 271, reward total was -19.0. running mean: -20.236269488877113, timestamp: 2022-08-20 09:02:51.353924\n",
      "resetting env. episode 272, reward total was -21.0. running mean: -20.243906793988344, timestamp: 2022-08-20 09:02:54.057696\n",
      "resetting env. episode 273, reward total was -19.0. running mean: -20.23146772604846, timestamp: 2022-08-20 09:02:57.188332\n",
      "resetting env. episode 274, reward total was -21.0. running mean: -20.239153048787976, timestamp: 2022-08-20 09:03:00.316972\n",
      "resetting env. episode 275, reward total was -20.0. running mean: -20.236761518300096, timestamp: 2022-08-20 09:03:03.929310\n",
      "resetting env. episode 276, reward total was -21.0. running mean: -20.244393903117096, timestamp: 2022-08-20 09:03:07.724167\n",
      "resetting env. episode 277, reward total was -19.0. running mean: -20.231949964085928, timestamp: 2022-08-20 09:03:11.192895\n",
      "resetting env. episode 278, reward total was -21.0. running mean: -20.23963046444507, timestamp: 2022-08-20 09:03:15.351778\n",
      "resetting env. episode 279, reward total was -21.0. running mean: -20.24723415980062, timestamp: 2022-08-20 09:03:18.159274\n",
      "resetting env. episode 280, reward total was -21.0. running mean: -20.254761818202613, timestamp: 2022-08-20 09:03:21.369713\n",
      "resetting env. episode 281, reward total was -20.0. running mean: -20.252214200020585, timestamp: 2022-08-20 09:03:25.356039\n",
      "resetting env. episode 282, reward total was -21.0. running mean: -20.25969205802038, timestamp: 2022-08-20 09:03:28.737996\n",
      "resetting env. episode 283, reward total was -20.0. running mean: -20.257095137440174, timestamp: 2022-08-20 09:03:32.257605\n",
      "resetting env. episode 284, reward total was -21.0. running mean: -20.264524186065774, timestamp: 2022-08-20 09:03:35.498924\n",
      "resetting env. episode 285, reward total was -20.0. running mean: -20.261878944205115, timestamp: 2022-08-20 09:03:39.770506\n",
      "resetting env. episode 286, reward total was -21.0. running mean: -20.269260154763064, timestamp: 2022-08-20 09:03:44.679386\n",
      "resetting env. episode 287, reward total was -16.0. running mean: -20.226567553215432, timestamp: 2022-08-20 09:03:49.452626\n",
      "resetting env. episode 288, reward total was -21.0. running mean: -20.23430187768328, timestamp: 2022-08-20 09:03:53.285380\n",
      "resetting env. episode 289, reward total was -18.0. running mean: -20.211958858906446, timestamp: 2022-08-20 09:03:57.125118\n",
      "resetting env. episode 290, reward total was -20.0. running mean: -20.20983927031738, timestamp: 2022-08-20 09:04:01.089520\n",
      "resetting env. episode 291, reward total was -21.0. running mean: -20.217740877614208, timestamp: 2022-08-20 09:04:03.803266\n",
      "resetting env. episode 292, reward total was -19.0. running mean: -20.20556346883807, timestamp: 2022-08-20 09:04:08.167602\n",
      "resetting env. episode 293, reward total was -20.0. running mean: -20.203507834149686, timestamp: 2022-08-20 09:04:13.661919\n",
      "resetting env. episode 294, reward total was -21.0. running mean: -20.21147275580819, timestamp: 2022-08-20 09:04:17.924523\n",
      "resetting env. episode 295, reward total was -20.0. running mean: -20.209358028250108, timestamp: 2022-08-20 09:04:22.882269\n",
      "resetting env. episode 296, reward total was -21.0. running mean: -20.217264447967608, timestamp: 2022-08-20 09:04:26.374933\n",
      "resetting env. episode 297, reward total was -21.0. running mean: -20.22509180348793, timestamp: 2022-08-20 09:04:30.454039\n",
      "resetting env. episode 298, reward total was -21.0. running mean: -20.23284088545305, timestamp: 2022-08-20 09:04:32.934404\n",
      "resetting env. episode 299, reward total was -17.0. running mean: -20.200512476598522, timestamp: 2022-08-20 09:04:37.195013\n",
      "resetting env. episode 300, reward total was -20.0. running mean: -20.198507351832536, timestamp: 2022-08-20 09:04:40.329633\n",
      "resetting env. episode 301, reward total was -21.0. running mean: -20.20652227831421, timestamp: 2022-08-20 09:04:43.509133\n",
      "resetting env. episode 302, reward total was -21.0. running mean: -20.21445705553107, timestamp: 2022-08-20 09:04:46.622821\n",
      "resetting env. episode 303, reward total was -20.0. running mean: -20.212312484975758, timestamp: 2022-08-20 09:04:50.657025\n",
      "resetting env. episode 304, reward total was -19.0. running mean: -20.200189360126, timestamp: 2022-08-20 09:04:54.157672\n",
      "resetting env. episode 305, reward total was -21.0. running mean: -20.20818746652474, timestamp: 2022-08-20 09:04:56.944223\n",
      "resetting env. episode 306, reward total was -16.0. running mean: -20.166105591859495, timestamp: 2022-08-20 09:05:00.657297\n",
      "resetting env. episode 307, reward total was -20.0. running mean: -20.1644445359409, timestamp: 2022-08-20 09:05:04.053219\n",
      "resetting env. episode 308, reward total was -19.0. running mean: -20.15280009058149, timestamp: 2022-08-20 09:05:07.548875\n",
      "resetting env. episode 309, reward total was -19.0. running mean: -20.141272089675677, timestamp: 2022-08-20 09:05:11.437483\n",
      "resetting env. episode 310, reward total was -20.0. running mean: -20.139859368778918, timestamp: 2022-08-20 09:05:14.368645\n",
      "resetting env. episode 311, reward total was -20.0. running mean: -20.13846077509113, timestamp: 2022-08-20 09:05:18.199419\n",
      "resetting env. episode 312, reward total was -18.0. running mean: -20.11707616734022, timestamp: 2022-08-20 09:05:22.676437\n",
      "resetting env. episode 313, reward total was -17.0. running mean: -20.085905405666818, timestamp: 2022-08-20 09:05:26.891172\n",
      "resetting env. episode 314, reward total was -21.0. running mean: -20.09504635161015, timestamp: 2022-08-20 09:05:30.219278\n",
      "resetting env. episode 315, reward total was -19.0. running mean: -20.08409588809405, timestamp: 2022-08-20 09:05:33.904429\n",
      "resetting env. episode 316, reward total was -21.0. running mean: -20.09325492921311, timestamp: 2022-08-20 09:05:38.103214\n",
      "resetting env. episode 317, reward total was -20.0. running mean: -20.09232237992098, timestamp: 2022-08-20 09:05:40.928649\n",
      "resetting env. episode 318, reward total was -21.0. running mean: -20.10139915612177, timestamp: 2022-08-20 09:05:44.671647\n",
      "resetting env. episode 319, reward total was -21.0. running mean: -20.11038516456055, timestamp: 2022-08-20 09:05:48.119430\n",
      "resetting env. episode 320, reward total was -21.0. running mean: -20.119281312914946, timestamp: 2022-08-20 09:05:51.811607\n",
      "resetting env. episode 321, reward total was -20.0. running mean: -20.118088499785795, timestamp: 2022-08-20 09:05:56.493047\n",
      "resetting env. episode 322, reward total was -21.0. running mean: -20.12690761478794, timestamp: 2022-08-20 09:05:59.940841\n",
      "resetting env. episode 323, reward total was -19.0. running mean: -20.11563853864006, timestamp: 2022-08-20 09:06:03.811490\n",
      "resetting env. episode 324, reward total was -20.0. running mean: -20.114482153253658, timestamp: 2022-08-20 09:06:08.653544\n",
      "resetting env. episode 325, reward total was -20.0. running mean: -20.11333733172112, timestamp: 2022-08-20 09:06:12.003591\n",
      "resetting env. episode 326, reward total was -18.0. running mean: -20.092203958403907, timestamp: 2022-08-20 09:06:16.305090\n",
      "resetting env. episode 327, reward total was -21.0. running mean: -20.10128191881987, timestamp: 2022-08-20 09:06:19.663114\n",
      "resetting env. episode 328, reward total was -17.0. running mean: -20.07026909963167, timestamp: 2022-08-20 09:06:25.731890\n",
      "resetting env. episode 329, reward total was -21.0. running mean: -20.079566408635355, timestamp: 2022-08-20 09:06:29.116850\n",
      "resetting env. episode 330, reward total was -20.0. running mean: -20.078770744549, timestamp: 2022-08-20 09:06:33.599859\n",
      "resetting env. episode 331, reward total was -19.0. running mean: -20.067983037103513, timestamp: 2022-08-20 09:06:37.609154\n",
      "resetting env. episode 332, reward total was -21.0. running mean: -20.077303206732477, timestamp: 2022-08-20 09:06:41.174613\n",
      "resetting env. episode 333, reward total was -19.0. running mean: -20.066530174665154, timestamp: 2022-08-20 09:06:45.373390\n",
      "resetting env. episode 334, reward total was -21.0. running mean: -20.0758648729185, timestamp: 2022-08-20 09:06:50.116714\n",
      "resetting env. episode 335, reward total was -20.0. running mean: -20.075106224189316, timestamp: 2022-08-20 09:06:54.651594\n",
      "resetting env. episode 336, reward total was -21.0. running mean: -20.084355161947425, timestamp: 2022-08-20 09:06:58.013602\n",
      "resetting env. episode 337, reward total was -19.0. running mean: -20.07351161032795, timestamp: 2022-08-20 09:07:02.183456\n",
      "resetting env. episode 338, reward total was -20.0. running mean: -20.072776494224673, timestamp: 2022-08-20 09:07:06.240612\n",
      "resetting env. episode 339, reward total was -20.0. running mean: -20.072048729282425, timestamp: 2022-08-20 09:07:10.241916\n",
      "resetting env. episode 340, reward total was -20.0. running mean: -20.0713282419896, timestamp: 2022-08-20 09:07:13.685738\n",
      "resetting env. episode 341, reward total was -20.0. running mean: -20.070614959569703, timestamp: 2022-08-20 09:07:18.161747\n",
      "resetting env. episode 342, reward total was -21.0. running mean: -20.079908809974008, timestamp: 2022-08-20 09:07:21.508798\n",
      "resetting env. episode 343, reward total was -21.0. running mean: -20.08910972187427, timestamp: 2022-08-20 09:07:25.555982\n",
      "resetting env. episode 344, reward total was -18.0. running mean: -20.068218624655525, timestamp: 2022-08-20 09:07:30.911669\n",
      "resetting env. episode 345, reward total was -20.0. running mean: -20.06753643840897, timestamp: 2022-08-20 09:07:35.430587\n",
      "resetting env. episode 346, reward total was -20.0. running mean: -20.066861074024878, timestamp: 2022-08-20 09:07:39.535615\n",
      "resetting env. episode 347, reward total was -18.0. running mean: -20.04619246328463, timestamp: 2022-08-20 09:07:43.330470\n",
      "resetting env. episode 348, reward total was -19.0. running mean: -20.035730538651784, timestamp: 2022-08-20 09:07:47.579115\n",
      "resetting env. episode 349, reward total was -21.0. running mean: -20.045373233265266, timestamp: 2022-08-20 09:07:51.902556\n",
      "resetting env. episode 350, reward total was -20.0. running mean: -20.04491950093261, timestamp: 2022-08-20 09:07:56.371639\n",
      "resetting env. episode 351, reward total was -21.0. running mean: -20.054470305923285, timestamp: 2022-08-20 09:07:59.503241\n",
      "resetting env. episode 352, reward total was -21.0. running mean: -20.063925602864053, timestamp: 2022-08-20 09:08:01.949701\n",
      "resetting env. episode 353, reward total was -20.0. running mean: -20.06328634683541, timestamp: 2022-08-20 09:08:05.659785\n",
      "resetting env. episode 354, reward total was -18.0. running mean: -20.042653483367058, timestamp: 2022-08-20 09:08:10.319340\n",
      "resetting env. episode 355, reward total was -20.0. running mean: -20.042226948533386, timestamp: 2022-08-20 09:08:14.299692\n",
      "resetting env. episode 356, reward total was -20.0. running mean: -20.041804679048052, timestamp: 2022-08-20 09:08:17.998816\n",
      "resetting env. episode 357, reward total was -18.0. running mean: -20.02138663225757, timestamp: 2022-08-20 09:08:23.463195\n",
      "resetting env. episode 358, reward total was -18.0. running mean: -20.001172765934996, timestamp: 2022-08-20 09:08:28.528661\n",
      "resetting env. episode 359, reward total was -18.0. running mean: -19.981161038275644, timestamp: 2022-08-20 09:08:34.780954\n",
      "resetting env. episode 360, reward total was -20.0. running mean: -19.981349427892887, timestamp: 2022-08-20 09:08:39.643954\n",
      "resetting env. episode 361, reward total was -20.0. running mean: -19.98153593361396, timestamp: 2022-08-20 09:08:44.458086\n",
      "resetting env. episode 362, reward total was -18.0. running mean: -19.96172057427782, timestamp: 2022-08-20 09:08:50.476998\n",
      "resetting env. episode 363, reward total was -19.0. running mean: -19.95210336853504, timestamp: 2022-08-20 09:08:55.631210\n",
      "resetting env. episode 364, reward total was -15.0. running mean: -19.90258233484969, timestamp: 2022-08-20 09:09:02.358230\n",
      "resetting env. episode 365, reward total was -20.0. running mean: -19.903556511501193, timestamp: 2022-08-20 09:09:07.477548\n",
      "resetting env. episode 366, reward total was -19.0. running mean: -19.894520946386184, timestamp: 2022-08-20 09:09:11.887756\n",
      "resetting env. episode 367, reward total was -19.0. running mean: -19.885575736922323, timestamp: 2022-08-20 09:09:16.290986\n",
      "resetting env. episode 368, reward total was -19.0. running mean: -19.8767199795531, timestamp: 2022-08-20 09:09:19.676936\n",
      "resetting env. episode 369, reward total was -19.0. running mean: -19.867952779757573, timestamp: 2022-08-20 09:09:24.044263\n",
      "resetting env. episode 370, reward total was -21.0. running mean: -19.879273251959997, timestamp: 2022-08-20 09:09:27.218775\n",
      "resetting env. episode 371, reward total was -20.0. running mean: -19.880480519440397, timestamp: 2022-08-20 09:09:30.950819\n",
      "resetting env. episode 372, reward total was -21.0. running mean: -19.891675714245995, timestamp: 2022-08-20 09:09:34.937147\n",
      "resetting env. episode 373, reward total was -20.0. running mean: -19.892758957103535, timestamp: 2022-08-20 09:09:38.374977\n",
      "resetting env. episode 374, reward total was -20.0. running mean: -19.8938313675325, timestamp: 2022-08-20 09:09:43.255916\n",
      "resetting env. episode 375, reward total was -21.0. running mean: -19.904893053857176, timestamp: 2022-08-20 09:09:48.250559\n",
      "resetting env. episode 376, reward total was -20.0. running mean: -19.9058441233186, timestamp: 2022-08-20 09:09:54.111900\n",
      "resetting env. episode 377, reward total was -19.0. running mean: -19.896785682085415, timestamp: 2022-08-20 09:09:58.373500\n",
      "resetting env. episode 378, reward total was -21.0. running mean: -19.907817825264562, timestamp: 2022-08-20 09:10:02.576266\n",
      "resetting env. episode 379, reward total was -21.0. running mean: -19.91873964701192, timestamp: 2022-08-20 09:10:07.702564\n",
      "resetting env. episode 380, reward total was -19.0. running mean: -19.9095522505418, timestamp: 2022-08-20 09:10:13.440226\n",
      "resetting env. episode 381, reward total was -21.0. running mean: -19.920456728036385, timestamp: 2022-08-20 09:10:16.313545\n",
      "resetting env. episode 382, reward total was -21.0. running mean: -19.93125216075602, timestamp: 2022-08-20 09:10:20.687853\n",
      "resetting env. episode 383, reward total was -20.0. running mean: -19.93193963914846, timestamp: 2022-08-20 09:10:24.706113\n",
      "resetting env. episode 384, reward total was -21.0. running mean: -19.942620242756977, timestamp: 2022-08-20 09:10:27.664207\n",
      "resetting env. episode 385, reward total was -17.0. running mean: -19.913194040329408, timestamp: 2022-08-20 09:10:32.764572\n",
      "resetting env. episode 386, reward total was -19.0. running mean: -19.904062099926115, timestamp: 2022-08-20 09:10:36.513553\n",
      "resetting env. episode 387, reward total was -18.0. running mean: -19.885021478926852, timestamp: 2022-08-20 09:10:40.937728\n",
      "resetting env. episode 388, reward total was -20.0. running mean: -19.886171264137584, timestamp: 2022-08-20 09:10:44.872210\n",
      "resetting env. episode 389, reward total was -20.0. running mean: -19.88730955149621, timestamp: 2022-08-20 09:10:48.007841\n",
      "resetting env. episode 390, reward total was -20.0. running mean: -19.888436455981246, timestamp: 2022-08-20 09:10:51.352886\n",
      "resetting env. episode 391, reward total was -18.0. running mean: -19.869552091421433, timestamp: 2022-08-20 09:10:55.328258\n",
      "resetting env. episode 392, reward total was -20.0. running mean: -19.870856570507218, timestamp: 2022-08-20 09:10:59.223849\n",
      "resetting env. episode 393, reward total was -21.0. running mean: -19.882148004802147, timestamp: 2022-08-20 09:11:02.602814\n",
      "resetting env. episode 394, reward total was -20.0. running mean: -19.883326524754125, timestamp: 2022-08-20 09:11:06.461500\n",
      "resetting env. episode 395, reward total was -20.0. running mean: -19.884493259506584, timestamp: 2022-08-20 09:11:10.333151\n",
      "resetting env. episode 396, reward total was -21.0. running mean: -19.89564832691152, timestamp: 2022-08-20 09:11:14.293565\n",
      "resetting env. episode 397, reward total was -19.0. running mean: -19.886691843642406, timestamp: 2022-08-20 09:11:17.710435\n",
      "resetting env. episode 398, reward total was -17.0. running mean: -19.857824925205986, timestamp: 2022-08-20 09:11:22.044845\n",
      "resetting env. episode 399, reward total was -19.0. running mean: -19.84924667595393, timestamp: 2022-08-20 09:11:26.537848\n",
      "resetting env. episode 400, reward total was -18.0. running mean: -19.83075420919439, timestamp: 2022-08-20 09:11:30.676773\n",
      "resetting env. episode 401, reward total was -21.0. running mean: -19.842446667102447, timestamp: 2022-08-20 09:11:34.140515\n",
      "resetting env. episode 402, reward total was -18.0. running mean: -19.824022200431422, timestamp: 2022-08-20 09:11:38.496871\n",
      "resetting env. episode 403, reward total was -21.0. running mean: -19.835781978427107, timestamp: 2022-08-20 09:11:41.534750\n",
      "resetting env. episode 404, reward total was -20.0. running mean: -19.837424158642836, timestamp: 2022-08-20 09:11:45.283728\n",
      "resetting env. episode 405, reward total was -21.0. running mean: -19.84904991705641, timestamp: 2022-08-20 09:11:49.384769\n",
      "resetting env. episode 406, reward total was -19.0. running mean: -19.840559417885846, timestamp: 2022-08-20 09:11:53.811941\n",
      "resetting env. episode 407, reward total was -21.0. running mean: -19.85215382370699, timestamp: 2022-08-20 09:11:57.878093\n",
      "resetting env. episode 408, reward total was -20.0. running mean: -19.853632285469917, timestamp: 2022-08-20 09:12:01.449519\n",
      "resetting env. episode 409, reward total was -20.0. running mean: -19.855095962615216, timestamp: 2022-08-20 09:12:04.819509\n",
      "resetting env. episode 410, reward total was -20.0. running mean: -19.856545002989062, timestamp: 2022-08-20 09:12:08.191500\n",
      "resetting env. episode 411, reward total was -19.0. running mean: -19.847979552959174, timestamp: 2022-08-20 09:12:11.926512\n",
      "resetting env. episode 412, reward total was -21.0. running mean: -19.85949975742958, timestamp: 2022-08-20 09:12:14.880619\n",
      "resetting env. episode 413, reward total was -20.0. running mean: -19.860904759855284, timestamp: 2022-08-20 09:12:18.990630\n",
      "resetting env. episode 414, reward total was -15.0. running mean: -19.81229571225673, timestamp: 2022-08-20 09:12:24.062074\n",
      "resetting env. episode 415, reward total was -18.0. running mean: -19.794172755134163, timestamp: 2022-08-20 09:12:27.975621\n",
      "resetting env. episode 416, reward total was -21.0. running mean: -19.80623102758282, timestamp: 2022-08-20 09:12:32.024789\n",
      "resetting env. episode 417, reward total was -19.0. running mean: -19.798168717306993, timestamp: 2022-08-20 09:12:35.478561\n",
      "resetting env. episode 418, reward total was -17.0. running mean: -19.770187030133926, timestamp: 2022-08-20 09:12:41.614161\n",
      "resetting env. episode 419, reward total was -21.0. running mean: -19.782485159832586, timestamp: 2022-08-20 09:12:45.194589\n",
      "resetting env. episode 420, reward total was -18.0. running mean: -19.76466030823426, timestamp: 2022-08-20 09:12:49.384388\n",
      "resetting env. episode 421, reward total was -20.0. running mean: -19.767013705151914, timestamp: 2022-08-20 09:12:53.818541\n",
      "resetting env. episode 422, reward total was -19.0. running mean: -19.759343568100395, timestamp: 2022-08-20 09:12:58.449177\n",
      "resetting env. episode 423, reward total was -21.0. running mean: -19.771750132419392, timestamp: 2022-08-20 09:13:02.688824\n",
      "resetting env. episode 424, reward total was -17.0. running mean: -19.744032631095198, timestamp: 2022-08-20 09:13:06.816794\n",
      "resetting env. episode 425, reward total was -19.0. running mean: -19.736592304784246, timestamp: 2022-08-20 09:13:10.574750\n",
      "resetting env. episode 426, reward total was -21.0. running mean: -19.749226381736403, timestamp: 2022-08-20 09:13:13.147869\n",
      "resetting env. episode 427, reward total was -18.0. running mean: -19.73173411791904, timestamp: 2022-08-20 09:13:16.904825\n",
      "resetting env. episode 428, reward total was -18.0. running mean: -19.71441677673985, timestamp: 2022-08-20 09:13:21.980260\n",
      "resetting env. episode 429, reward total was -17.0. running mean: -19.687272608972453, timestamp: 2022-08-20 09:13:25.950644\n",
      "resetting env. episode 430, reward total was -17.0. running mean: -19.66039988288273, timestamp: 2022-08-20 09:13:30.192310\n",
      "resetting env. episode 431, reward total was -20.0. running mean: -19.6637958840539, timestamp: 2022-08-20 09:13:33.109513\n",
      "resetting env. episode 432, reward total was -21.0. running mean: -19.67715792521336, timestamp: 2022-08-20 09:13:36.476510\n",
      "resetting env. episode 433, reward total was -19.0. running mean: -19.67038634596123, timestamp: 2022-08-20 09:13:40.304278\n",
      "resetting env. episode 434, reward total was -21.0. running mean: -19.68368248250162, timestamp: 2022-08-20 09:13:43.143689\n",
      "resetting env. episode 435, reward total was -19.0. running mean: -19.676845657676605, timestamp: 2022-08-20 09:13:46.537617\n",
      "resetting env. episode 436, reward total was -21.0. running mean: -19.69007720109984, timestamp: 2022-08-20 09:13:50.169906\n",
      "resetting env. episode 437, reward total was -20.0. running mean: -19.69317642908884, timestamp: 2022-08-20 09:13:53.521949\n",
      "resetting env. episode 438, reward total was -19.0. running mean: -19.686244664797954, timestamp: 2022-08-20 09:13:56.864014\n",
      "resetting env. episode 439, reward total was -19.0. running mean: -19.679382218149975, timestamp: 2022-08-20 09:14:00.564127\n",
      "resetting env. episode 440, reward total was -18.0. running mean: -19.662588395968474, timestamp: 2022-08-20 09:14:04.450736\n",
      "resetting env. episode 441, reward total was -17.0. running mean: -19.635962512008792, timestamp: 2022-08-20 09:14:09.724638\n",
      "resetting env. episode 442, reward total was -19.0. running mean: -19.629602886888705, timestamp: 2022-08-20 09:14:13.652140\n",
      "resetting env. episode 443, reward total was -18.0. running mean: -19.61330685801982, timestamp: 2022-08-20 09:14:17.556702\n",
      "resetting env. episode 444, reward total was -20.0. running mean: -19.61717378943962, timestamp: 2022-08-20 09:14:21.012464\n",
      "resetting env. episode 445, reward total was -20.0. running mean: -19.621002051545222, timestamp: 2022-08-20 09:14:25.083586\n",
      "resetting env. episode 446, reward total was -20.0. running mean: -19.62479203102977, timestamp: 2022-08-20 09:14:28.198257\n",
      "resetting env. episode 447, reward total was -21.0. running mean: -19.638544110719472, timestamp: 2022-08-20 09:14:31.723834\n",
      "resetting env. episode 448, reward total was -20.0. running mean: -19.642158669612275, timestamp: 2022-08-20 09:14:36.333515\n",
      "resetting env. episode 449, reward total was -19.0. running mean: -19.635737082916155, timestamp: 2022-08-20 09:14:40.286944\n",
      "resetting env. episode 450, reward total was -17.0. running mean: -19.609379712086994, timestamp: 2022-08-20 09:14:45.783252\n",
      "resetting env. episode 451, reward total was -21.0. running mean: -19.623285914966125, timestamp: 2022-08-20 09:14:49.357698\n",
      "resetting env. episode 452, reward total was -21.0. running mean: -19.637053055816466, timestamp: 2022-08-20 09:14:52.859340\n",
      "resetting env. episode 453, reward total was -19.0. running mean: -19.6306825252583, timestamp: 2022-08-20 09:14:56.374941\n",
      "resetting env. episode 454, reward total was -20.0. running mean: -19.634375700005716, timestamp: 2022-08-20 09:14:59.699056\n",
      "resetting env. episode 455, reward total was -21.0. running mean: -19.64803194300566, timestamp: 2022-08-20 09:15:02.557416\n",
      "resetting env. episode 456, reward total was -20.0. running mean: -19.651551623575603, timestamp: 2022-08-20 09:15:05.179407\n",
      "resetting env. episode 457, reward total was -20.0. running mean: -19.655036107339846, timestamp: 2022-08-20 09:15:08.843613\n",
      "resetting env. episode 458, reward total was -18.0. running mean: -19.638485746266447, timestamp: 2022-08-20 09:15:12.856892\n",
      "resetting env. episode 459, reward total was -20.0. running mean: -19.642100888803782, timestamp: 2022-08-20 09:15:15.910723\n",
      "resetting env. episode 460, reward total was -17.0. running mean: -19.615679879915746, timestamp: 2022-08-20 09:15:20.648086\n",
      "resetting env. episode 461, reward total was -20.0. running mean: -19.619523081116586, timestamp: 2022-08-20 09:15:24.786995\n",
      "resetting env. episode 462, reward total was -20.0. running mean: -19.62332785030542, timestamp: 2022-08-20 09:15:28.458184\n",
      "resetting env. episode 463, reward total was -21.0. running mean: -19.637094571802365, timestamp: 2022-08-20 09:15:32.202175\n",
      "resetting env. episode 464, reward total was -21.0. running mean: -19.650723626084343, timestamp: 2022-08-20 09:15:35.575158\n",
      "resetting env. episode 465, reward total was -19.0. running mean: -19.6442163898235, timestamp: 2022-08-20 09:15:40.457110\n",
      "resetting env. episode 466, reward total was -19.0. running mean: -19.637774225925266, timestamp: 2022-08-20 09:15:44.042526\n",
      "resetting env. episode 467, reward total was -19.0. running mean: -19.631396483666016, timestamp: 2022-08-20 09:15:47.847355\n",
      "resetting env. episode 468, reward total was -21.0. running mean: -19.645082518829355, timestamp: 2022-08-20 09:15:52.746260\n",
      "resetting env. episode 469, reward total was -17.0. running mean: -19.618631693641063, timestamp: 2022-08-20 09:15:57.221298\n",
      "resetting env. episode 470, reward total was -20.0. running mean: -19.62244537670465, timestamp: 2022-08-20 09:16:02.365577\n",
      "resetting env. episode 471, reward total was -20.0. running mean: -19.626220922937602, timestamp: 2022-08-20 09:16:06.795708\n",
      "resetting env. episode 472, reward total was -17.0. running mean: -19.599958713708226, timestamp: 2022-08-20 09:16:11.388429\n",
      "resetting env. episode 473, reward total was -18.0. running mean: -19.583959126571145, timestamp: 2022-08-20 09:16:15.252102\n",
      "resetting env. episode 474, reward total was -19.0. running mean: -19.578119535305433, timestamp: 2022-08-20 09:16:21.396677\n",
      "resetting env. episode 475, reward total was -20.0. running mean: -19.58233833995238, timestamp: 2022-08-20 09:16:27.739723\n",
      "resetting env. episode 476, reward total was -21.0. running mean: -19.596514956552856, timestamp: 2022-08-20 09:16:32.113041\n",
      "resetting env. episode 477, reward total was -19.0. running mean: -19.59054980698733, timestamp: 2022-08-20 09:16:36.325772\n",
      "resetting env. episode 478, reward total was -20.0. running mean: -19.594644308917456, timestamp: 2022-08-20 09:16:41.255601\n",
      "resetting env. episode 479, reward total was -19.0. running mean: -19.588697865828284, timestamp: 2022-08-20 09:16:46.670121\n",
      "resetting env. episode 480, reward total was -19.0. running mean: -19.58281088717, timestamp: 2022-08-20 09:16:50.448027\n",
      "resetting env. episode 481, reward total was -19.0. running mean: -19.5769827782983, timestamp: 2022-08-20 09:16:55.326983\n",
      "resetting env. episode 482, reward total was -20.0. running mean: -19.58121295051532, timestamp: 2022-08-20 09:17:01.446625\n",
      "resetting env. episode 483, reward total was -20.0. running mean: -19.585400821010165, timestamp: 2022-08-20 09:17:06.712549\n",
      "resetting env. episode 484, reward total was -21.0. running mean: -19.599546812800064, timestamp: 2022-08-20 09:17:12.031333\n",
      "resetting env. episode 485, reward total was -21.0. running mean: -19.613551344672064, timestamp: 2022-08-20 09:17:16.156311\n",
      "resetting env. episode 486, reward total was -18.0. running mean: -19.597415831225344, timestamp: 2022-08-20 09:17:20.794909\n",
      "resetting env. episode 487, reward total was -19.0. running mean: -19.591441672913092, timestamp: 2022-08-20 09:17:25.552191\n",
      "resetting env. episode 488, reward total was -19.0. running mean: -19.585527256183962, timestamp: 2022-08-20 09:17:30.430176\n",
      "resetting env. episode 489, reward total was -18.0. running mean: -19.569671983622122, timestamp: 2022-08-20 09:17:36.357307\n",
      "resetting env. episode 490, reward total was -21.0. running mean: -19.583975263785902, timestamp: 2022-08-20 09:17:40.507222\n",
      "resetting env. episode 491, reward total was -19.0. running mean: -19.578135511148044, timestamp: 2022-08-20 09:17:44.431743\n",
      "resetting env. episode 492, reward total was -21.0. running mean: -19.592354156036563, timestamp: 2022-08-20 09:17:49.356561\n",
      "resetting env. episode 493, reward total was -17.0. running mean: -19.5664306144762, timestamp: 2022-08-20 09:17:54.208592\n",
      "resetting env. episode 494, reward total was -16.0. running mean: -19.530766308331437, timestamp: 2022-08-20 09:18:01.599835\n",
      "resetting env. episode 495, reward total was -21.0. running mean: -19.54545864524812, timestamp: 2022-08-20 09:18:06.224476\n",
      "resetting env. episode 496, reward total was -19.0. running mean: -19.540004058795642, timestamp: 2022-08-20 09:18:13.597771\n",
      "resetting env. episode 497, reward total was -19.0. running mean: -19.534604018207688, timestamp: 2022-08-20 09:18:20.263960\n",
      "resetting env. episode 498, reward total was -19.0. running mean: -19.529257978025612, timestamp: 2022-08-20 09:18:27.320096\n",
      "resetting env. episode 499, reward total was -18.0. running mean: -19.513965398245354, timestamp: 2022-08-20 09:18:32.766530\n",
      "resetting env. episode 500, reward total was -18.0. running mean: -19.4988257442629, timestamp: 2022-08-20 09:18:37.226603\n",
      "resetting env. episode 501, reward total was -14.0. running mean: -19.44383748682027, timestamp: 2022-08-20 09:18:43.440003\n",
      "resetting env. episode 502, reward total was -18.0. running mean: -19.42939911195207, timestamp: 2022-08-20 09:18:49.599530\n",
      "resetting env. episode 503, reward total was -21.0. running mean: -19.445105120832547, timestamp: 2022-08-20 09:18:56.330540\n",
      "resetting env. episode 504, reward total was -19.0. running mean: -19.440654069624223, timestamp: 2022-08-20 09:19:02.586815\n",
      "resetting env. episode 505, reward total was -19.0. running mean: -19.436247528927982, timestamp: 2022-08-20 09:19:06.912287\n",
      "resetting env. episode 506, reward total was -16.0. running mean: -19.4018850536387, timestamp: 2022-08-20 09:19:13.499648\n",
      "resetting env. episode 507, reward total was -21.0. running mean: -19.417866203102314, timestamp: 2022-08-20 09:19:17.815111\n",
      "resetting env. episode 508, reward total was -19.0. running mean: -19.41368754107129, timestamp: 2022-08-20 09:19:24.362616\n",
      "resetting env. episode 509, reward total was -18.0. running mean: -19.399550665660577, timestamp: 2022-08-20 09:19:35.036780\n",
      "resetting env. episode 510, reward total was -20.0. running mean: -19.40555515900397, timestamp: 2022-08-20 09:19:42.094933\n",
      "resetting env. episode 511, reward total was -19.0. running mean: -19.401499607413932, timestamp: 2022-08-20 09:19:49.710556\n",
      "resetting env. episode 512, reward total was -21.0. running mean: -19.417484611339795, timestamp: 2022-08-20 09:19:56.623078\n",
      "resetting env. episode 513, reward total was -20.0. running mean: -19.423309765226396, timestamp: 2022-08-20 09:20:01.849109\n",
      "resetting env. episode 514, reward total was -21.0. running mean: -19.43907666757413, timestamp: 2022-08-20 09:20:06.312182\n",
      "resetting env. episode 515, reward total was -19.0. running mean: -19.43468590089839, timestamp: 2022-08-20 09:20:11.758621\n",
      "resetting env. episode 516, reward total was -19.0. running mean: -19.430339041889408, timestamp: 2022-08-20 09:20:16.066110\n",
      "resetting env. episode 517, reward total was -20.0. running mean: -19.43603565147051, timestamp: 2022-08-20 09:20:21.746921\n",
      "resetting env. episode 518, reward total was -17.0. running mean: -19.41167529495581, timestamp: 2022-08-20 09:20:26.482265\n",
      "resetting env. episode 519, reward total was -19.0. running mean: -19.407558542006253, timestamp: 2022-08-20 09:20:34.149536\n",
      "resetting env. episode 520, reward total was -21.0. running mean: -19.42348295658619, timestamp: 2022-08-20 09:20:42.774482\n",
      "resetting env. episode 521, reward total was -19.0. running mean: -19.41924812702033, timestamp: 2022-08-20 09:20:47.068008\n",
      "resetting env. episode 522, reward total was -20.0. running mean: -19.425055645750128, timestamp: 2022-08-20 09:20:51.188990\n",
      "resetting env. episode 523, reward total was -20.0. running mean: -19.430805089292626, timestamp: 2022-08-20 09:20:56.028059\n",
      "resetting env. episode 524, reward total was -19.0. running mean: -19.426497038399702, timestamp: 2022-08-20 09:21:02.115785\n",
      "resetting env. episode 525, reward total was -21.0. running mean: -19.442232068015706, timestamp: 2022-08-20 09:21:06.085170\n",
      "resetting env. episode 526, reward total was -18.0. running mean: -19.427809747335548, timestamp: 2022-08-20 09:21:12.187860\n",
      "resetting env. episode 527, reward total was -18.0. running mean: -19.413531649862193, timestamp: 2022-08-20 09:21:17.488690\n",
      "resetting env. episode 528, reward total was -20.0. running mean: -19.419396333363572, timestamp: 2022-08-20 09:21:22.010602\n",
      "resetting env. episode 529, reward total was -17.0. running mean: -19.39520237002994, timestamp: 2022-08-20 09:21:30.650511\n",
      "resetting env. episode 530, reward total was -18.0. running mean: -19.38125034632964, timestamp: 2022-08-20 09:21:38.881507\n",
      "resetting env. episode 531, reward total was -15.0. running mean: -19.337437842866343, timestamp: 2022-08-20 09:21:44.480542\n",
      "resetting env. episode 532, reward total was -19.0. running mean: -19.33406346443768, timestamp: 2022-08-20 09:21:50.713879\n",
      "resetting env. episode 533, reward total was -21.0. running mean: -19.350722829793305, timestamp: 2022-08-20 09:21:55.617772\n",
      "resetting env. episode 534, reward total was -21.0. running mean: -19.367215601495374, timestamp: 2022-08-20 09:21:59.338837\n",
      "resetting env. episode 535, reward total was -20.0. running mean: -19.37354344548042, timestamp: 2022-08-20 09:22:03.765989\n",
      "resetting env. episode 536, reward total was -19.0. running mean: -19.369808011025615, timestamp: 2022-08-20 09:22:09.289227\n",
      "resetting env. episode 537, reward total was -17.0. running mean: -19.34610993091536, timestamp: 2022-08-20 09:22:14.923167\n",
      "resetting env. episode 538, reward total was -19.0. running mean: -19.34264883160621, timestamp: 2022-08-20 09:22:19.938761\n",
      "resetting env. episode 539, reward total was -19.0. running mean: -19.339222343290146, timestamp: 2022-08-20 09:22:26.879209\n",
      "resetting env. episode 540, reward total was -20.0. running mean: -19.345830119857244, timestamp: 2022-08-20 09:22:31.926716\n",
      "resetting env. episode 541, reward total was -21.0. running mean: -19.362371818658673, timestamp: 2022-08-20 09:22:36.112538\n",
      "resetting env. episode 542, reward total was -20.0. running mean: -19.368748100472086, timestamp: 2022-08-20 09:22:41.498153\n",
      "resetting env. episode 543, reward total was -17.0. running mean: -19.345060619467368, timestamp: 2022-08-20 09:22:46.598500\n",
      "resetting env. episode 544, reward total was -19.0. running mean: -19.341610013272696, timestamp: 2022-08-20 09:22:54.110422\n",
      "resetting env. episode 545, reward total was -17.0. running mean: -19.31819391313997, timestamp: 2022-08-20 09:23:05.065153\n",
      "resetting env. episode 546, reward total was -17.0. running mean: -19.295011974008574, timestamp: 2022-08-20 09:23:11.859976\n",
      "resetting env. episode 547, reward total was -20.0. running mean: -19.302061854268487, timestamp: 2022-08-20 09:23:16.511543\n",
      "resetting env. episode 548, reward total was -19.0. running mean: -19.299041235725802, timestamp: 2022-08-20 09:23:21.441367\n",
      "resetting env. episode 549, reward total was -19.0. running mean: -19.296050823368546, timestamp: 2022-08-20 09:23:33.047343\n",
      "resetting env. episode 550, reward total was -20.0. running mean: -19.30309031513486, timestamp: 2022-08-20 09:23:41.294299\n",
      "resetting env. episode 551, reward total was -17.0. running mean: -19.280059411983512, timestamp: 2022-08-20 09:23:50.730075\n",
      "resetting env. episode 552, reward total was -15.0. running mean: -19.237258817863676, timestamp: 2022-08-20 09:24:05.188430\n",
      "resetting env. episode 553, reward total was -19.0. running mean: -19.234886229685042, timestamp: 2022-08-20 09:24:11.189386\n",
      "resetting env. episode 554, reward total was -18.0. running mean: -19.222537367388192, timestamp: 2022-08-20 09:24:18.603569\n",
      "resetting env. episode 555, reward total was -20.0. running mean: -19.23031199371431, timestamp: 2022-08-20 09:24:23.798683\n",
      "resetting env. episode 556, reward total was -19.0. running mean: -19.228008873777167, timestamp: 2022-08-20 09:24:29.154367\n",
      "resetting env. episode 557, reward total was -19.0. running mean: -19.225728785039397, timestamp: 2022-08-20 09:24:33.463864\n",
      "resetting env. episode 558, reward total was -20.0. running mean: -19.233471497189, timestamp: 2022-08-20 09:24:37.838154\n",
      "resetting env. episode 559, reward total was -17.0. running mean: -19.211136782217114, timestamp: 2022-08-20 09:24:43.228747\n",
      "resetting env. episode 560, reward total was -21.0. running mean: -19.229025414394943, timestamp: 2022-08-20 09:24:51.880619\n",
      "resetting env. episode 561, reward total was -20.0. running mean: -19.236735160250994, timestamp: 2022-08-20 09:24:59.349654\n",
      "resetting env. episode 562, reward total was -17.0. running mean: -19.214367808648486, timestamp: 2022-08-20 09:25:06.144494\n",
      "resetting env. episode 563, reward total was -18.0. running mean: -19.202224130562, timestamp: 2022-08-20 09:25:10.522790\n",
      "resetting env. episode 564, reward total was -21.0. running mean: -19.220201889256384, timestamp: 2022-08-20 09:25:16.247518\n",
      "resetting env. episode 565, reward total was -20.0. running mean: -19.227999870363817, timestamp: 2022-08-20 09:25:23.523063\n",
      "resetting env. episode 566, reward total was -20.0. running mean: -19.235719871660176, timestamp: 2022-08-20 09:25:32.848113\n",
      "resetting env. episode 567, reward total was -19.0. running mean: -19.233362672943574, timestamp: 2022-08-20 09:25:38.768289\n",
      "resetting env. episode 568, reward total was -18.0. running mean: -19.221029046214138, timestamp: 2022-08-20 09:25:45.158208\n",
      "resetting env. episode 569, reward total was -17.0. running mean: -19.198818755751997, timestamp: 2022-08-20 09:25:49.516561\n",
      "resetting env. episode 570, reward total was -21.0. running mean: -19.216830568194478, timestamp: 2022-08-20 09:25:54.058420\n",
      "resetting env. episode 571, reward total was -19.0. running mean: -19.214662262512533, timestamp: 2022-08-20 09:25:58.816699\n",
      "resetting env. episode 572, reward total was -19.0. running mean: -19.21251563988741, timestamp: 2022-08-20 09:26:04.800705\n",
      "resetting env. episode 573, reward total was -19.0. running mean: -19.21039048348854, timestamp: 2022-08-20 09:26:09.011448\n",
      "resetting env. episode 574, reward total was -21.0. running mean: -19.228286578653655, timestamp: 2022-08-20 09:26:13.078580\n",
      "resetting env. episode 575, reward total was -17.0. running mean: -19.20600371286712, timestamp: 2022-08-20 09:26:19.075552\n",
      "resetting env. episode 576, reward total was -17.0. running mean: -19.18394367573845, timestamp: 2022-08-20 09:26:24.258695\n",
      "resetting env. episode 577, reward total was -19.0. running mean: -19.18210423898107, timestamp: 2022-08-20 09:26:28.327818\n",
      "resetting env. episode 578, reward total was -14.0. running mean: -19.130283196591257, timestamp: 2022-08-20 09:26:34.293870\n",
      "resetting env. episode 579, reward total was -19.0. running mean: -19.128980364625345, timestamp: 2022-08-20 09:26:38.911542\n",
      "resetting env. episode 580, reward total was -18.0. running mean: -19.117690560979092, timestamp: 2022-08-20 09:26:44.124591\n",
      "resetting env. episode 581, reward total was -20.0. running mean: -19.1265136553693, timestamp: 2022-08-20 09:26:49.086329\n",
      "resetting env. episode 582, reward total was -18.0. running mean: -19.11524851881561, timestamp: 2022-08-20 09:26:55.078315\n",
      "resetting env. episode 583, reward total was -16.0. running mean: -19.084096033627453, timestamp: 2022-08-20 09:27:00.517772\n",
      "resetting env. episode 584, reward total was -18.0. running mean: -19.07325507329118, timestamp: 2022-08-20 09:27:05.748790\n",
      "resetting env. episode 585, reward total was -20.0. running mean: -19.082522522558268, timestamp: 2022-08-20 09:27:10.115141\n",
      "resetting env. episode 586, reward total was -21.0. running mean: -19.101697297332684, timestamp: 2022-08-20 09:27:16.070201\n",
      "resetting env. episode 587, reward total was -17.0. running mean: -19.08068032435936, timestamp: 2022-08-20 09:27:21.148625\n",
      "resetting env. episode 588, reward total was -17.0. running mean: -19.05987352111577, timestamp: 2022-08-20 09:27:27.539553\n",
      "resetting env. episode 589, reward total was -20.0. running mean: -19.06927478590461, timestamp: 2022-08-20 09:27:32.191109\n",
      "resetting env. episode 590, reward total was -19.0. running mean: -19.068582038045566, timestamp: 2022-08-20 09:27:36.666149\n",
      "resetting env. episode 591, reward total was -16.0. running mean: -19.03789621766511, timestamp: 2022-08-20 09:27:43.981593\n",
      "resetting env. episode 592, reward total was -16.0. running mean: -19.00751725548846, timestamp: 2022-08-20 09:27:50.413401\n",
      "resetting env. episode 593, reward total was -18.0. running mean: -18.997442082933574, timestamp: 2022-08-20 09:27:56.119182\n",
      "resetting env. episode 594, reward total was -20.0. running mean: -19.007467662104236, timestamp: 2022-08-20 09:28:02.343516\n",
      "resetting env. episode 595, reward total was -19.0. running mean: -19.007392985483197, timestamp: 2022-08-20 09:28:08.148996\n",
      "resetting env. episode 596, reward total was -17.0. running mean: -18.987319055628365, timestamp: 2022-08-20 09:28:14.440192\n",
      "resetting env. episode 597, reward total was -17.0. running mean: -18.967445865072083, timestamp: 2022-08-20 09:28:21.199113\n",
      "resetting env. episode 598, reward total was -20.0. running mean: -18.97777140642136, timestamp: 2022-08-20 09:28:26.581725\n",
      "resetting env. episode 599, reward total was -19.0. running mean: -18.97799369235715, timestamp: 2022-08-20 09:28:33.754551\n",
      "resetting env. episode 600, reward total was -18.0. running mean: -18.968213755433577, timestamp: 2022-08-20 09:28:39.476256\n",
      "resetting env. episode 601, reward total was -21.0. running mean: -18.988531617879243, timestamp: 2022-08-20 09:28:45.065318\n",
      "resetting env. episode 602, reward total was -19.0. running mean: -18.988646301700452, timestamp: 2022-08-20 09:28:49.996138\n",
      "resetting env. episode 603, reward total was -20.0. running mean: -18.998759838683448, timestamp: 2022-08-20 09:28:55.627099\n",
      "resetting env. episode 604, reward total was -15.0. running mean: -18.95877224029661, timestamp: 2022-08-20 09:29:02.650312\n",
      "resetting env. episode 605, reward total was -17.0. running mean: -18.939184517893647, timestamp: 2022-08-20 09:29:07.895295\n",
      "resetting env. episode 606, reward total was -20.0. running mean: -18.94979267271471, timestamp: 2022-08-20 09:29:13.079486\n",
      "resetting env. episode 607, reward total was -20.0. running mean: -18.960294745987564, timestamp: 2022-08-20 09:29:18.533854\n",
      "resetting env. episode 608, reward total was -21.0. running mean: -18.98069179852769, timestamp: 2022-08-20 09:29:24.151842\n",
      "resetting env. episode 609, reward total was -17.0. running mean: -18.960884880542416, timestamp: 2022-08-20 09:29:31.411433\n",
      "resetting env. episode 610, reward total was -20.0. running mean: -18.97127603173699, timestamp: 2022-08-20 09:29:36.681348\n",
      "resetting env. episode 611, reward total was -17.0. running mean: -18.951563271419623, timestamp: 2022-08-20 09:29:44.991033\n",
      "resetting env. episode 612, reward total was -18.0. running mean: -18.942047638705425, timestamp: 2022-08-20 09:29:51.050827\n",
      "resetting env. episode 613, reward total was -16.0. running mean: -18.91262716231837, timestamp: 2022-08-20 09:29:56.326720\n",
      "resetting env. episode 614, reward total was -21.0. running mean: -18.933500890695186, timestamp: 2022-08-20 09:30:00.614260\n",
      "resetting env. episode 615, reward total was -20.0. running mean: -18.944165881788233, timestamp: 2022-08-20 09:30:05.590962\n",
      "resetting env. episode 616, reward total was -17.0. running mean: -18.92472422297035, timestamp: 2022-08-20 09:30:11.189990\n",
      "resetting env. episode 617, reward total was -19.0. running mean: -18.925476980740648, timestamp: 2022-08-20 09:30:16.148737\n",
      "resetting env. episode 618, reward total was -21.0. running mean: -18.946222210933243, timestamp: 2022-08-20 09:30:21.388747\n",
      "resetting env. episode 619, reward total was -19.0. running mean: -18.94675998882391, timestamp: 2022-08-20 09:30:26.202863\n",
      "resetting env. episode 620, reward total was -20.0. running mean: -18.95729238893567, timestamp: 2022-08-20 09:30:31.031954\n",
      "resetting env. episode 621, reward total was -19.0. running mean: -18.957719465046313, timestamp: 2022-08-20 09:30:35.916906\n",
      "resetting env. episode 622, reward total was -17.0. running mean: -18.93814227039585, timestamp: 2022-08-20 09:30:44.443107\n",
      "resetting env. episode 623, reward total was -13.0. running mean: -18.87876084769189, timestamp: 2022-08-20 09:30:53.022174\n",
      "resetting env. episode 624, reward total was -19.0. running mean: -18.879973239214973, timestamp: 2022-08-20 09:30:58.952326\n",
      "resetting env. episode 625, reward total was -19.0. running mean: -18.881173506822826, timestamp: 2022-08-20 09:31:06.804337\n",
      "resetting env. episode 626, reward total was -18.0. running mean: -18.872361771754598, timestamp: 2022-08-20 09:31:12.970850\n",
      "resetting env. episode 627, reward total was -21.0. running mean: -18.893638154037053, timestamp: 2022-08-20 09:31:22.126380\n",
      "resetting env. episode 628, reward total was -17.0. running mean: -18.874701772496685, timestamp: 2022-08-20 09:31:36.099031\n",
      "resetting env. episode 629, reward total was -19.0. running mean: -18.87595475477172, timestamp: 2022-08-20 09:31:46.689724\n",
      "resetting env. episode 630, reward total was -20.0. running mean: -18.887195207224, timestamp: 2022-08-20 09:31:52.869226\n",
      "resetting env. episode 631, reward total was -15.0. running mean: -18.848323255151758, timestamp: 2022-08-20 09:31:58.763447\n",
      "resetting env. episode 632, reward total was -20.0. running mean: -18.85984002260024, timestamp: 2022-08-20 09:32:05.053633\n",
      "resetting env. episode 633, reward total was -16.0. running mean: -18.831241622374236, timestamp: 2022-08-20 09:32:11.661968\n",
      "resetting env. episode 634, reward total was -17.0. running mean: -18.812929206150496, timestamp: 2022-08-20 09:32:16.598775\n",
      "resetting env. episode 635, reward total was -21.0. running mean: -18.834799914088993, timestamp: 2022-08-20 09:32:22.159908\n",
      "resetting env. episode 636, reward total was -15.0. running mean: -18.7964519149481, timestamp: 2022-08-20 09:32:31.015242\n",
      "resetting env. episode 637, reward total was -17.0. running mean: -18.77848739579862, timestamp: 2022-08-20 09:32:35.982959\n",
      "resetting env. episode 638, reward total was -16.0. running mean: -18.750702521840633, timestamp: 2022-08-20 09:32:42.049751\n",
      "resetting env. episode 639, reward total was -19.0. running mean: -18.75319549662223, timestamp: 2022-08-20 09:32:47.647780\n",
      "resetting env. episode 640, reward total was -19.0. running mean: -18.75566354165601, timestamp: 2022-08-20 09:32:54.848532\n",
      "resetting env. episode 641, reward total was -17.0. running mean: -18.73810690623945, timestamp: 2022-08-20 09:33:03.745755\n",
      "resetting env. episode 642, reward total was -17.0. running mean: -18.72072583717706, timestamp: 2022-08-20 09:33:11.183869\n",
      "resetting env. episode 643, reward total was -19.0. running mean: -18.72351857880529, timestamp: 2022-08-20 09:33:22.881598\n",
      "resetting env. episode 644, reward total was -19.0. running mean: -18.726283393017237, timestamp: 2022-08-20 09:33:33.909129\n",
      "resetting env. episode 645, reward total was -20.0. running mean: -18.739020559087063, timestamp: 2022-08-20 09:33:41.666392\n",
      "resetting env. episode 646, reward total was -18.0. running mean: -18.731630353496193, timestamp: 2022-08-20 09:33:49.576245\n",
      "resetting env. episode 647, reward total was -15.0. running mean: -18.69431404996123, timestamp: 2022-08-20 09:33:56.891695\n",
      "resetting env. episode 648, reward total was -19.0. running mean: -18.69737090946162, timestamp: 2022-08-20 09:34:02.825841\n",
      "resetting env. episode 649, reward total was -19.0. running mean: -18.700397200367004, timestamp: 2022-08-20 09:34:08.211433\n",
      "resetting env. episode 650, reward total was -19.0. running mean: -18.703393228363336, timestamp: 2022-08-20 09:34:13.674831\n",
      "resetting env. episode 651, reward total was -16.0. running mean: -18.676359296079703, timestamp: 2022-08-20 09:34:20.745937\n",
      "resetting env. episode 652, reward total was -21.0. running mean: -18.699595703118906, timestamp: 2022-08-20 09:34:25.779494\n",
      "resetting env. episode 653, reward total was -18.0. running mean: -18.692599746087716, timestamp: 2022-08-20 09:34:33.494852\n",
      "resetting env. episode 654, reward total was -17.0. running mean: -18.67567374862684, timestamp: 2022-08-20 09:34:40.219873\n",
      "resetting env. episode 655, reward total was -16.0. running mean: -18.64891701114057, timestamp: 2022-08-20 09:34:46.233798\n",
      "resetting env. episode 656, reward total was -15.0. running mean: -18.61242784102916, timestamp: 2022-08-20 09:34:52.208829\n",
      "resetting env. episode 657, reward total was -19.0. running mean: -18.616303562618867, timestamp: 2022-08-20 09:34:57.775946\n",
      "resetting env. episode 658, reward total was -18.0. running mean: -18.610140526992677, timestamp: 2022-08-20 09:35:09.425849\n",
      "resetting env. episode 659, reward total was -19.0. running mean: -18.61403912172275, timestamp: 2022-08-20 09:35:15.551436\n",
      "resetting env. episode 660, reward total was -17.0. running mean: -18.597898730505523, timestamp: 2022-08-20 09:35:22.882847\n",
      "resetting env. episode 661, reward total was -18.0. running mean: -18.591919743200467, timestamp: 2022-08-20 09:35:30.338906\n",
      "resetting env. episode 662, reward total was -18.0. running mean: -18.586000545768464, timestamp: 2022-08-20 09:35:37.515724\n",
      "resetting env. episode 663, reward total was -18.0. running mean: -18.58014054031078, timestamp: 2022-08-20 09:35:43.378063\n",
      "resetting env. episode 664, reward total was -21.0. running mean: -18.604339134907672, timestamp: 2022-08-20 09:35:52.063835\n",
      "resetting env. episode 665, reward total was -20.0. running mean: -18.618295743558594, timestamp: 2022-08-20 09:35:57.518255\n",
      "resetting env. episode 666, reward total was -21.0. running mean: -18.642112786123008, timestamp: 2022-08-20 09:36:02.399210\n",
      "resetting env. episode 667, reward total was -19.0. running mean: -18.64569165826178, timestamp: 2022-08-20 09:36:06.753569\n",
      "resetting env. episode 668, reward total was -16.0. running mean: -18.61923474167916, timestamp: 2022-08-20 09:36:13.534447\n",
      "resetting env. episode 669, reward total was -18.0. running mean: -18.61304239426237, timestamp: 2022-08-20 09:36:20.132808\n",
      "resetting env. episode 670, reward total was -16.0. running mean: -18.586911970319743, timestamp: 2022-08-20 09:36:27.987812\n",
      "resetting env. episode 671, reward total was -13.0. running mean: -18.531042850616544, timestamp: 2022-08-20 09:36:33.903996\n",
      "resetting env. episode 672, reward total was -20.0. running mean: -18.545732422110376, timestamp: 2022-08-20 09:36:38.312213\n",
      "resetting env. episode 673, reward total was -17.0. running mean: -18.530275097889273, timestamp: 2022-08-20 09:36:43.452474\n",
      "resetting env. episode 674, reward total was -21.0. running mean: -18.554972346910382, timestamp: 2022-08-20 09:36:47.306178\n",
      "resetting env. episode 675, reward total was -19.0. running mean: -18.55942262344128, timestamp: 2022-08-20 09:36:54.255596\n",
      "resetting env. episode 676, reward total was -20.0. running mean: -18.573828397206864, timestamp: 2022-08-20 09:36:59.771885\n",
      "resetting env. episode 677, reward total was -17.0. running mean: -18.558090113234798, timestamp: 2022-08-20 09:37:08.710962\n",
      "resetting env. episode 678, reward total was -19.0. running mean: -18.56250921210245, timestamp: 2022-08-20 09:37:17.646075\n",
      "resetting env. episode 679, reward total was -19.0. running mean: -18.566884119981427, timestamp: 2022-08-20 09:37:29.434564\n",
      "resetting env. episode 680, reward total was -17.0. running mean: -18.551215278781616, timestamp: 2022-08-20 09:37:35.417570\n",
      "resetting env. episode 681, reward total was -19.0. running mean: -18.5557031259938, timestamp: 2022-08-20 09:37:44.519241\n",
      "resetting env. episode 682, reward total was -19.0. running mean: -18.560146094733863, timestamp: 2022-08-20 09:37:50.426451\n",
      "resetting env. episode 683, reward total was -19.0. running mean: -18.564544633786525, timestamp: 2022-08-20 09:37:59.901128\n",
      "resetting env. episode 684, reward total was -17.0. running mean: -18.54889918744866, timestamp: 2022-08-20 09:38:05.174030\n",
      "resetting env. episode 685, reward total was -19.0. running mean: -18.553410195574177, timestamp: 2022-08-20 09:38:11.571928\n",
      "resetting env. episode 686, reward total was -21.0. running mean: -18.577876093618436, timestamp: 2022-08-20 09:38:17.231799\n",
      "resetting env. episode 687, reward total was -17.0. running mean: -18.562097332682253, timestamp: 2022-08-20 09:38:23.778329\n",
      "resetting env. episode 688, reward total was -17.0. running mean: -18.54647635935543, timestamp: 2022-08-20 09:38:29.481058\n",
      "resetting env. episode 689, reward total was -21.0. running mean: -18.571011595761878, timestamp: 2022-08-20 09:38:34.143596\n",
      "resetting env. episode 690, reward total was -21.0. running mean: -18.59530147980426, timestamp: 2022-08-20 09:38:39.214043\n",
      "resetting env. episode 691, reward total was -19.0. running mean: -18.59934846500622, timestamp: 2022-08-20 09:38:47.680421\n",
      "resetting env. episode 692, reward total was -16.0. running mean: -18.573354980356157, timestamp: 2022-08-20 09:38:57.504157\n",
      "resetting env. episode 693, reward total was -19.0. running mean: -18.577621430552597, timestamp: 2022-08-20 09:39:06.625772\n",
      "resetting env. episode 694, reward total was -21.0. running mean: -18.60184521624707, timestamp: 2022-08-20 09:39:12.063237\n",
      "resetting env. episode 695, reward total was -21.0. running mean: -18.6258267640846, timestamp: 2022-08-20 09:39:17.713133\n",
      "resetting env. episode 696, reward total was -19.0. running mean: -18.629568496443756, timestamp: 2022-08-20 09:39:22.301870\n",
      "resetting env. episode 697, reward total was -17.0. running mean: -18.61327281147932, timestamp: 2022-08-20 09:39:28.436469\n",
      "resetting env. episode 698, reward total was -19.0. running mean: -18.61714008336453, timestamp: 2022-08-20 09:39:34.043510\n",
      "resetting env. episode 699, reward total was -18.0. running mean: -18.61096868253088, timestamp: 2022-08-20 09:39:39.984602\n",
      "resetting env. episode 700, reward total was -19.0. running mean: -18.614858995705575, timestamp: 2022-08-20 09:39:44.731911\n",
      "resetting env. episode 701, reward total was -18.0. running mean: -18.608710405748518, timestamp: 2022-08-20 09:39:50.434669\n",
      "resetting env. episode 702, reward total was -20.0. running mean: -18.622623301691032, timestamp: 2022-08-20 09:39:55.583911\n",
      "resetting env. episode 703, reward total was -20.0. running mean: -18.63639706867412, timestamp: 2022-08-20 09:40:00.845844\n",
      "resetting env. episode 704, reward total was -20.0. running mean: -18.65003309798738, timestamp: 2022-08-20 09:40:08.080504\n",
      "resetting env. episode 705, reward total was -15.0. running mean: -18.613532767007502, timestamp: 2022-08-20 09:40:13.984719\n",
      "resetting env. episode 706, reward total was -19.0. running mean: -18.617397439337427, timestamp: 2022-08-20 09:40:20.014603\n",
      "resetting env. episode 707, reward total was -15.0. running mean: -18.581223464944053, timestamp: 2022-08-20 09:40:26.548138\n",
      "resetting env. episode 708, reward total was -17.0. running mean: -18.565411230294615, timestamp: 2022-08-20 09:40:32.193047\n",
      "resetting env. episode 709, reward total was -17.0. running mean: -18.54975711799167, timestamp: 2022-08-20 09:40:38.543075\n",
      "resetting env. episode 710, reward total was -20.0. running mean: -18.564259546811755, timestamp: 2022-08-20 09:40:43.483870\n",
      "resetting env. episode 711, reward total was -17.0. running mean: -18.54861695134364, timestamp: 2022-08-20 09:40:50.596861\n",
      "resetting env. episode 712, reward total was -19.0. running mean: -18.553130781830205, timestamp: 2022-08-20 09:40:56.297618\n",
      "resetting env. episode 713, reward total was -17.0. running mean: -18.537599474011905, timestamp: 2022-08-20 09:41:02.134018\n",
      "resetting env. episode 714, reward total was -17.0. running mean: -18.522223479271787, timestamp: 2022-08-20 09:41:07.884645\n",
      "resetting env. episode 715, reward total was -19.0. running mean: -18.52700124447907, timestamp: 2022-08-20 09:41:14.086069\n",
      "resetting env. episode 716, reward total was -12.0. running mean: -18.461731232034282, timestamp: 2022-08-20 09:41:22.122595\n",
      "resetting env. episode 717, reward total was -17.0. running mean: -18.447113919713942, timestamp: 2022-08-20 09:41:27.921087\n",
      "resetting env. episode 718, reward total was -20.0. running mean: -18.462642780516802, timestamp: 2022-08-20 09:41:32.736225\n",
      "resetting env. episode 719, reward total was -17.0. running mean: -18.448016352711637, timestamp: 2022-08-20 09:41:38.907724\n",
      "resetting env. episode 720, reward total was -17.0. running mean: -18.433536189184522, timestamp: 2022-08-20 09:41:46.044643\n",
      "resetting env. episode 721, reward total was -16.0. running mean: -18.409200827292675, timestamp: 2022-08-20 09:41:52.666942\n",
      "resetting env. episode 722, reward total was -16.0. running mean: -18.38510881901975, timestamp: 2022-08-20 09:42:03.128990\n",
      "resetting env. episode 723, reward total was -17.0. running mean: -18.37125773082955, timestamp: 2022-08-20 09:42:13.118789\n",
      "resetting env. episode 724, reward total was -20.0. running mean: -18.387545153521256, timestamp: 2022-08-20 09:42:20.366600\n",
      "resetting env. episode 725, reward total was -18.0. running mean: -18.383669701986044, timestamp: 2022-08-20 09:42:27.777803\n",
      "resetting env. episode 726, reward total was -19.0. running mean: -18.389833004966185, timestamp: 2022-08-20 09:42:34.721242\n",
      "resetting env. episode 727, reward total was -16.0. running mean: -18.365934674916524, timestamp: 2022-08-20 09:42:41.891063\n",
      "resetting env. episode 728, reward total was -20.0. running mean: -18.382275328167356, timestamp: 2022-08-20 09:42:48.200197\n",
      "resetting env. episode 729, reward total was -19.0. running mean: -18.388452574885683, timestamp: 2022-08-20 09:42:53.533947\n",
      "resetting env. episode 730, reward total was -18.0. running mean: -18.384568049136824, timestamp: 2022-08-20 09:42:59.446137\n",
      "resetting env. episode 731, reward total was -19.0. running mean: -18.390722368645456, timestamp: 2022-08-20 09:43:05.051156\n",
      "resetting env. episode 732, reward total was -20.0. running mean: -18.406815144959, timestamp: 2022-08-20 09:43:10.411827\n",
      "resetting env. episode 733, reward total was -16.0. running mean: -18.38274699350941, timestamp: 2022-08-20 09:43:16.372894\n",
      "resetting env. episode 734, reward total was -20.0. running mean: -18.398919523574317, timestamp: 2022-08-20 09:43:23.912738\n",
      "resetting env. episode 735, reward total was -18.0. running mean: -18.394930328338575, timestamp: 2022-08-20 09:43:30.741487\n",
      "resetting env. episode 736, reward total was -18.0. running mean: -18.39098102505519, timestamp: 2022-08-20 09:43:39.676603\n",
      "resetting env. episode 737, reward total was -17.0. running mean: -18.37707121480464, timestamp: 2022-08-20 09:43:45.676574\n",
      "resetting env. episode 738, reward total was -18.0. running mean: -18.373300502656594, timestamp: 2022-08-20 09:43:51.852057\n",
      "resetting env. episode 739, reward total was -18.0. running mean: -18.369567497630026, timestamp: 2022-08-20 09:43:56.993315\n",
      "resetting env. episode 740, reward total was -19.0. running mean: -18.37587182265373, timestamp: 2022-08-20 09:44:05.863605\n",
      "resetting env. episode 741, reward total was -19.0. running mean: -18.38211310442719, timestamp: 2022-08-20 09:44:13.524131\n",
      "resetting env. episode 742, reward total was -17.0. running mean: -18.36829197338292, timestamp: 2022-08-20 09:44:21.914729\n",
      "resetting env. episode 743, reward total was -18.0. running mean: -18.36460905364909, timestamp: 2022-08-20 09:44:29.770701\n",
      "resetting env. episode 744, reward total was -16.0. running mean: -18.3409629631126, timestamp: 2022-08-20 09:44:37.260685\n",
      "resetting env. episode 745, reward total was -17.0. running mean: -18.327553333481475, timestamp: 2022-08-20 09:44:44.501325\n",
      "resetting env. episode 746, reward total was -14.0. running mean: -18.28427780014666, timestamp: 2022-08-20 09:44:52.063121\n",
      "resetting env. episode 747, reward total was -17.0. running mean: -18.271435022145194, timestamp: 2022-08-20 09:45:00.266185\n",
      "resetting env. episode 748, reward total was -18.0. running mean: -18.26872067192374, timestamp: 2022-08-20 09:45:07.555703\n",
      "resetting env. episode 749, reward total was -19.0. running mean: -18.276033465204506, timestamp: 2022-08-20 09:45:12.060659\n",
      "resetting env. episode 750, reward total was -18.0. running mean: -18.27327313055246, timestamp: 2022-08-20 09:45:17.457234\n",
      "resetting env. episode 751, reward total was -18.0. running mean: -18.270540399246936, timestamp: 2022-08-20 09:45:22.327217\n",
      "resetting env. episode 752, reward total was -19.0. running mean: -18.27783499525447, timestamp: 2022-08-20 09:45:27.926250\n",
      "resetting env. episode 753, reward total was -17.0. running mean: -18.265056645301925, timestamp: 2022-08-20 09:45:33.831477\n",
      "resetting env. episode 754, reward total was -16.0. running mean: -18.242406078848905, timestamp: 2022-08-20 09:45:44.022227\n",
      "resetting env. episode 755, reward total was -17.0. running mean: -18.22998201806042, timestamp: 2022-08-20 09:45:53.037129\n",
      "resetting env. episode 756, reward total was -16.0. running mean: -18.207682197879816, timestamp: 2022-08-20 09:46:00.080331\n",
      "resetting env. episode 757, reward total was -18.0. running mean: -18.205605375901015, timestamp: 2022-08-20 09:46:05.643438\n",
      "resetting env. episode 758, reward total was -16.0. running mean: -18.183549322142007, timestamp: 2022-08-20 09:46:09.827250\n",
      "resetting env. episode 759, reward total was -19.0. running mean: -18.191713828920587, timestamp: 2022-08-20 09:46:14.439921\n",
      "resetting env. episode 760, reward total was -16.0. running mean: -18.169796690631383, timestamp: 2022-08-20 09:46:21.318534\n",
      "resetting env. episode 761, reward total was -20.0. running mean: -18.18809872372507, timestamp: 2022-08-20 09:46:25.003683\n",
      "resetting env. episode 762, reward total was -20.0. running mean: -18.20621773648782, timestamp: 2022-08-20 09:46:29.655248\n",
      "resetting env. episode 763, reward total was -21.0. running mean: -18.234155559122943, timestamp: 2022-08-20 09:46:34.199105\n",
      "resetting env. episode 764, reward total was -16.0. running mean: -18.211814003531714, timestamp: 2022-08-20 09:46:42.802107\n",
      "resetting env. episode 765, reward total was -16.0. running mean: -18.189695863496397, timestamp: 2022-08-20 09:46:51.510856\n",
      "resetting env. episode 766, reward total was -18.0. running mean: -18.18779890486143, timestamp: 2022-08-20 09:46:57.598556\n",
      "resetting env. episode 767, reward total was -18.0. running mean: -18.185920915812815, timestamp: 2022-08-20 09:47:02.907366\n",
      "resetting env. episode 768, reward total was -17.0. running mean: -18.17406170665469, timestamp: 2022-08-20 09:47:08.123423\n",
      "resetting env. episode 769, reward total was -13.0. running mean: -18.12232108958814, timestamp: 2022-08-20 09:47:17.854413\n",
      "resetting env. episode 770, reward total was -20.0. running mean: -18.141097878692257, timestamp: 2022-08-20 09:47:21.375001\n",
      "resetting env. episode 771, reward total was -18.0. running mean: -18.139686899905335, timestamp: 2022-08-20 09:47:26.088406\n",
      "resetting env. episode 772, reward total was -14.0. running mean: -18.09829003090628, timestamp: 2022-08-20 09:47:34.636553\n",
      "resetting env. episode 773, reward total was -18.0. running mean: -18.09730713059722, timestamp: 2022-08-20 09:47:39.720965\n",
      "resetting env. episode 774, reward total was -19.0. running mean: -18.10633405929125, timestamp: 2022-08-20 09:47:46.801039\n",
      "resetting env. episode 775, reward total was -16.0. running mean: -18.085270718698336, timestamp: 2022-08-20 09:47:53.741487\n",
      "resetting env. episode 776, reward total was -13.0. running mean: -18.034418011511352, timestamp: 2022-08-20 09:48:00.865443\n",
      "resetting env. episode 777, reward total was -21.0. running mean: -18.06407383139624, timestamp: 2022-08-20 09:48:04.570543\n",
      "resetting env. episode 778, reward total was -17.0. running mean: -18.05343309308228, timestamp: 2022-08-20 09:48:09.655947\n",
      "resetting env. episode 779, reward total was -21.0. running mean: -18.08289876215146, timestamp: 2022-08-20 09:48:15.163225\n",
      "resetting env. episode 780, reward total was -17.0. running mean: -18.072069774529947, timestamp: 2022-08-20 09:48:20.924824\n",
      "resetting env. episode 781, reward total was -18.0. running mean: -18.071349076784646, timestamp: 2022-08-20 09:48:26.976647\n",
      "resetting env. episode 782, reward total was -16.0. running mean: -18.0506355860168, timestamp: 2022-08-20 09:48:32.371227\n",
      "resetting env. episode 783, reward total was -19.0. running mean: -18.060129230156633, timestamp: 2022-08-20 09:48:37.227248\n",
      "resetting env. episode 784, reward total was -16.0. running mean: -18.039527937855066, timestamp: 2022-08-20 09:48:43.086589\n",
      "resetting env. episode 785, reward total was -17.0. running mean: -18.029132658476517, timestamp: 2022-08-20 09:48:48.449252\n",
      "resetting env. episode 786, reward total was -19.0. running mean: -18.038841331891753, timestamp: 2022-08-20 09:48:53.132733\n",
      "resetting env. episode 787, reward total was -12.0. running mean: -17.978452918572835, timestamp: 2022-08-20 09:48:58.535291\n",
      "resetting env. episode 788, reward total was -16.0. running mean: -17.958668389387107, timestamp: 2022-08-20 09:49:03.607734\n",
      "resetting env. episode 789, reward total was -16.0. running mean: -17.939081705493237, timestamp: 2022-08-20 09:49:09.510966\n",
      "resetting env. episode 790, reward total was -18.0. running mean: -17.939690888438303, timestamp: 2022-08-20 09:49:14.309157\n",
      "resetting env. episode 791, reward total was -19.0. running mean: -17.950293979553923, timestamp: 2022-08-20 09:49:20.169465\n",
      "resetting env. episode 792, reward total was -18.0. running mean: -17.950791039758382, timestamp: 2022-08-20 09:49:25.727607\n",
      "resetting env. episode 793, reward total was -21.0. running mean: -17.9812831293608, timestamp: 2022-08-20 09:49:30.361220\n",
      "resetting env. episode 794, reward total was -17.0. running mean: -17.971470298067196, timestamp: 2022-08-20 09:49:34.637791\n",
      "resetting env. episode 795, reward total was -13.0. running mean: -17.921755595086523, timestamp: 2022-08-20 09:49:40.526051\n",
      "resetting env. episode 796, reward total was -19.0. running mean: -17.932538039135657, timestamp: 2022-08-20 09:49:46.951880\n",
      "resetting env. episode 797, reward total was -19.0. running mean: -17.9432126587443, timestamp: 2022-08-20 09:49:51.786950\n",
      "resetting env. episode 798, reward total was -15.0. running mean: -17.913780532156856, timestamp: 2022-08-20 09:49:59.020680\n",
      "resetting env. episode 799, reward total was -14.0. running mean: -17.874642726835287, timestamp: 2022-08-20 09:50:05.631942\n",
      "resetting env. episode 800, reward total was -17.0. running mean: -17.865896299566934, timestamp: 2022-08-20 09:50:12.460690\n",
      "resetting env. episode 801, reward total was -17.0. running mean: -17.857237336571266, timestamp: 2022-08-20 09:50:18.644194\n",
      "resetting env. episode 802, reward total was -18.0. running mean: -17.858664963205552, timestamp: 2022-08-20 09:50:22.646463\n",
      "resetting env. episode 803, reward total was -18.0. running mean: -17.860078313573496, timestamp: 2022-08-20 09:50:28.712250\n",
      "resetting env. episode 804, reward total was -15.0. running mean: -17.83147753043776, timestamp: 2022-08-20 09:50:33.672990\n",
      "resetting env. episode 805, reward total was -19.0. running mean: -17.84316275513338, timestamp: 2022-08-20 09:50:38.162986\n",
      "resetting env. episode 806, reward total was -20.0. running mean: -17.864731127582047, timestamp: 2022-08-20 09:50:42.431577\n",
      "resetting env. episode 807, reward total was -15.0. running mean: -17.836083816306225, timestamp: 2022-08-20 09:50:48.882337\n",
      "resetting env. episode 808, reward total was -20.0. running mean: -17.857722978143162, timestamp: 2022-08-20 09:50:54.717738\n",
      "resetting env. episode 809, reward total was -15.0. running mean: -17.82914574836173, timestamp: 2022-08-20 09:51:00.091400\n",
      "resetting env. episode 810, reward total was -20.0. running mean: -17.85085429087811, timestamp: 2022-08-20 09:51:04.746929\n",
      "resetting env. episode 811, reward total was -17.0. running mean: -17.842345747969333, timestamp: 2022-08-20 09:51:09.585992\n",
      "resetting env. episode 812, reward total was -17.0. running mean: -17.833922290489642, timestamp: 2022-08-20 09:51:15.127182\n",
      "resetting env. episode 813, reward total was -19.0. running mean: -17.845583067584748, timestamp: 2022-08-20 09:51:19.540401\n",
      "resetting env. episode 814, reward total was -21.0. running mean: -17.8771272369089, timestamp: 2022-08-20 09:51:24.180982\n",
      "resetting env. episode 815, reward total was -12.0. running mean: -17.818355964539812, timestamp: 2022-08-20 09:51:29.864788\n",
      "resetting env. episode 816, reward total was -17.0. running mean: -17.810172404894416, timestamp: 2022-08-20 09:51:34.293949\n",
      "resetting env. episode 817, reward total was -15.0. running mean: -17.78207068084547, timestamp: 2022-08-20 09:51:39.600763\n",
      "resetting env. episode 818, reward total was -13.0. running mean: -17.734249974037017, timestamp: 2022-08-20 09:51:45.454118\n",
      "resetting env. episode 819, reward total was -14.0. running mean: -17.696907474296648, timestamp: 2022-08-20 09:51:50.588422\n",
      "resetting env. episode 820, reward total was -18.0. running mean: -17.69993839955368, timestamp: 2022-08-20 09:51:55.645877\n",
      "resetting env. episode 821, reward total was -19.0. running mean: -17.712939015558145, timestamp: 2022-08-20 09:52:00.413133\n",
      "resetting env. episode 822, reward total was -13.0. running mean: -17.665809625402563, timestamp: 2022-08-20 09:52:06.658437\n",
      "resetting env. episode 823, reward total was -19.0. running mean: -17.679151529148537, timestamp: 2022-08-20 09:52:11.982207\n",
      "resetting env. episode 824, reward total was -20.0. running mean: -17.702360013857053, timestamp: 2022-08-20 09:52:16.040360\n",
      "resetting env. episode 825, reward total was -19.0. running mean: -17.715336413718482, timestamp: 2022-08-20 09:52:19.565936\n",
      "resetting env. episode 826, reward total was -16.0. running mean: -17.698183049581296, timestamp: 2022-08-20 09:52:24.248421\n",
      "resetting env. episode 827, reward total was -17.0. running mean: -17.691201219085485, timestamp: 2022-08-20 09:52:31.063205\n",
      "resetting env. episode 828, reward total was -15.0. running mean: -17.66428920689463, timestamp: 2022-08-20 09:52:35.806535\n",
      "resetting env. episode 829, reward total was -16.0. running mean: -17.647646314825682, timestamp: 2022-08-20 09:52:40.886945\n",
      "resetting env. episode 830, reward total was -16.0. running mean: -17.631169851677424, timestamp: 2022-08-20 09:52:48.062764\n",
      "resetting env. episode 831, reward total was -16.0. running mean: -17.61485815316065, timestamp: 2022-08-20 09:52:52.112939\n",
      "resetting env. episode 832, reward total was -21.0. running mean: -17.648709571629045, timestamp: 2022-08-20 09:52:56.265840\n",
      "resetting env. episode 833, reward total was -16.0. running mean: -17.632222475912755, timestamp: 2022-08-20 09:53:00.158433\n",
      "resetting env. episode 834, reward total was -17.0. running mean: -17.625900251153627, timestamp: 2022-08-20 09:53:05.552016\n",
      "resetting env. episode 835, reward total was -17.0. running mean: -17.619641248642093, timestamp: 2022-08-20 09:53:10.800995\n",
      "resetting env. episode 836, reward total was -17.0. running mean: -17.613444836155672, timestamp: 2022-08-20 09:53:18.404663\n",
      "resetting env. episode 837, reward total was -16.0. running mean: -17.597310387794117, timestamp: 2022-08-20 09:53:25.417914\n",
      "resetting env. episode 838, reward total was -17.0. running mean: -17.59133728391618, timestamp: 2022-08-20 09:53:34.084748\n",
      "resetting env. episode 839, reward total was -14.0. running mean: -17.555423911077018, timestamp: 2022-08-20 09:53:39.992959\n",
      "resetting env. episode 840, reward total was -14.0. running mean: -17.519869671966248, timestamp: 2022-08-20 09:53:45.503225\n",
      "resetting env. episode 841, reward total was -18.0. running mean: -17.524670975246586, timestamp: 2022-08-20 09:53:51.025492\n",
      "resetting env. episode 842, reward total was -16.0. running mean: -17.509424265494122, timestamp: 2022-08-20 09:53:57.540054\n",
      "resetting env. episode 843, reward total was -21.0. running mean: -17.54433002283918, timestamp: 2022-08-20 09:54:02.359170\n",
      "resetting env. episode 844, reward total was -19.0. running mean: -17.55888672261079, timestamp: 2022-08-20 09:54:06.503094\n",
      "resetting env. episode 845, reward total was -19.0. running mean: -17.573297855384684, timestamp: 2022-08-20 09:54:11.527663\n",
      "resetting env. episode 846, reward total was -11.0. running mean: -17.507564876830838, timestamp: 2022-08-20 09:54:17.427894\n",
      "resetting env. episode 847, reward total was -16.0. running mean: -17.492489228062528, timestamp: 2022-08-20 09:54:22.480386\n",
      "resetting env. episode 848, reward total was -16.0. running mean: -17.477564335781903, timestamp: 2022-08-20 09:54:30.549816\n",
      "resetting env. episode 849, reward total was -18.0. running mean: -17.482788692424084, timestamp: 2022-08-20 09:54:36.211684\n",
      "resetting env. episode 850, reward total was -21.0. running mean: -17.517960805499843, timestamp: 2022-08-20 09:54:40.799421\n",
      "resetting env. episode 851, reward total was -16.0. running mean: -17.502781197444843, timestamp: 2022-08-20 09:54:49.572966\n",
      "resetting env. episode 852, reward total was -20.0. running mean: -17.527753385470394, timestamp: 2022-08-20 09:54:54.992481\n",
      "resetting env. episode 853, reward total was -18.0. running mean: -17.53247585161569, timestamp: 2022-08-20 09:54:59.916322\n",
      "resetting env. episode 854, reward total was -21.0. running mean: -17.567151093099532, timestamp: 2022-08-20 09:55:05.640021\n",
      "resetting env. episode 855, reward total was -19.0. running mean: -17.581479582168537, timestamp: 2022-08-20 09:55:13.769290\n",
      "resetting env. episode 856, reward total was -17.0. running mean: -17.575664786346852, timestamp: 2022-08-20 09:55:19.841061\n",
      "resetting env. episode 857, reward total was -14.0. running mean: -17.539908138483383, timestamp: 2022-08-20 09:55:26.362631\n",
      "resetting env. episode 858, reward total was -19.0. running mean: -17.55450905709855, timestamp: 2022-08-20 09:55:32.000582\n",
      "resetting env. episode 859, reward total was -16.0. running mean: -17.538963966527565, timestamp: 2022-08-20 09:55:38.844268\n",
      "resetting env. episode 860, reward total was -17.0. running mean: -17.53357432686229, timestamp: 2022-08-20 09:55:44.312650\n",
      "resetting env. episode 861, reward total was -17.0. running mean: -17.52823858359367, timestamp: 2022-08-20 09:55:53.456208\n",
      "resetting env. episode 862, reward total was -12.0. running mean: -17.472956197757732, timestamp: 2022-08-20 09:56:00.898314\n",
      "resetting env. episode 863, reward total was -18.0. running mean: -17.478226635780153, timestamp: 2022-08-20 09:56:05.967764\n",
      "resetting env. episode 864, reward total was -18.0. running mean: -17.48344436942235, timestamp: 2022-08-20 09:56:10.007965\n",
      "resetting env. episode 865, reward total was -15.0. running mean: -17.458609925728126, timestamp: 2022-08-20 09:56:15.752609\n",
      "resetting env. episode 866, reward total was -18.0. running mean: -17.464023826470843, timestamp: 2022-08-20 09:56:20.501916\n",
      "resetting env. episode 867, reward total was -19.0. running mean: -17.479383588206137, timestamp: 2022-08-20 09:56:24.931081\n",
      "resetting env. episode 868, reward total was -18.0. running mean: -17.484589752324077, timestamp: 2022-08-20 09:56:31.072658\n",
      "resetting env. episode 869, reward total was -17.0. running mean: -17.479743854800837, timestamp: 2022-08-20 09:56:37.061654\n",
      "resetting env. episode 870, reward total was -17.0. running mean: -17.47494641625283, timestamp: 2022-08-20 09:56:42.154040\n",
      "resetting env. episode 871, reward total was -18.0. running mean: -17.480196952090303, timestamp: 2022-08-20 09:56:46.580206\n",
      "resetting env. episode 872, reward total was -16.0. running mean: -17.4653949825694, timestamp: 2022-08-20 09:56:51.663621\n",
      "resetting env. episode 873, reward total was -15.0. running mean: -17.440741032743706, timestamp: 2022-08-20 09:56:57.353418\n",
      "resetting env. episode 874, reward total was -16.0. running mean: -17.42633362241627, timestamp: 2022-08-20 09:57:02.967405\n",
      "resetting env. episode 875, reward total was -17.0. running mean: -17.42207028619211, timestamp: 2022-08-20 09:57:08.074752\n",
      "resetting env. episode 876, reward total was -15.0. running mean: -17.39784958333019, timestamp: 2022-08-20 09:57:13.595993\n",
      "resetting env. episode 877, reward total was -19.0. running mean: -17.413871087496887, timestamp: 2022-08-20 09:57:18.844963\n",
      "resetting env. episode 878, reward total was -16.0. running mean: -17.399732376621916, timestamp: 2022-08-20 09:57:24.892825\n",
      "resetting env. episode 879, reward total was -19.0. running mean: -17.4157350528557, timestamp: 2022-08-20 09:57:30.657416\n",
      "resetting env. episode 880, reward total was -19.0. running mean: -17.431577702327143, timestamp: 2022-08-20 09:57:35.235163\n",
      "resetting env. episode 881, reward total was -16.0. running mean: -17.417261925303873, timestamp: 2022-08-20 09:57:41.716831\n",
      "resetting env. episode 882, reward total was -17.0. running mean: -17.413089306050836, timestamp: 2022-08-20 09:57:47.394650\n",
      "resetting env. episode 883, reward total was -19.0. running mean: -17.42895841299033, timestamp: 2022-08-20 09:57:51.506659\n",
      "resetting env. episode 884, reward total was -17.0. running mean: -17.42466882886043, timestamp: 2022-08-20 09:57:57.728030\n",
      "resetting env. episode 885, reward total was -16.0. running mean: -17.410422140571825, timestamp: 2022-08-20 09:58:03.153528\n",
      "resetting env. episode 886, reward total was -15.0. running mean: -17.386317919166107, timestamp: 2022-08-20 09:58:09.301094\n",
      "resetting env. episode 887, reward total was -16.0. running mean: -17.372454739974447, timestamp: 2022-08-20 09:58:15.105578\n",
      "resetting env. episode 888, reward total was -14.0. running mean: -17.338730192574705, timestamp: 2022-08-20 09:58:19.762133\n",
      "resetting env. episode 889, reward total was -18.0. running mean: -17.345342890648958, timestamp: 2022-08-20 09:58:25.798997\n",
      "resetting env. episode 890, reward total was -16.0. running mean: -17.331889461742467, timestamp: 2022-08-20 09:58:32.055274\n",
      "resetting env. episode 891, reward total was -13.0. running mean: -17.288570567125042, timestamp: 2022-08-20 09:58:37.747067\n",
      "resetting env. episode 892, reward total was -20.0. running mean: -17.31568486145379, timestamp: 2022-08-20 09:58:42.104410\n",
      "resetting env. episode 893, reward total was -14.0. running mean: -17.28252801283925, timestamp: 2022-08-20 09:58:49.811809\n",
      "resetting env. episode 894, reward total was -15.0. running mean: -17.259702732710856, timestamp: 2022-08-20 09:58:55.064770\n",
      "resetting env. episode 895, reward total was -19.0. running mean: -17.27710570538375, timestamp: 2022-08-20 09:58:59.879898\n",
      "resetting env. episode 896, reward total was -14.0. running mean: -17.244334648329914, timestamp: 2022-08-20 09:59:06.588971\n",
      "resetting env. episode 897, reward total was -14.0. running mean: -17.211891301846617, timestamp: 2022-08-20 09:59:12.869176\n",
      "resetting env. episode 898, reward total was -15.0. running mean: -17.18977238882815, timestamp: 2022-08-20 09:59:17.533707\n",
      "resetting env. episode 899, reward total was -14.0. running mean: -17.157874664939868, timestamp: 2022-08-20 09:59:23.254415\n",
      "resetting env. episode 900, reward total was -18.0. running mean: -17.16629591829047, timestamp: 2022-08-20 09:59:28.262061\n",
      "resetting env. episode 901, reward total was -18.0. running mean: -17.174632959107566, timestamp: 2022-08-20 09:59:33.168916\n",
      "resetting env. episode 902, reward total was -20.0. running mean: -17.202886629516488, timestamp: 2022-08-20 09:59:37.156257\n",
      "resetting env. episode 903, reward total was -16.0. running mean: -17.190857763221324, timestamp: 2022-08-20 09:59:43.190129\n",
      "resetting env. episode 904, reward total was -15.0. running mean: -17.16894918558911, timestamp: 2022-08-20 09:59:49.211035\n",
      "resetting env. episode 905, reward total was -18.0. running mean: -17.17725969373322, timestamp: 2022-08-20 09:59:54.024170\n",
      "resetting env. episode 906, reward total was -18.0. running mean: -17.185487096795885, timestamp: 2022-08-20 10:00:00.610565\n",
      "resetting env. episode 907, reward total was -13.0. running mean: -17.143632225827925, timestamp: 2022-08-20 10:00:07.011454\n",
      "resetting env. episode 908, reward total was -15.0. running mean: -17.122195903569644, timestamp: 2022-08-20 10:00:12.717204\n",
      "resetting env. episode 909, reward total was -16.0. running mean: -17.110973944533946, timestamp: 2022-08-20 10:00:18.591500\n",
      "resetting env. episode 910, reward total was -19.0. running mean: -17.12986420508861, timestamp: 2022-08-20 10:00:23.537286\n",
      "resetting env. episode 911, reward total was -19.0. running mean: -17.148565563037725, timestamp: 2022-08-20 10:00:28.938844\n",
      "resetting env. episode 912, reward total was -16.0. running mean: -17.137079907407347, timestamp: 2022-08-20 10:00:34.818127\n",
      "resetting env. episode 913, reward total was -13.0. running mean: -17.095709108333274, timestamp: 2022-08-20 10:00:41.711701\n",
      "resetting env. episode 914, reward total was -15.0. running mean: -17.07475201724994, timestamp: 2022-08-20 10:00:48.101620\n",
      "resetting env. episode 915, reward total was -21.0. running mean: -17.11400449707744, timestamp: 2022-08-20 10:00:51.344949\n",
      "resetting env. episode 916, reward total was -19.0. running mean: -17.132864452106666, timestamp: 2022-08-20 10:00:56.159083\n",
      "resetting env. episode 917, reward total was -16.0. running mean: -17.121535807585598, timestamp: 2022-08-20 10:01:01.510777\n",
      "resetting env. episode 918, reward total was -19.0. running mean: -17.140320449509744, timestamp: 2022-08-20 10:01:05.318602\n",
      "resetting env. episode 919, reward total was -17.0. running mean: -17.13891724501465, timestamp: 2022-08-20 10:01:10.096827\n",
      "resetting env. episode 920, reward total was -15.0. running mean: -17.1175280725645, timestamp: 2022-08-20 10:01:15.676911\n",
      "resetting env. episode 921, reward total was -19.0. running mean: -17.136352791838856, timestamp: 2022-08-20 10:01:21.560185\n",
      "resetting env. episode 922, reward total was -16.0. running mean: -17.12498926392047, timestamp: 2022-08-20 10:01:26.211751\n",
      "resetting env. episode 923, reward total was -17.0. running mean: -17.123739371281264, timestamp: 2022-08-20 10:01:31.712050\n",
      "resetting env. episode 924, reward total was -19.0. running mean: -17.142501977568454, timestamp: 2022-08-20 10:01:37.380898\n",
      "resetting env. episode 925, reward total was -15.0. running mean: -17.121076957792766, timestamp: 2022-08-20 10:01:42.799413\n",
      "resetting env. episode 926, reward total was -18.0. running mean: -17.129866188214837, timestamp: 2022-08-20 10:01:48.050377\n",
      "resetting env. episode 927, reward total was -10.0. running mean: -17.05856752633269, timestamp: 2022-08-20 10:01:55.875460\n",
      "resetting env. episode 928, reward total was -17.0. running mean: -17.057981851069364, timestamp: 2022-08-20 10:02:02.055941\n",
      "resetting env. episode 929, reward total was -15.0. running mean: -17.03740203255867, timestamp: 2022-08-20 10:02:08.174585\n",
      "resetting env. episode 930, reward total was -17.0. running mean: -17.037028012233087, timestamp: 2022-08-20 10:02:12.195840\n",
      "resetting env. episode 931, reward total was -20.0. running mean: -17.066657732110755, timestamp: 2022-08-20 10:02:15.549871\n",
      "resetting env. episode 932, reward total was -18.0. running mean: -17.075991154789648, timestamp: 2022-08-20 10:02:19.058492\n",
      "resetting env. episode 933, reward total was -14.0. running mean: -17.04523124324175, timestamp: 2022-08-20 10:02:24.256598\n",
      "resetting env. episode 934, reward total was -16.0. running mean: -17.034778930809335, timestamp: 2022-08-20 10:02:29.939407\n",
      "resetting env. episode 935, reward total was -13.0. running mean: -16.99443114150124, timestamp: 2022-08-20 10:02:35.276142\n",
      "resetting env. episode 936, reward total was -16.0. running mean: -16.98448683008623, timestamp: 2022-08-20 10:02:39.373191\n",
      "resetting env. episode 937, reward total was -17.0. running mean: -16.984641961785368, timestamp: 2022-08-20 10:02:44.289050\n",
      "resetting env. episode 938, reward total was -13.0. running mean: -16.944795542167512, timestamp: 2022-08-20 10:02:49.571964\n",
      "resetting env. episode 939, reward total was -17.0. running mean: -16.94534758674584, timestamp: 2022-08-20 10:02:53.628087\n",
      "resetting env. episode 940, reward total was -20.0. running mean: -16.97589411087838, timestamp: 2022-08-20 10:02:58.385375\n",
      "resetting env. episode 941, reward total was -19.0. running mean: -16.996135169769595, timestamp: 2022-08-20 10:03:02.657951\n",
      "resetting env. episode 942, reward total was -19.0. running mean: -17.0161738180719, timestamp: 2022-08-20 10:03:06.346092\n",
      "resetting env. episode 943, reward total was -19.0. running mean: -17.03601207989118, timestamp: 2022-08-20 10:03:10.846064\n",
      "resetting env. episode 944, reward total was -17.0. running mean: -17.03565195909227, timestamp: 2022-08-20 10:03:15.159562\n",
      "resetting env. episode 945, reward total was -16.0. running mean: -17.02529543950135, timestamp: 2022-08-20 10:03:19.736300\n",
      "resetting env. episode 946, reward total was -14.0. running mean: -16.995042485106335, timestamp: 2022-08-20 10:03:24.290128\n",
      "resetting env. episode 947, reward total was -18.0. running mean: -17.005092060255272, timestamp: 2022-08-20 10:03:28.318360\n",
      "resetting env. episode 948, reward total was -16.0. running mean: -16.99504113965272, timestamp: 2022-08-20 10:03:32.537084\n",
      "resetting env. episode 949, reward total was -16.0. running mean: -16.985090728256193, timestamp: 2022-08-20 10:03:36.000824\n",
      "resetting env. episode 950, reward total was -19.0. running mean: -17.00523982097363, timestamp: 2022-08-20 10:03:40.472873\n",
      "resetting env. episode 951, reward total was -19.0. running mean: -17.025187422763896, timestamp: 2022-08-20 10:03:45.350831\n",
      "resetting env. episode 952, reward total was -14.0. running mean: -16.994935548536258, timestamp: 2022-08-20 10:03:51.461498\n",
      "resetting env. episode 953, reward total was -15.0. running mean: -16.974986193050896, timestamp: 2022-08-20 10:03:56.256681\n",
      "resetting env. episode 954, reward total was -20.0. running mean: -17.005236331120386, timestamp: 2022-08-20 10:04:00.285910\n",
      "resetting env. episode 955, reward total was -12.0. running mean: -16.955183967809184, timestamp: 2022-08-20 10:04:05.175841\n",
      "resetting env. episode 956, reward total was -21.0. running mean: -16.995632128131092, timestamp: 2022-08-20 10:04:08.850019\n",
      "resetting env. episode 957, reward total was -18.0. running mean: -17.00567580684978, timestamp: 2022-08-20 10:04:12.887227\n",
      "resetting env. episode 958, reward total was -15.0. running mean: -16.98561904878128, timestamp: 2022-08-20 10:04:18.888214\n",
      "resetting env. episode 959, reward total was -17.0. running mean: -16.98576285829347, timestamp: 2022-08-20 10:04:23.855908\n",
      "resetting env. episode 960, reward total was -19.0. running mean: -17.005905229710535, timestamp: 2022-08-20 10:04:28.471570\n",
      "resetting env. episode 961, reward total was -19.0. running mean: -17.02584617741343, timestamp: 2022-08-20 10:04:32.961568\n",
      "resetting env. episode 962, reward total was -14.0. running mean: -16.995587715639296, timestamp: 2022-08-20 10:04:37.778720\n",
      "resetting env. episode 963, reward total was -15.0. running mean: -16.9756318384829, timestamp: 2022-08-20 10:04:43.650996\n",
      "resetting env. episode 964, reward total was -15.0. running mean: -16.955875520098072, timestamp: 2022-08-20 10:04:48.286604\n",
      "resetting env. episode 965, reward total was -14.0. running mean: -16.92631676489709, timestamp: 2022-08-20 10:04:53.102730\n",
      "resetting env. episode 966, reward total was -13.0. running mean: -16.88705359724812, timestamp: 2022-08-20 10:04:57.877967\n",
      "resetting env. episode 967, reward total was -17.0. running mean: -16.88818306127564, timestamp: 2022-08-20 10:05:03.022250\n",
      "resetting env. episode 968, reward total was -13.0. running mean: -16.84930123066288, timestamp: 2022-08-20 10:05:09.712334\n",
      "resetting env. episode 969, reward total was -14.0. running mean: -16.82080821835625, timestamp: 2022-08-20 10:05:14.594284\n",
      "resetting env. episode 970, reward total was -10.0. running mean: -16.752600136172692, timestamp: 2022-08-20 10:05:21.166715\n",
      "resetting env. episode 971, reward total was -17.0. running mean: -16.755074134810968, timestamp: 2022-08-20 10:05:26.567283\n",
      "resetting env. episode 972, reward total was -18.0. running mean: -16.76752339346286, timestamp: 2022-08-20 10:05:29.758749\n",
      "resetting env. episode 973, reward total was -15.0. running mean: -16.74984815952823, timestamp: 2022-08-20 10:05:36.102792\n",
      "resetting env. episode 974, reward total was -13.0. running mean: -16.71234967793295, timestamp: 2022-08-20 10:05:42.635330\n",
      "resetting env. episode 975, reward total was -15.0. running mean: -16.69522618115362, timestamp: 2022-08-20 10:05:48.451782\n",
      "resetting env. episode 976, reward total was -17.0. running mean: -16.698273919342082, timestamp: 2022-08-20 10:05:52.692450\n",
      "resetting env. episode 977, reward total was -15.0. running mean: -16.68129118014866, timestamp: 2022-08-20 10:05:57.061769\n",
      "resetting env. episode 978, reward total was -17.0. running mean: -16.684478268347174, timestamp: 2022-08-20 10:06:02.442388\n",
      "resetting env. episode 979, reward total was -16.0. running mean: -16.6776334856637, timestamp: 2022-08-20 10:06:09.033770\n",
      "resetting env. episode 980, reward total was -16.0. running mean: -16.670857150807063, timestamp: 2022-08-20 10:06:14.621830\n",
      "resetting env. episode 981, reward total was -14.0. running mean: -16.644148579298992, timestamp: 2022-08-20 10:06:19.147741\n",
      "resetting env. episode 982, reward total was -17.0. running mean: -16.647707093506003, timestamp: 2022-08-20 10:06:24.365786\n",
      "resetting env. episode 983, reward total was -13.0. running mean: -16.611230022570943, timestamp: 2022-08-20 10:06:30.841474\n",
      "resetting env. episode 984, reward total was -11.0. running mean: -16.555117722345233, timestamp: 2022-08-20 10:06:37.217434\n",
      "resetting env. episode 985, reward total was -20.0. running mean: -16.58956654512178, timestamp: 2022-08-20 10:06:42.991002\n",
      "resetting env. episode 986, reward total was -18.0. running mean: -16.603670879670563, timestamp: 2022-08-20 10:06:48.062445\n",
      "resetting env. episode 987, reward total was -17.0. running mean: -16.607634170873858, timestamp: 2022-08-20 10:06:52.768862\n",
      "resetting env. episode 988, reward total was -19.0. running mean: -16.63155782916512, timestamp: 2022-08-20 10:06:57.991905\n",
      "resetting env. episode 989, reward total was -20.0. running mean: -16.66524225087347, timestamp: 2022-08-20 10:07:01.987222\n",
      "resetting env. episode 990, reward total was -19.0. running mean: -16.688589828364734, timestamp: 2022-08-20 10:07:06.552021\n",
      "resetting env. episode 991, reward total was -14.0. running mean: -16.661703930081085, timestamp: 2022-08-20 10:07:12.752451\n",
      "resetting env. episode 992, reward total was -15.0. running mean: -16.645086890780274, timestamp: 2022-08-20 10:07:18.597822\n",
      "resetting env. episode 993, reward total was -14.0. running mean: -16.618636021872472, timestamp: 2022-08-20 10:07:23.894664\n",
      "resetting env. episode 994, reward total was -17.0. running mean: -16.62244966165375, timestamp: 2022-08-20 10:07:28.563186\n",
      "resetting env. episode 995, reward total was -19.0. running mean: -16.646225165037215, timestamp: 2022-08-20 10:07:32.862693\n",
      "resetting env. episode 996, reward total was -21.0. running mean: -16.689762913386843, timestamp: 2022-08-20 10:07:38.263257\n",
      "resetting env. episode 997, reward total was -18.0. running mean: -16.702865284252976, timestamp: 2022-08-20 10:07:43.231975\n",
      "resetting env. episode 998, reward total was -13.0. running mean: -16.665836631410446, timestamp: 2022-08-20 10:07:49.651817\n",
      "resetting env. episode 999, reward total was -16.0. running mean: -16.65917826509634, timestamp: 2022-08-20 10:07:54.380175\n",
      "resetting env. episode 1000, reward total was -15.0. running mean: -16.642586482445374, timestamp: 2022-08-20 10:07:59.987189\n",
      "resetting env. episode 1001, reward total was -15.0. running mean: -16.62616061762092, timestamp: 2022-08-20 10:08:04.229849\n",
      "resetting env. episode 1002, reward total was -17.0. running mean: -16.62989901144471, timestamp: 2022-08-20 10:08:09.662327\n",
      "resetting env. episode 1003, reward total was -14.0. running mean: -16.603600021330262, timestamp: 2022-08-20 10:08:15.558594\n",
      "resetting env. episode 1004, reward total was -15.0. running mean: -16.587564021116957, timestamp: 2022-08-20 10:08:21.051882\n",
      "resetting env. episode 1005, reward total was -17.0. running mean: -16.59168838090579, timestamp: 2022-08-20 10:08:26.845396\n",
      "resetting env. episode 1006, reward total was -17.0. running mean: -16.595771497096734, timestamp: 2022-08-20 10:08:33.684120\n",
      "resetting env. episode 1007, reward total was -18.0. running mean: -16.609813782125766, timestamp: 2022-08-20 10:08:39.790793\n",
      "resetting env. episode 1008, reward total was -19.0. running mean: -16.63371564430451, timestamp: 2022-08-20 10:08:45.074670\n",
      "resetting env. episode 1009, reward total was -17.0. running mean: -16.637378487861465, timestamp: 2022-08-20 10:08:53.380469\n",
      "resetting env. episode 1010, reward total was -19.0. running mean: -16.66100470298285, timestamp: 2022-08-20 10:08:59.374445\n",
      "resetting env. episode 1011, reward total was -18.0. running mean: -16.674394655953023, timestamp: 2022-08-20 10:09:04.836846\n",
      "resetting env. episode 1012, reward total was -19.0. running mean: -16.697650709393493, timestamp: 2022-08-20 10:09:11.260679\n",
      "resetting env. episode 1013, reward total was -17.0. running mean: -16.70067420229956, timestamp: 2022-08-20 10:09:17.031249\n",
      "resetting env. episode 1014, reward total was -14.0. running mean: -16.673667460276565, timestamp: 2022-08-20 10:09:27.807445\n",
      "resetting env. episode 1015, reward total was -15.0. running mean: -16.656930785673797, timestamp: 2022-08-20 10:09:38.501858\n",
      "resetting env. episode 1016, reward total was -17.0. running mean: -16.660361477817062, timestamp: 2022-08-20 10:09:45.891110\n",
      "resetting env. episode 1017, reward total was -13.0. running mean: -16.62375786303889, timestamp: 2022-08-20 10:09:54.819242\n",
      "resetting env. episode 1018, reward total was -15.0. running mean: -16.607520284408498, timestamp: 2022-08-20 10:09:59.871737\n",
      "resetting env. episode 1019, reward total was -13.0. running mean: -16.571445081564413, timestamp: 2022-08-20 10:10:06.408265\n",
      "resetting env. episode 1020, reward total was -19.0. running mean: -16.59573063074877, timestamp: 2022-08-20 10:10:10.966083\n",
      "resetting env. episode 1021, reward total was -17.0. running mean: -16.599773324441284, timestamp: 2022-08-20 10:10:16.623960\n",
      "resetting env. episode 1022, reward total was -19.0. running mean: -16.62377559119687, timestamp: 2022-08-20 10:10:21.511895\n",
      "resetting env. episode 1023, reward total was -21.0. running mean: -16.667537835284904, timestamp: 2022-08-20 10:10:25.692718\n",
      "resetting env. episode 1024, reward total was -17.0. running mean: -16.670862456932056, timestamp: 2022-08-20 10:10:31.579982\n",
      "resetting env. episode 1025, reward total was -10.0. running mean: -16.604153832362737, timestamp: 2022-08-20 10:10:36.729216\n",
      "resetting env. episode 1026, reward total was -15.0. running mean: -16.588112294039107, timestamp: 2022-08-20 10:10:42.338224\n",
      "resetting env. episode 1027, reward total was -18.0. running mean: -16.602231171098715, timestamp: 2022-08-20 10:10:46.499102\n",
      "resetting env. episode 1028, reward total was -14.0. running mean: -16.57620885938773, timestamp: 2022-08-20 10:10:51.804921\n",
      "resetting env. episode 1029, reward total was -19.0. running mean: -16.600446770793855, timestamp: 2022-08-20 10:10:56.302896\n",
      "resetting env. episode 1030, reward total was -19.0. running mean: -16.624442303085917, timestamp: 2022-08-20 10:11:01.327466\n",
      "resetting env. episode 1031, reward total was -14.0. running mean: -16.598197880055057, timestamp: 2022-08-20 10:11:06.766867\n",
      "resetting env. episode 1032, reward total was -17.0. running mean: -16.602215901254507, timestamp: 2022-08-20 10:11:11.439508\n",
      "resetting env. episode 1033, reward total was -18.0. running mean: -16.61619374224196, timestamp: 2022-08-20 10:11:16.209758\n",
      "resetting env. episode 1034, reward total was -16.0. running mean: -16.610031804819542, timestamp: 2022-08-20 10:11:22.636614\n",
      "resetting env. episode 1035, reward total was -14.0. running mean: -16.58393148677135, timestamp: 2022-08-20 10:11:28.840996\n",
      "resetting env. episode 1036, reward total was -17.0. running mean: -16.588092171903636, timestamp: 2022-08-20 10:11:35.287763\n",
      "resetting env. episode 1037, reward total was -15.0. running mean: -16.572211250184598, timestamp: 2022-08-20 10:11:40.862861\n",
      "resetting env. episode 1038, reward total was -15.0. running mean: -16.55648913768275, timestamp: 2022-08-20 10:11:48.469528\n",
      "resetting env. episode 1039, reward total was -17.0. running mean: -16.560924246305923, timestamp: 2022-08-20 10:11:52.522697\n",
      "resetting env. episode 1040, reward total was -11.0. running mean: -16.505315003842863, timestamp: 2022-08-20 10:12:00.368722\n",
      "resetting env. episode 1041, reward total was -16.0. running mean: -16.500261853804435, timestamp: 2022-08-20 10:12:09.058497\n",
      "resetting env. episode 1042, reward total was -21.0. running mean: -16.545259235266393, timestamp: 2022-08-20 10:12:14.892898\n",
      "resetting env. episode 1043, reward total was -17.0. running mean: -16.54980664291373, timestamp: 2022-08-20 10:12:21.554092\n",
      "resetting env. episode 1044, reward total was -10.0. running mean: -16.484308576484594, timestamp: 2022-08-20 10:12:29.146799\n",
      "resetting env. episode 1045, reward total was -14.0. running mean: -16.459465490719747, timestamp: 2022-08-20 10:12:35.286386\n",
      "resetting env. episode 1046, reward total was -12.0. running mean: -16.41487083581255, timestamp: 2022-08-20 10:12:41.547650\n",
      "resetting env. episode 1047, reward total was -19.0. running mean: -16.440722127454425, timestamp: 2022-08-20 10:12:48.823202\n",
      "resetting env. episode 1048, reward total was -15.0. running mean: -16.42631490617988, timestamp: 2022-08-20 10:12:56.382001\n",
      "resetting env. episode 1049, reward total was -13.0. running mean: -16.39205175711808, timestamp: 2022-08-20 10:13:03.863003\n",
      "resetting env. episode 1050, reward total was -14.0. running mean: -16.368131239546898, timestamp: 2022-08-20 10:13:09.687434\n",
      "resetting env. episode 1051, reward total was -14.0. running mean: -16.34444992715143, timestamp: 2022-08-20 10:13:18.161780\n",
      "resetting env. episode 1052, reward total was -19.0. running mean: -16.37100542787992, timestamp: 2022-08-20 10:13:25.130155\n",
      "resetting env. episode 1053, reward total was -19.0. running mean: -16.39729537360112, timestamp: 2022-08-20 10:13:30.545677\n",
      "resetting env. episode 1054, reward total was -21.0. running mean: -16.443322419865112, timestamp: 2022-08-20 10:13:35.214203\n",
      "resetting env. episode 1055, reward total was -19.0. running mean: -16.468889195666463, timestamp: 2022-08-20 10:13:39.760049\n",
      "resetting env. episode 1056, reward total was -19.0. running mean: -16.4942003037098, timestamp: 2022-08-20 10:13:44.747716\n",
      "resetting env. episode 1057, reward total was -18.0. running mean: -16.509258300672702, timestamp: 2022-08-20 10:13:49.146958\n",
      "resetting env. episode 1058, reward total was -20.0. running mean: -16.544165717665972, timestamp: 2022-08-20 10:13:54.635287\n",
      "resetting env. episode 1059, reward total was -17.0. running mean: -16.548724060489313, timestamp: 2022-08-20 10:13:59.589047\n",
      "resetting env. episode 1060, reward total was -12.0. running mean: -16.50323681988442, timestamp: 2022-08-20 10:14:06.151513\n",
      "resetting env. episode 1061, reward total was -10.0. running mean: -16.43820445168558, timestamp: 2022-08-20 10:14:12.815690\n",
      "resetting env. episode 1062, reward total was -16.0. running mean: -16.433822407168723, timestamp: 2022-08-20 10:14:18.241189\n",
      "resetting env. episode 1063, reward total was -15.0. running mean: -16.419484183097033, timestamp: 2022-08-20 10:14:23.684638\n",
      "resetting env. episode 1064, reward total was -17.0. running mean: -16.425289341266065, timestamp: 2022-08-20 10:14:28.627425\n",
      "resetting env. episode 1065, reward total was -13.0. running mean: -16.391036447853402, timestamp: 2022-08-20 10:14:35.015354\n",
      "resetting env. episode 1066, reward total was -19.0. running mean: -16.417126083374868, timestamp: 2022-08-20 10:14:40.437855\n",
      "resetting env. episode 1067, reward total was -17.0. running mean: -16.422954822541122, timestamp: 2022-08-20 10:14:45.324795\n",
      "resetting env. episode 1068, reward total was -19.0. running mean: -16.448725274315713, timestamp: 2022-08-20 10:14:51.249955\n",
      "resetting env. episode 1069, reward total was -11.0. running mean: -16.394238021572555, timestamp: 2022-08-20 10:14:57.794500\n",
      "resetting env. episode 1070, reward total was -13.0. running mean: -16.360295641356828, timestamp: 2022-08-20 10:15:04.219289\n",
      "resetting env. episode 1071, reward total was -16.0. running mean: -16.35669268494326, timestamp: 2022-08-20 10:15:09.765464\n",
      "resetting env. episode 1072, reward total was -13.0. running mean: -16.323125758093823, timestamp: 2022-08-20 10:15:17.490815\n",
      "resetting env. episode 1073, reward total was -15.0. running mean: -16.309894500512883, timestamp: 2022-08-20 10:15:25.002735\n",
      "resetting env. episode 1074, reward total was -15.0. running mean: -16.296795555507753, timestamp: 2022-08-20 10:15:30.268667\n",
      "resetting env. episode 1075, reward total was -14.0. running mean: -16.273827599952675, timestamp: 2022-08-20 10:15:36.608715\n",
      "resetting env. episode 1076, reward total was -18.0. running mean: -16.29108932395315, timestamp: 2022-08-20 10:15:42.752289\n",
      "resetting env. episode 1077, reward total was -13.0. running mean: -16.258178430713617, timestamp: 2022-08-20 10:15:50.309091\n",
      "resetting env. episode 1078, reward total was -17.0. running mean: -16.265596646406483, timestamp: 2022-08-20 10:15:57.130857\n",
      "resetting env. episode 1079, reward total was -17.0. running mean: -16.27294067994242, timestamp: 2022-08-20 10:16:04.780427\n",
      "resetting env. episode 1080, reward total was -14.0. running mean: -16.250211273142995, timestamp: 2022-08-20 10:16:11.726842\n",
      "resetting env. episode 1081, reward total was -13.0. running mean: -16.217709160411562, timestamp: 2022-08-20 10:16:18.689232\n",
      "resetting env. episode 1082, reward total was -15.0. running mean: -16.205532068807447, timestamp: 2022-08-20 10:16:26.298890\n",
      "resetting env. episode 1083, reward total was -11.0. running mean: -16.15347674811937, timestamp: 2022-08-20 10:16:34.168856\n",
      "resetting env. episode 1084, reward total was -15.0. running mean: -16.141941980638176, timestamp: 2022-08-20 10:16:42.865608\n",
      "resetting env. episode 1085, reward total was -13.0. running mean: -16.110522560831793, timestamp: 2022-08-20 10:16:49.946678\n",
      "resetting env. episode 1086, reward total was -14.0. running mean: -16.089417335223473, timestamp: 2022-08-20 10:16:58.367199\n",
      "resetting env. episode 1087, reward total was -9.0. running mean: -16.01852316187124, timestamp: 2022-08-20 10:17:10.105794\n",
      "resetting env. episode 1088, reward total was -17.0. running mean: -16.02833793025253, timestamp: 2022-08-20 10:17:17.860066\n",
      "resetting env. episode 1089, reward total was -14.0. running mean: -16.008054550950003, timestamp: 2022-08-20 10:17:26.323445\n",
      "resetting env. episode 1090, reward total was -15.0. running mean: -15.997974005440504, timestamp: 2022-08-20 10:17:34.192423\n",
      "resetting env. episode 1091, reward total was -16.0. running mean: -15.9979942653861, timestamp: 2022-08-20 10:17:42.645817\n",
      "resetting env. episode 1092, reward total was -15.0. running mean: -15.988014322732239, timestamp: 2022-08-20 10:17:50.152748\n",
      "resetting env. episode 1093, reward total was -19.0. running mean: -16.018134179504916, timestamp: 2022-08-20 10:17:55.981169\n",
      "resetting env. episode 1094, reward total was -21.0. running mean: -16.067952837709868, timestamp: 2022-08-20 10:18:02.211516\n",
      "resetting env. episode 1095, reward total was -15.0. running mean: -16.057273309332768, timestamp: 2022-08-20 10:18:08.882684\n",
      "resetting env. episode 1096, reward total was -17.0. running mean: -16.06670057623944, timestamp: 2022-08-20 10:18:16.236029\n",
      "resetting env. episode 1097, reward total was -14.0. running mean: -16.046033570477046, timestamp: 2022-08-20 10:18:22.254940\n",
      "resetting env. episode 1098, reward total was -13.0. running mean: -16.015573234772276, timestamp: 2022-08-20 10:18:29.015867\n",
      "resetting env. episode 1099, reward total was -15.0. running mean: -16.005417502424553, timestamp: 2022-08-20 10:18:35.710974\n",
      "resetting env. episode 1100, reward total was -18.0. running mean: -16.025363327400306, timestamp: 2022-08-20 10:18:42.929676\n",
      "resetting env. episode 1101, reward total was -11.0. running mean: -15.975109694126303, timestamp: 2022-08-20 10:18:49.601844\n",
      "resetting env. episode 1102, reward total was -10.0. running mean: -15.915358597185039, timestamp: 2022-08-20 10:18:56.886369\n",
      "resetting env. episode 1103, reward total was -13.0. running mean: -15.886205011213189, timestamp: 2022-08-20 10:19:04.948847\n",
      "resetting env. episode 1104, reward total was -15.0. running mean: -15.877342961101057, timestamp: 2022-08-20 10:19:11.807484\n",
      "resetting env. episode 1105, reward total was -16.0. running mean: -15.878569531490047, timestamp: 2022-08-20 10:19:16.521883\n",
      "resetting env. episode 1106, reward total was -14.0. running mean: -15.859783836175147, timestamp: 2022-08-20 10:19:22.762202\n",
      "resetting env. episode 1107, reward total was -15.0. running mean: -15.851185997813396, timestamp: 2022-08-20 10:19:28.359243\n",
      "resetting env. episode 1108, reward total was -19.0. running mean: -15.882674137835261, timestamp: 2022-08-20 10:19:33.468585\n",
      "resetting env. episode 1109, reward total was -11.0. running mean: -15.833847396456909, timestamp: 2022-08-20 10:19:40.345204\n",
      "resetting env. episode 1110, reward total was -12.0. running mean: -15.795508922492338, timestamp: 2022-08-20 10:19:47.286677\n",
      "resetting env. episode 1111, reward total was -19.0. running mean: -15.827553833267414, timestamp: 2022-08-20 10:19:54.442522\n",
      "resetting env. episode 1112, reward total was -16.0. running mean: -15.82927829493474, timestamp: 2022-08-20 10:20:00.821470\n",
      "resetting env. episode 1113, reward total was -16.0. running mean: -15.830985511985393, timestamp: 2022-08-20 10:20:06.405544\n",
      "resetting env. episode 1114, reward total was -14.0. running mean: -15.81267565686554, timestamp: 2022-08-20 10:20:12.016548\n",
      "resetting env. episode 1115, reward total was -17.0. running mean: -15.824548900296884, timestamp: 2022-08-20 10:20:19.499549\n",
      "resetting env. episode 1116, reward total was -17.0. running mean: -15.836303411293915, timestamp: 2022-08-20 10:20:25.747843\n",
      "resetting env. episode 1117, reward total was -18.0. running mean: -15.857940377180975, timestamp: 2022-08-20 10:20:31.435638\n",
      "resetting env. episode 1118, reward total was -16.0. running mean: -15.859360973409165, timestamp: 2022-08-20 10:20:37.700892\n",
      "resetting env. episode 1119, reward total was -12.0. running mean: -15.820767363675072, timestamp: 2022-08-20 10:20:47.877718\n",
      "resetting env. episode 1120, reward total was -17.0. running mean: -15.832559690038321, timestamp: 2022-08-20 10:20:52.980050\n",
      "resetting env. episode 1121, reward total was -16.0. running mean: -15.834234093137939, timestamp: 2022-08-20 10:20:59.220370\n",
      "resetting env. episode 1122, reward total was -15.0. running mean: -15.825891752206559, timestamp: 2022-08-20 10:21:04.307771\n",
      "resetting env. episode 1123, reward total was -16.0. running mean: -15.827632834684493, timestamp: 2022-08-20 10:21:11.178408\n",
      "resetting env. episode 1124, reward total was -17.0. running mean: -15.839356506337648, timestamp: 2022-08-20 10:21:15.411120\n",
      "resetting env. episode 1125, reward total was -15.0. running mean: -15.830962941274272, timestamp: 2022-08-20 10:21:22.816298\n",
      "resetting env. episode 1126, reward total was -11.0. running mean: -15.782653311861528, timestamp: 2022-08-20 10:21:30.701221\n",
      "resetting env. episode 1127, reward total was -18.0. running mean: -15.804826778742912, timestamp: 2022-08-20 10:21:37.360422\n",
      "resetting env. episode 1128, reward total was -9.0. running mean: -15.736778510955483, timestamp: 2022-08-20 10:21:45.699134\n",
      "resetting env. episode 1129, reward total was -16.0. running mean: -15.739410725845929, timestamp: 2022-08-20 10:21:51.983364\n",
      "resetting env. episode 1130, reward total was -13.0. running mean: -15.71201661858747, timestamp: 2022-08-20 10:21:59.513207\n",
      "resetting env. episode 1131, reward total was -17.0. running mean: -15.724896452401595, timestamp: 2022-08-20 10:22:05.772476\n",
      "resetting env. episode 1132, reward total was -17.0. running mean: -15.73764748787758, timestamp: 2022-08-20 10:22:11.954949\n",
      "resetting env. episode 1133, reward total was -19.0. running mean: -15.770271012998803, timestamp: 2022-08-20 10:22:17.499131\n",
      "resetting env. episode 1134, reward total was -14.0. running mean: -15.752568302868815, timestamp: 2022-08-20 10:22:24.768701\n",
      "resetting env. episode 1135, reward total was -10.0. running mean: -15.695042619840127, timestamp: 2022-08-20 10:22:31.086812\n",
      "resetting env. episode 1136, reward total was -14.0. running mean: -15.678092193641726, timestamp: 2022-08-20 10:22:39.143275\n",
      "resetting env. episode 1137, reward total was -15.0. running mean: -15.671311271705308, timestamp: 2022-08-20 10:22:44.994635\n",
      "resetting env. episode 1138, reward total was -12.0. running mean: -15.634598158988254, timestamp: 2022-08-20 10:22:51.850309\n",
      "resetting env. episode 1139, reward total was -18.0. running mean: -15.65825217739837, timestamp: 2022-08-20 10:22:58.935372\n",
      "resetting env. episode 1140, reward total was -11.0. running mean: -15.611669655624386, timestamp: 2022-08-20 10:23:07.178340\n",
      "resetting env. episode 1141, reward total was -17.0. running mean: -15.625552959068141, timestamp: 2022-08-20 10:23:13.710877\n",
      "resetting env. episode 1142, reward total was -13.0. running mean: -15.59929742947746, timestamp: 2022-08-20 10:23:21.980776\n",
      "resetting env. episode 1143, reward total was -13.0. running mean: -15.573304455182686, timestamp: 2022-08-20 10:23:31.678877\n",
      "resetting env. episode 1144, reward total was -15.0. running mean: -15.56757141063086, timestamp: 2022-08-20 10:23:38.079739\n",
      "resetting env. episode 1145, reward total was -12.0. running mean: -15.53189569652455, timestamp: 2022-08-20 10:23:45.517863\n",
      "resetting env. episode 1146, reward total was -15.0. running mean: -15.526576739559305, timestamp: 2022-08-20 10:23:51.796076\n",
      "resetting env. episode 1147, reward total was -14.0. running mean: -15.511310972163713, timestamp: 2022-08-20 10:23:59.003810\n",
      "resetting env. episode 1148, reward total was -19.0. running mean: -15.546197862442074, timestamp: 2022-08-20 10:24:04.993798\n",
      "resetting env. episode 1149, reward total was -16.0. running mean: -15.550735883817653, timestamp: 2022-08-20 10:24:10.465171\n",
      "resetting env. episode 1150, reward total was -14.0. running mean: -15.535228524979477, timestamp: 2022-08-20 10:24:17.678917\n",
      "resetting env. episode 1151, reward total was -18.0. running mean: -15.559876239729682, timestamp: 2022-08-20 10:24:22.912900\n",
      "resetting env. episode 1152, reward total was -9.0. running mean: -15.494277477332385, timestamp: 2022-08-20 10:24:31.947749\n",
      "resetting env. episode 1153, reward total was -14.0. running mean: -15.479334702559061, timestamp: 2022-08-20 10:24:40.129878\n",
      "resetting env. episode 1154, reward total was -15.0. running mean: -15.474541355533471, timestamp: 2022-08-20 10:24:48.044723\n",
      "resetting env. episode 1155, reward total was -19.0. running mean: -15.509795941978137, timestamp: 2022-08-20 10:24:54.066625\n",
      "resetting env. episode 1156, reward total was -18.0. running mean: -15.534697982558354, timestamp: 2022-08-20 10:24:58.133791\n",
      "resetting env. episode 1157, reward total was -13.0. running mean: -15.509351002732771, timestamp: 2022-08-20 10:25:05.478124\n",
      "resetting env. episode 1158, reward total was -15.0. running mean: -15.504257492705444, timestamp: 2022-08-20 10:25:12.576150\n",
      "resetting env. episode 1159, reward total was -15.0. running mean: -15.499214917778389, timestamp: 2022-08-20 10:25:18.747653\n",
      "resetting env. episode 1160, reward total was -15.0. running mean: -15.494222768600604, timestamp: 2022-08-20 10:25:25.072746\n",
      "resetting env. episode 1161, reward total was -17.0. running mean: -15.509280540914599, timestamp: 2022-08-20 10:25:30.504230\n",
      "resetting env. episode 1162, reward total was -17.0. running mean: -15.524187735505453, timestamp: 2022-08-20 10:25:36.396479\n",
      "resetting env. episode 1163, reward total was -13.0. running mean: -15.4989458581504, timestamp: 2022-08-20 10:25:42.866184\n",
      "resetting env. episode 1164, reward total was -6.0. running mean: -15.403956399568896, timestamp: 2022-08-20 10:25:52.943253\n",
      "resetting env. episode 1165, reward total was -15.0. running mean: -15.399916835573208, timestamp: 2022-08-20 10:26:00.899981\n",
      "resetting env. episode 1166, reward total was -18.0. running mean: -15.425917667217476, timestamp: 2022-08-20 10:26:06.290572\n",
      "resetting env. episode 1167, reward total was -14.0. running mean: -15.411658490545301, timestamp: 2022-08-20 10:26:13.291368\n",
      "resetting env. episode 1168, reward total was -13.0. running mean: -15.387541905639848, timestamp: 2022-08-20 10:26:19.584547\n",
      "resetting env. episode 1169, reward total was -15.0. running mean: -15.38366648658345, timestamp: 2022-08-20 10:26:25.352129\n",
      "resetting env. episode 1170, reward total was -16.0. running mean: -15.389829821717615, timestamp: 2022-08-20 10:26:31.217451\n",
      "resetting env. episode 1171, reward total was -19.0. running mean: -15.425931523500438, timestamp: 2022-08-20 10:26:34.916569\n",
      "resetting env. episode 1172, reward total was -15.0. running mean: -15.421672208265434, timestamp: 2022-08-20 10:26:39.733686\n",
      "resetting env. episode 1173, reward total was -19.0. running mean: -15.457455486182779, timestamp: 2022-08-20 10:26:44.287515\n",
      "resetting env. episode 1174, reward total was -13.0. running mean: -15.432880931320952, timestamp: 2022-08-20 10:26:49.670162\n",
      "resetting env. episode 1175, reward total was -16.0. running mean: -15.438552122007742, timestamp: 2022-08-20 10:26:54.679737\n",
      "resetting env. episode 1176, reward total was -14.0. running mean: -15.424166600787665, timestamp: 2022-08-20 10:26:59.932695\n",
      "resetting env. episode 1177, reward total was -15.0. running mean: -15.419924934779788, timestamp: 2022-08-20 10:27:05.459922\n",
      "resetting env. episode 1178, reward total was -17.0. running mean: -15.43572568543199, timestamp: 2022-08-20 10:27:11.612477\n",
      "resetting env. episode 1179, reward total was -10.0. running mean: -15.38136842857767, timestamp: 2022-08-20 10:27:18.212833\n",
      "resetting env. episode 1180, reward total was -16.0. running mean: -15.387554744291894, timestamp: 2022-08-20 10:27:22.234083\n",
      "resetting env. episode 1181, reward total was -15.0. running mean: -15.383679196848975, timestamp: 2022-08-20 10:27:29.241355\n",
      "resetting env. episode 1182, reward total was -16.0. running mean: -15.389842404880486, timestamp: 2022-08-20 10:27:35.271236\n",
      "resetting env. episode 1183, reward total was -18.0. running mean: -15.415943980831681, timestamp: 2022-08-20 10:27:39.759239\n",
      "resetting env. episode 1184, reward total was -17.0. running mean: -15.431784541023363, timestamp: 2022-08-20 10:27:44.153495\n",
      "resetting env. episode 1185, reward total was -17.0. running mean: -15.44746669561313, timestamp: 2022-08-20 10:27:49.136174\n",
      "resetting env. episode 1186, reward total was -15.0. running mean: -15.442992028656999, timestamp: 2022-08-20 10:27:56.503510\n",
      "resetting env. episode 1187, reward total was -11.0. running mean: -15.398562108370427, timestamp: 2022-08-20 10:28:04.091235\n",
      "resetting env. episode 1188, reward total was -15.0. running mean: -15.394576487286724, timestamp: 2022-08-20 10:28:10.079196\n",
      "resetting env. episode 1189, reward total was -14.0. running mean: -15.380630722413857, timestamp: 2022-08-20 10:28:16.359412\n",
      "resetting env. episode 1190, reward total was -15.0. running mean: -15.376824415189718, timestamp: 2022-08-20 10:28:23.137289\n",
      "resetting env. episode 1191, reward total was -18.0. running mean: -15.40305617103782, timestamp: 2022-08-20 10:28:27.713061\n",
      "resetting env. episode 1192, reward total was -15.0. running mean: -15.399025609327442, timestamp: 2022-08-20 10:28:32.874291\n",
      "resetting env. episode 1193, reward total was -10.0. running mean: -15.345035353234167, timestamp: 2022-08-20 10:28:38.416447\n",
      "resetting env. episode 1194, reward total was -18.0. running mean: -15.371584999701824, timestamp: 2022-08-20 10:28:43.274464\n",
      "resetting env. episode 1195, reward total was -20.0. running mean: -15.417869149704805, timestamp: 2022-08-20 10:28:47.597907\n",
      "resetting env. episode 1196, reward total was -19.0. running mean: -15.453690458207756, timestamp: 2022-08-20 10:28:54.036695\n",
      "resetting env. episode 1197, reward total was -11.0. running mean: -15.409153553625679, timestamp: 2022-08-20 10:28:59.658668\n",
      "resetting env. episode 1198, reward total was -17.0. running mean: -15.425062018089422, timestamp: 2022-08-20 10:29:03.986100\n",
      "resetting env. episode 1199, reward total was -16.0. running mean: -15.430811397908528, timestamp: 2022-08-20 10:29:09.874363\n",
      "resetting env. episode 1200, reward total was -15.0. running mean: -15.426503283929442, timestamp: 2022-08-20 10:29:15.472398\n",
      "resetting env. episode 1201, reward total was -17.0. running mean: -15.442238251090147, timestamp: 2022-08-20 10:29:20.896898\n",
      "resetting env. episode 1202, reward total was -19.0. running mean: -15.477815868579246, timestamp: 2022-08-20 10:29:24.909174\n",
      "resetting env. episode 1203, reward total was -14.0. running mean: -15.463037709893454, timestamp: 2022-08-20 10:29:30.269844\n",
      "resetting env. episode 1204, reward total was -14.0. running mean: -15.44840733279452, timestamp: 2022-08-20 10:29:36.282771\n",
      "resetting env. episode 1205, reward total was -19.0. running mean: -15.483923259466575, timestamp: 2022-08-20 10:29:42.355539\n",
      "resetting env. episode 1206, reward total was -15.0. running mean: -15.47908402687191, timestamp: 2022-08-20 10:29:48.391406\n",
      "resetting env. episode 1207, reward total was -16.0. running mean: -15.48429318660319, timestamp: 2022-08-20 10:29:55.810608\n",
      "resetting env. episode 1208, reward total was -16.0. running mean: -15.489450254737159, timestamp: 2022-08-20 10:30:02.327183\n",
      "resetting env. episode 1209, reward total was -11.0. running mean: -15.444555752189787, timestamp: 2022-08-20 10:30:09.199789\n",
      "resetting env. episode 1210, reward total was -13.0. running mean: -15.42011019466789, timestamp: 2022-08-20 10:30:16.411510\n",
      "resetting env. episode 1211, reward total was -18.0. running mean: -15.44590909272121, timestamp: 2022-08-20 10:30:22.933076\n",
      "resetting env. episode 1212, reward total was -14.0. running mean: -15.431450001794, timestamp: 2022-08-20 10:30:30.273454\n",
      "resetting env. episode 1213, reward total was -14.0. running mean: -15.41713550177606, timestamp: 2022-08-20 10:30:36.360029\n",
      "resetting env. episode 1214, reward total was -12.0. running mean: -15.3829641467583, timestamp: 2022-08-20 10:30:43.225634\n",
      "resetting env. episode 1215, reward total was -15.0. running mean: -15.379134505290716, timestamp: 2022-08-20 10:30:50.060333\n",
      "resetting env. episode 1216, reward total was -17.0. running mean: -15.39534316023781, timestamp: 2022-08-20 10:30:56.986816\n",
      "resetting env. episode 1217, reward total was -19.0. running mean: -15.43138972863543, timestamp: 2022-08-20 10:31:01.955537\n",
      "resetting env. episode 1218, reward total was -13.0. running mean: -15.407075831349077, timestamp: 2022-08-20 10:31:09.161276\n",
      "resetting env. episode 1219, reward total was -16.0. running mean: -15.413005073035587, timestamp: 2022-08-20 10:31:14.782252\n",
      "resetting env. episode 1220, reward total was -15.0. running mean: -15.408875022305232, timestamp: 2022-08-20 10:31:21.353684\n",
      "resetting env. episode 1221, reward total was -11.0. running mean: -15.364786272082178, timestamp: 2022-08-20 10:31:28.416804\n",
      "resetting env. episode 1222, reward total was -17.0. running mean: -15.381138409361355, timestamp: 2022-08-20 10:31:35.061044\n",
      "resetting env. episode 1223, reward total was -10.0. running mean: -15.327327025267742, timestamp: 2022-08-20 10:31:44.296359\n",
      "resetting env. episode 1224, reward total was -12.0. running mean: -15.294053755015064, timestamp: 2022-08-20 10:31:51.306621\n",
      "resetting env. episode 1225, reward total was -13.0. running mean: -15.271113217464913, timestamp: 2022-08-20 10:31:58.721798\n",
      "resetting env. episode 1226, reward total was -12.0. running mean: -15.238402085290263, timestamp: 2022-08-20 10:32:06.114040\n",
      "resetting env. episode 1227, reward total was -16.0. running mean: -15.24601806443736, timestamp: 2022-08-20 10:32:13.679817\n",
      "resetting env. episode 1228, reward total was -20.0. running mean: -15.293557883792985, timestamp: 2022-08-20 10:32:19.037529\n",
      "resetting env. episode 1229, reward total was -19.0. running mean: -15.330622304955055, timestamp: 2022-08-20 10:32:25.746563\n",
      "resetting env. episode 1230, reward total was -15.0. running mean: -15.327316081905504, timestamp: 2022-08-20 10:32:33.310368\n",
      "resetting env. episode 1231, reward total was -18.0. running mean: -15.35404292108645, timestamp: 2022-08-20 10:32:41.236161\n",
      "resetting env. episode 1232, reward total was -11.0. running mean: -15.310502491875585, timestamp: 2022-08-20 10:32:48.521687\n",
      "resetting env. episode 1233, reward total was -13.0. running mean: -15.28739746695683, timestamp: 2022-08-20 10:32:56.020640\n",
      "resetting env. episode 1234, reward total was -7.0. running mean: -15.204523492287263, timestamp: 2022-08-20 10:33:05.152249\n",
      "resetting env. episode 1235, reward total was -15.0. running mean: -15.20247825736439, timestamp: 2022-08-20 10:33:14.861281\n",
      "resetting env. episode 1236, reward total was -15.0. running mean: -15.200453474790747, timestamp: 2022-08-20 10:33:23.886157\n",
      "resetting env. episode 1237, reward total was -19.0. running mean: -15.238448940042838, timestamp: 2022-08-20 10:33:30.787709\n",
      "resetting env. episode 1238, reward total was -13.0. running mean: -15.21606445064241, timestamp: 2022-08-20 10:33:39.556267\n",
      "resetting env. episode 1239, reward total was -17.0. running mean: -15.233903806135986, timestamp: 2022-08-20 10:33:47.495048\n",
      "resetting env. episode 1240, reward total was -13.0. running mean: -15.211564768074627, timestamp: 2022-08-20 10:33:54.092414\n",
      "resetting env. episode 1241, reward total was -12.0. running mean: -15.17944912039388, timestamp: 2022-08-20 10:34:01.426811\n",
      "resetting env. episode 1242, reward total was -19.0. running mean: -15.21765462918994, timestamp: 2022-08-20 10:34:08.623571\n",
      "resetting env. episode 1243, reward total was -6.0. running mean: -15.125478082898042, timestamp: 2022-08-20 10:34:19.887462\n",
      "resetting env. episode 1244, reward total was -10.0. running mean: -15.074223302069061, timestamp: 2022-08-20 10:34:28.565271\n",
      "resetting env. episode 1245, reward total was -17.0. running mean: -15.093481069048371, timestamp: 2022-08-20 10:34:35.064898\n",
      "resetting env. episode 1246, reward total was -17.0. running mean: -15.112546258357888, timestamp: 2022-08-20 10:34:43.030606\n",
      "resetting env. episode 1247, reward total was -18.0. running mean: -15.141420795774309, timestamp: 2022-08-20 10:34:49.860349\n",
      "resetting env. episode 1248, reward total was -13.0. running mean: -15.120006587816567, timestamp: 2022-08-20 10:35:01.863295\n",
      "resetting env. episode 1249, reward total was -12.0. running mean: -15.0888065219384, timestamp: 2022-08-20 10:35:10.948977\n",
      "resetting env. episode 1250, reward total was -14.0. running mean: -15.077918456719017, timestamp: 2022-08-20 10:35:17.375798\n",
      "resetting env. episode 1251, reward total was -13.0. running mean: -15.057139272151828, timestamp: 2022-08-20 10:35:24.399027\n",
      "resetting env. episode 1252, reward total was -13.0. running mean: -15.03656787943031, timestamp: 2022-08-20 10:35:32.923251\n",
      "resetting env. episode 1253, reward total was -9.0. running mean: -14.976202200636006, timestamp: 2022-08-20 10:35:40.938812\n",
      "resetting env. episode 1254, reward total was -17.0. running mean: -14.996440178629646, timestamp: 2022-08-20 10:35:46.367302\n",
      "resetting env. episode 1255, reward total was -16.0. running mean: -15.006475776843349, timestamp: 2022-08-20 10:35:52.505894\n",
      "resetting env. episode 1256, reward total was -19.0. running mean: -15.046411019074915, timestamp: 2022-08-20 10:35:58.255526\n",
      "resetting env. episode 1257, reward total was -13.0. running mean: -15.025946908884167, timestamp: 2022-08-20 10:36:03.960276\n",
      "resetting env. episode 1258, reward total was -17.0. running mean: -15.045687439795325, timestamp: 2022-08-20 10:36:10.112831\n",
      "resetting env. episode 1259, reward total was -4.0. running mean: -14.93523056539737, timestamp: 2022-08-20 10:36:17.789315\n",
      "resetting env. episode 1260, reward total was -14.0. running mean: -14.925878259743397, timestamp: 2022-08-20 10:36:24.327838\n",
      "resetting env. episode 1261, reward total was -9.0. running mean: -14.866619477145964, timestamp: 2022-08-20 10:36:32.291546\n",
      "resetting env. episode 1262, reward total was -14.0. running mean: -14.857953282374504, timestamp: 2022-08-20 10:36:39.917164\n",
      "resetting env. episode 1263, reward total was -13.0. running mean: -14.83937374955076, timestamp: 2022-08-20 10:36:48.101288\n",
      "resetting env. episode 1264, reward total was -16.0. running mean: -14.850980012055253, timestamp: 2022-08-20 10:36:53.318343\n",
      "resetting env. episode 1265, reward total was -16.0. running mean: -14.862470211934701, timestamp: 2022-08-20 10:36:58.905409\n",
      "resetting env. episode 1266, reward total was -18.0. running mean: -14.893845509815353, timestamp: 2022-08-20 10:37:05.254437\n",
      "resetting env. episode 1267, reward total was -16.0. running mean: -14.9049070547172, timestamp: 2022-08-20 10:37:11.819887\n",
      "resetting env. episode 1268, reward total was -19.0. running mean: -14.945857984170027, timestamp: 2022-08-20 10:37:18.546917\n",
      "resetting env. episode 1269, reward total was -9.0. running mean: -14.886399404328326, timestamp: 2022-08-20 10:37:27.487039\n",
      "resetting env. episode 1270, reward total was -9.0. running mean: -14.827535410285043, timestamp: 2022-08-20 10:37:36.707391\n",
      "resetting env. episode 1271, reward total was -15.0. running mean: -14.829260056182193, timestamp: 2022-08-20 10:37:44.373870\n",
      "resetting env. episode 1272, reward total was -13.0. running mean: -14.810967455620371, timestamp: 2022-08-20 10:37:51.935659\n",
      "resetting env. episode 1273, reward total was -17.0. running mean: -14.832857781064167, timestamp: 2022-08-20 10:37:57.118805\n",
      "resetting env. episode 1274, reward total was -15.0. running mean: -14.834529203253526, timestamp: 2022-08-20 10:38:02.207202\n",
      "resetting env. episode 1275, reward total was -14.0. running mean: -14.82618391122099, timestamp: 2022-08-20 10:38:07.264683\n",
      "resetting env. episode 1276, reward total was -11.0. running mean: -14.78792207210878, timestamp: 2022-08-20 10:38:13.653608\n",
      "resetting env. episode 1277, reward total was -17.0. running mean: -14.810042851387692, timestamp: 2022-08-20 10:38:19.031230\n",
      "resetting env. episode 1278, reward total was -11.0. running mean: -14.771942422873815, timestamp: 2022-08-20 10:38:25.259583\n",
      "resetting env. episode 1279, reward total was -21.0. running mean: -14.834222998645078, timestamp: 2022-08-20 10:38:29.319759\n",
      "resetting env. episode 1280, reward total was -7.0. running mean: -14.755880768658628, timestamp: 2022-08-20 10:38:36.714963\n",
      "resetting env. episode 1281, reward total was -11.0. running mean: -14.718321960972041, timestamp: 2022-08-20 10:38:42.478556\n",
      "resetting env. episode 1282, reward total was -10.0. running mean: -14.67113874136232, timestamp: 2022-08-20 10:38:48.082581\n",
      "resetting env. episode 1283, reward total was -17.0. running mean: -14.694427353948697, timestamp: 2022-08-20 10:38:53.026361\n",
      "resetting env. episode 1284, reward total was -12.0. running mean: -14.667483080409209, timestamp: 2022-08-20 10:39:01.346125\n",
      "resetting env. episode 1285, reward total was -12.0. running mean: -14.640808249605115, timestamp: 2022-08-20 10:39:07.386976\n",
      "resetting env. episode 1286, reward total was -14.0. running mean: -14.634400167109064, timestamp: 2022-08-20 10:39:13.162539\n",
      "resetting env. episode 1287, reward total was -15.0. running mean: -14.638056165437973, timestamp: 2022-08-20 10:39:17.948779\n",
      "resetting env. episode 1288, reward total was -13.0. running mean: -14.621675603783594, timestamp: 2022-08-20 10:39:24.151176\n",
      "resetting env. episode 1289, reward total was -21.0. running mean: -14.685458847745759, timestamp: 2022-08-20 10:39:28.102603\n",
      "resetting env. episode 1290, reward total was -17.0. running mean: -14.7086042592683, timestamp: 2022-08-20 10:39:33.765466\n",
      "resetting env. episode 1291, reward total was -11.0. running mean: -14.671518216675617, timestamp: 2022-08-20 10:39:39.874138\n",
      "resetting env. episode 1292, reward total was -14.0. running mean: -14.66480303450886, timestamp: 2022-08-20 10:39:45.161007\n",
      "resetting env. episode 1293, reward total was -10.0. running mean: -14.618155004163771, timestamp: 2022-08-20 10:39:53.007036\n",
      "resetting env. episode 1294, reward total was -13.0. running mean: -14.601973454122135, timestamp: 2022-08-20 10:39:58.682863\n",
      "resetting env. episode 1295, reward total was -13.0. running mean: -14.585953719580914, timestamp: 2022-08-20 10:40:05.556492\n",
      "resetting env. episode 1296, reward total was -14.0. running mean: -14.580094182385105, timestamp: 2022-08-20 10:40:11.717022\n",
      "resetting env. episode 1297, reward total was -11.0. running mean: -14.544293240561254, timestamp: 2022-08-20 10:40:19.009530\n",
      "resetting env. episode 1298, reward total was -13.0. running mean: -14.528850308155642, timestamp: 2022-08-20 10:40:24.625517\n",
      "resetting env. episode 1299, reward total was -16.0. running mean: -14.543561805074086, timestamp: 2022-08-20 10:40:30.522754\n",
      "resetting env. episode 1300, reward total was -13.0. running mean: -14.528126187023346, timestamp: 2022-08-20 10:40:37.311610\n",
      "resetting env. episode 1301, reward total was -8.0. running mean: -14.462844925153114, timestamp: 2022-08-20 10:40:44.198228\n",
      "resetting env. episode 1302, reward total was -15.0. running mean: -14.468216475901583, timestamp: 2022-08-20 10:40:50.392643\n",
      "resetting env. episode 1303, reward total was -13.0. running mean: -14.453534311142567, timestamp: 2022-08-20 10:40:57.219395\n",
      "resetting env. episode 1304, reward total was -17.0. running mean: -14.47899896803114, timestamp: 2022-08-20 10:41:03.059802\n",
      "resetting env. episode 1305, reward total was -17.0. running mean: -14.504208978350828, timestamp: 2022-08-20 10:41:08.363605\n",
      "resetting env. episode 1306, reward total was -11.0. running mean: -14.469166888567319, timestamp: 2022-08-20 10:41:14.878193\n",
      "resetting env. episode 1307, reward total was -12.0. running mean: -14.444475219681644, timestamp: 2022-08-20 10:41:20.042388\n",
      "resetting env. episode 1308, reward total was -9.0. running mean: -14.390030467484827, timestamp: 2022-08-20 10:41:27.531373\n",
      "resetting env. episode 1309, reward total was -17.0. running mean: -14.416130162809978, timestamp: 2022-08-20 10:41:33.345828\n",
      "resetting env. episode 1310, reward total was -13.0. running mean: -14.401968861181878, timestamp: 2022-08-20 10:41:39.406627\n",
      "resetting env. episode 1311, reward total was -17.0. running mean: -14.42794917257006, timestamp: 2022-08-20 10:41:45.423545\n",
      "resetting env. episode 1312, reward total was -16.0. running mean: -14.443669680844359, timestamp: 2022-08-20 10:41:51.070452\n",
      "resetting env. episode 1313, reward total was -16.0. running mean: -14.459232984035914, timestamp: 2022-08-20 10:41:57.358642\n",
      "resetting env. episode 1314, reward total was -15.0. running mean: -14.464640654195556, timestamp: 2022-08-20 10:42:03.041451\n",
      "resetting env. episode 1315, reward total was -16.0. running mean: -14.4799942476536, timestamp: 2022-08-20 10:42:07.853588\n",
      "resetting env. episode 1316, reward total was -14.0. running mean: -14.475194305177064, timestamp: 2022-08-20 10:42:14.227551\n",
      "resetting env. episode 1317, reward total was -14.0. running mean: -14.470442362125294, timestamp: 2022-08-20 10:42:19.440619\n",
      "resetting env. episode 1318, reward total was -12.0. running mean: -14.44573793850404, timestamp: 2022-08-20 10:42:26.143701\n",
      "resetting env. episode 1319, reward total was -9.0. running mean: -14.391280559119, timestamp: 2022-08-20 10:42:33.725436\n",
      "resetting env. episode 1320, reward total was -15.0. running mean: -14.39736775352781, timestamp: 2022-08-20 10:42:39.514957\n",
      "resetting env. episode 1321, reward total was -11.0. running mean: -14.363394075992531, timestamp: 2022-08-20 10:42:46.379638\n",
      "resetting env. episode 1322, reward total was -8.0. running mean: -14.299760135232605, timestamp: 2022-08-20 10:42:54.356287\n",
      "resetting env. episode 1323, reward total was -16.0. running mean: -14.31676253388028, timestamp: 2022-08-20 10:43:00.192368\n",
      "resetting env. episode 1324, reward total was -11.0. running mean: -14.283594908541476, timestamp: 2022-08-20 10:43:06.143461\n",
      "resetting env. episode 1325, reward total was -15.0. running mean: -14.290758959456062, timestamp: 2022-08-20 10:43:12.363834\n",
      "resetting env. episode 1326, reward total was -12.0. running mean: -14.2678513698615, timestamp: 2022-08-20 10:43:19.391053\n",
      "resetting env. episode 1327, reward total was -5.0. running mean: -14.175172856162886, timestamp: 2022-08-20 10:43:27.727768\n",
      "resetting env. episode 1328, reward total was -15.0. running mean: -14.183421127601257, timestamp: 2022-08-20 10:43:34.181519\n",
      "resetting env. episode 1329, reward total was -16.0. running mean: -14.201586916325244, timestamp: 2022-08-20 10:43:39.640923\n",
      "resetting env. episode 1330, reward total was -10.0. running mean: -14.159571047161991, timestamp: 2022-08-20 10:43:47.055104\n",
      "resetting env. episode 1331, reward total was -12.0. running mean: -14.13797533669037, timestamp: 2022-08-20 10:43:53.897815\n",
      "resetting env. episode 1332, reward total was -11.0. running mean: -14.106595583323466, timestamp: 2022-08-20 10:44:01.524428\n",
      "resetting env. episode 1333, reward total was -17.0. running mean: -14.135529627490232, timestamp: 2022-08-20 10:44:07.105511\n",
      "resetting env. episode 1334, reward total was -16.0. running mean: -14.15417433121533, timestamp: 2022-08-20 10:44:12.072236\n",
      "resetting env. episode 1335, reward total was -12.0. running mean: -14.132632587903176, timestamp: 2022-08-20 10:44:19.668959\n",
      "resetting env. episode 1336, reward total was -7.0. running mean: -14.061306262024145, timestamp: 2022-08-20 10:44:27.418229\n",
      "resetting env. episode 1337, reward total was -18.0. running mean: -14.100693199403903, timestamp: 2022-08-20 10:44:33.420170\n",
      "resetting env. episode 1338, reward total was -11.0. running mean: -14.069686267409864, timestamp: 2022-08-20 10:44:38.546470\n",
      "resetting env. episode 1339, reward total was -12.0. running mean: -14.048989404735764, timestamp: 2022-08-20 10:44:44.528479\n",
      "resetting env. episode 1340, reward total was -14.0. running mean: -14.048499510688407, timestamp: 2022-08-20 10:44:50.482564\n",
      "resetting env. episode 1341, reward total was -12.0. running mean: -14.028014515581521, timestamp: 2022-08-20 10:44:56.476544\n",
      "resetting env. episode 1342, reward total was -9.0. running mean: -13.977734370425706, timestamp: 2022-08-20 10:45:03.086874\n",
      "resetting env. episode 1343, reward total was -10.0. running mean: -13.937957026721449, timestamp: 2022-08-20 10:45:09.876725\n",
      "resetting env. episode 1344, reward total was -10.0. running mean: -13.898577456454234, timestamp: 2022-08-20 10:45:18.164572\n",
      "resetting env. episode 1345, reward total was -13.0. running mean: -13.889591681889693, timestamp: 2022-08-20 10:45:24.329092\n",
      "resetting env. episode 1346, reward total was -11.0. running mean: -13.860695765070794, timestamp: 2022-08-20 10:45:31.234635\n",
      "resetting env. episode 1347, reward total was -21.0. running mean: -13.932088807420087, timestamp: 2022-08-20 10:45:36.251224\n",
      "resetting env. episode 1348, reward total was -15.0. running mean: -13.942767919345886, timestamp: 2022-08-20 10:45:42.612221\n",
      "resetting env. episode 1349, reward total was -14.0. running mean: -13.943340240152429, timestamp: 2022-08-20 10:45:49.072952\n",
      "resetting env. episode 1350, reward total was -9.0. running mean: -13.893906837750905, timestamp: 2022-08-20 10:45:55.624438\n",
      "resetting env. episode 1351, reward total was -6.0. running mean: -13.814967769373396, timestamp: 2022-08-20 10:46:02.891015\n",
      "resetting env. episode 1352, reward total was -15.0. running mean: -13.826818091679662, timestamp: 2022-08-20 10:46:08.678544\n",
      "resetting env. episode 1353, reward total was -14.0. running mean: -13.828549910762867, timestamp: 2022-08-20 10:46:15.010624\n",
      "resetting env. episode 1354, reward total was -13.0. running mean: -13.820264411655238, timestamp: 2022-08-20 10:46:21.892225\n",
      "resetting env. episode 1355, reward total was -13.0. running mean: -13.812061767538687, timestamp: 2022-08-20 10:46:29.006208\n",
      "resetting env. episode 1356, reward total was -9.0. running mean: -13.7639411498633, timestamp: 2022-08-20 10:46:37.039737\n",
      "resetting env. episode 1357, reward total was -9.0. running mean: -13.716301738364667, timestamp: 2022-08-20 10:46:43.840558\n",
      "resetting env. episode 1358, reward total was -1.0. running mean: -13.589138720981019, timestamp: 2022-08-20 10:46:52.913307\n",
      "resetting env. episode 1359, reward total was -13.0. running mean: -13.58324733377121, timestamp: 2022-08-20 10:46:59.934538\n",
      "resetting env. episode 1360, reward total was -12.0. running mean: -13.567414860433496, timestamp: 2022-08-20 10:47:07.109361\n",
      "resetting env. episode 1361, reward total was -9.0. running mean: -13.52174071182916, timestamp: 2022-08-20 10:47:14.920482\n",
      "resetting env. episode 1362, reward total was -14.0. running mean: -13.526523304710869, timestamp: 2022-08-20 10:47:20.893543\n",
      "resetting env. episode 1363, reward total was -8.0. running mean: -13.47125807166376, timestamp: 2022-08-20 10:47:28.934022\n",
      "resetting env. episode 1364, reward total was -14.0. running mean: -13.476545490947123, timestamp: 2022-08-20 10:47:35.915362\n",
      "resetting env. episode 1365, reward total was -15.0. running mean: -13.491780036037651, timestamp: 2022-08-20 10:47:41.262068\n",
      "resetting env. episode 1366, reward total was -11.0. running mean: -13.466862235677274, timestamp: 2022-08-20 10:47:48.874723\n",
      "resetting env. episode 1367, reward total was -11.0. running mean: -13.4421936133205, timestamp: 2022-08-20 10:47:55.521951\n",
      "resetting env. episode 1368, reward total was -11.0. running mean: -13.417771677187295, timestamp: 2022-08-20 10:48:02.526231\n",
      "resetting env. episode 1369, reward total was -10.0. running mean: -13.383593960415421, timestamp: 2022-08-20 10:48:08.293814\n",
      "resetting env. episode 1370, reward total was -11.0. running mean: -13.359758020811267, timestamp: 2022-08-20 10:48:14.419443\n",
      "resetting env. episode 1371, reward total was -16.0. running mean: -13.386160440603154, timestamp: 2022-08-20 10:48:20.304711\n",
      "resetting env. episode 1372, reward total was -15.0. running mean: -13.402298836197122, timestamp: 2022-08-20 10:48:25.495836\n",
      "resetting env. episode 1373, reward total was -8.0. running mean: -13.34827584783515, timestamp: 2022-08-20 10:48:32.923979\n",
      "resetting env. episode 1374, reward total was -11.0. running mean: -13.324793089356799, timestamp: 2022-08-20 10:48:40.954537\n",
      "resetting env. episode 1375, reward total was -13.0. running mean: -13.321545158463232, timestamp: 2022-08-20 10:48:48.218129\n",
      "resetting env. episode 1376, reward total was -11.0. running mean: -13.298329706878599, timestamp: 2022-08-20 10:48:55.054851\n",
      "resetting env. episode 1377, reward total was -13.0. running mean: -13.295346409809813, timestamp: 2022-08-20 10:49:01.941416\n",
      "resetting env. episode 1378, reward total was -8.0. running mean: -13.242392945711716, timestamp: 2022-08-20 10:49:09.607922\n",
      "resetting env. episode 1379, reward total was -16.0. running mean: -13.2699690162546, timestamp: 2022-08-20 10:49:14.691332\n",
      "resetting env. episode 1380, reward total was 1.0. running mean: -13.127269326092053, timestamp: 2022-08-20 10:49:23.835890\n",
      "resetting env. episode 1381, reward total was -10.0. running mean: -13.095996632831133, timestamp: 2022-08-20 10:49:32.432910\n",
      "resetting env. episode 1382, reward total was -5.0. running mean: -13.015036666502821, timestamp: 2022-08-20 10:49:40.094433\n",
      "resetting env. episode 1383, reward total was -13.0. running mean: -13.014886299837794, timestamp: 2022-08-20 10:49:47.266259\n",
      "resetting env. episode 1384, reward total was -15.0. running mean: -13.034737436839416, timestamp: 2022-08-20 10:49:52.895216\n",
      "resetting env. episode 1385, reward total was -15.0. running mean: -13.054390062471022, timestamp: 2022-08-20 10:49:59.113592\n",
      "resetting env. episode 1386, reward total was -11.0. running mean: -13.033846161846311, timestamp: 2022-08-20 10:50:05.540414\n",
      "resetting env. episode 1387, reward total was -13.0. running mean: -13.03350770022785, timestamp: 2022-08-20 10:50:12.079933\n",
      "resetting env. episode 1388, reward total was -15.0. running mean: -13.05317262322557, timestamp: 2022-08-20 10:50:19.391390\n",
      "resetting env. episode 1389, reward total was -15.0. running mean: -13.072640896993315, timestamp: 2022-08-20 10:50:25.148002\n",
      "resetting env. episode 1390, reward total was -10.0. running mean: -13.041914488023382, timestamp: 2022-08-20 10:50:32.704802\n",
      "resetting env. episode 1391, reward total was -9.0. running mean: -13.001495343143148, timestamp: 2022-08-20 10:50:39.742020\n",
      "resetting env. episode 1392, reward total was -17.0. running mean: -13.041480389711717, timestamp: 2022-08-20 10:50:45.465695\n",
      "resetting env. episode 1393, reward total was -13.0. running mean: -13.0410655858146, timestamp: 2022-08-20 10:50:52.710327\n",
      "resetting env. episode 1394, reward total was -17.0. running mean: -13.080654929956454, timestamp: 2022-08-20 10:50:58.514814\n",
      "resetting env. episode 1395, reward total was -15.0. running mean: -13.09984838065689, timestamp: 2022-08-20 10:51:05.168030\n",
      "resetting env. episode 1396, reward total was -17.0. running mean: -13.13884989685032, timestamp: 2022-08-20 10:51:11.026368\n",
      "resetting env. episode 1397, reward total was -9.0. running mean: -13.097461397881817, timestamp: 2022-08-20 10:51:18.359770\n",
      "resetting env. episode 1398, reward total was -15.0. running mean: -13.116486783903, timestamp: 2022-08-20 10:51:25.951474\n",
      "resetting env. episode 1399, reward total was -10.0. running mean: -13.085321916063968, timestamp: 2022-08-20 10:51:32.399238\n",
      "resetting env. episode 1400, reward total was -15.0. running mean: -13.104468696903329, timestamp: 2022-08-20 10:51:38.717350\n",
      "resetting env. episode 1401, reward total was -10.0. running mean: -13.073424009934294, timestamp: 2022-08-20 10:51:46.410787\n",
      "resetting env. episode 1402, reward total was -13.0. running mean: -13.072689769834952, timestamp: 2022-08-20 10:51:52.947315\n",
      "resetting env. episode 1403, reward total was -14.0. running mean: -13.081962872136602, timestamp: 2022-08-20 10:51:59.862829\n",
      "resetting env. episode 1404, reward total was -9.0. running mean: -13.041143243415236, timestamp: 2022-08-20 10:52:07.586186\n",
      "resetting env. episode 1405, reward total was -15.0. running mean: -13.060731810981084, timestamp: 2022-08-20 10:52:13.096457\n",
      "resetting env. episode 1406, reward total was -12.0. running mean: -13.050124492871271, timestamp: 2022-08-20 10:52:21.965749\n",
      "resetting env. episode 1407, reward total was -6.0. running mean: -12.979623247942559, timestamp: 2022-08-20 10:52:29.174480\n",
      "resetting env. episode 1408, reward total was -18.0. running mean: -13.029827015463134, timestamp: 2022-08-20 10:52:34.184089\n",
      "resetting env. episode 1409, reward total was -17.0. running mean: -13.069528745308503, timestamp: 2022-08-20 10:52:39.545758\n",
      "resetting env. episode 1410, reward total was -11.0. running mean: -13.048833457855418, timestamp: 2022-08-20 10:52:46.933013\n",
      "resetting env. episode 1411, reward total was -5.0. running mean: -12.968345123276864, timestamp: 2022-08-20 10:52:54.619464\n",
      "resetting env. episode 1412, reward total was -9.0. running mean: -12.928661672044095, timestamp: 2022-08-20 10:53:02.898337\n",
      "resetting env. episode 1413, reward total was -15.0. running mean: -12.949375055323655, timestamp: 2022-08-20 10:53:08.484403\n",
      "resetting env. episode 1414, reward total was -11.0. running mean: -12.929881304770417, timestamp: 2022-08-20 10:53:16.315471\n",
      "resetting env. episode 1415, reward total was -9.0. running mean: -12.890582491722713, timestamp: 2022-08-20 10:53:24.363958\n",
      "resetting env. episode 1416, reward total was -18.0. running mean: -12.941676666805485, timestamp: 2022-08-20 10:53:31.442039\n",
      "resetting env. episode 1417, reward total was -5.0. running mean: -12.862259900137431, timestamp: 2022-08-20 10:53:42.195295\n",
      "resetting env. episode 1418, reward total was -13.0. running mean: -12.863637301136057, timestamp: 2022-08-20 10:53:49.720181\n",
      "resetting env. episode 1419, reward total was -16.0. running mean: -12.895000928124697, timestamp: 2022-08-20 10:53:55.080852\n",
      "resetting env. episode 1420, reward total was -12.0. running mean: -12.886050918843448, timestamp: 2022-08-20 10:54:02.301553\n",
      "resetting env. episode 1421, reward total was -9.0. running mean: -12.847190409655013, timestamp: 2022-08-20 10:54:10.092726\n",
      "resetting env. episode 1422, reward total was -11.0. running mean: -12.828718505558463, timestamp: 2022-08-20 10:54:18.636885\n",
      "resetting env. episode 1423, reward total was -10.0. running mean: -12.800431320502877, timestamp: 2022-08-20 10:54:26.870875\n",
      "resetting env. episode 1424, reward total was -17.0. running mean: -12.842427007297848, timestamp: 2022-08-20 10:54:32.808007\n",
      "resetting env. episode 1425, reward total was -17.0. running mean: -12.88400273722487, timestamp: 2022-08-20 10:54:39.305640\n",
      "resetting env. episode 1426, reward total was -13.0. running mean: -12.885162709852622, timestamp: 2022-08-20 10:54:45.659657\n",
      "resetting env. episode 1427, reward total was -8.0. running mean: -12.836311082754095, timestamp: 2022-08-20 10:54:54.861057\n",
      "resetting env. episode 1428, reward total was -10.0. running mean: -12.807947971926554, timestamp: 2022-08-20 10:55:00.932858\n",
      "resetting env. episode 1429, reward total was -12.0. running mean: -12.799868492207288, timestamp: 2022-08-20 10:55:09.575727\n",
      "resetting env. episode 1430, reward total was -4.0. running mean: -12.711869807285215, timestamp: 2022-08-20 10:55:17.906458\n",
      "resetting env. episode 1431, reward total was -6.0. running mean: -12.644751109212363, timestamp: 2022-08-20 10:55:25.722566\n",
      "resetting env. episode 1432, reward total was -13.0. running mean: -12.64830359812024, timestamp: 2022-08-20 10:55:32.049653\n",
      "resetting env. episode 1433, reward total was -16.0. running mean: -12.681820562139038, timestamp: 2022-08-20 10:55:39.326203\n",
      "resetting env. episode 1434, reward total was -9.0. running mean: -12.645002356517647, timestamp: 2022-08-20 10:55:46.877028\n",
      "resetting env. episode 1435, reward total was -8.0. running mean: -12.59855233295247, timestamp: 2022-08-20 10:55:57.474694\n",
      "resetting env. episode 1436, reward total was -18.0. running mean: -12.652566809622945, timestamp: 2022-08-20 10:56:03.396869\n",
      "resetting env. episode 1437, reward total was -9.0. running mean: -12.616041141526715, timestamp: 2022-08-20 10:56:14.845264\n",
      "resetting env. episode 1438, reward total was -15.0. running mean: -12.639880730111448, timestamp: 2022-08-20 10:56:23.935964\n",
      "resetting env. episode 1439, reward total was -10.0. running mean: -12.613481922810333, timestamp: 2022-08-20 10:56:33.564228\n",
      "resetting env. episode 1440, reward total was -10.0. running mean: -12.58734710358223, timestamp: 2022-08-20 10:56:42.626999\n",
      "resetting env. episode 1441, reward total was -13.0. running mean: -12.591473632546409, timestamp: 2022-08-20 10:56:50.433137\n",
      "resetting env. episode 1442, reward total was -16.0. running mean: -12.625558896220944, timestamp: 2022-08-20 10:56:58.184433\n",
      "resetting env. episode 1443, reward total was -13.0. running mean: -12.629303307258736, timestamp: 2022-08-20 10:57:06.142144\n",
      "resetting env. episode 1444, reward total was -14.0. running mean: -12.64301027418615, timestamp: 2022-08-20 10:57:12.376478\n",
      "resetting env. episode 1445, reward total was -11.0. running mean: -12.626580171444287, timestamp: 2022-08-20 10:57:21.448232\n",
      "resetting env. episode 1446, reward total was -10.0. running mean: -12.600314369729844, timestamp: 2022-08-20 10:57:28.354798\n",
      "resetting env. episode 1447, reward total was -21.0. running mean: -12.684311226032547, timestamp: 2022-08-20 10:57:33.365378\n",
      "resetting env. episode 1448, reward total was -12.0. running mean: -12.67746811377222, timestamp: 2022-08-20 10:57:40.625968\n",
      "resetting env. episode 1449, reward total was -14.0. running mean: -12.690693432634498, timestamp: 2022-08-20 10:57:46.258913\n",
      "resetting env. episode 1450, reward total was -10.0. running mean: -12.663786498308154, timestamp: 2022-08-20 10:57:55.010519\n",
      "resetting env. episode 1451, reward total was -9.0. running mean: -12.627148633325072, timestamp: 2022-08-20 10:58:03.634466\n",
      "resetting env. episode 1452, reward total was -14.0. running mean: -12.640877146991823, timestamp: 2022-08-20 10:58:10.731496\n",
      "resetting env. episode 1453, reward total was -11.0. running mean: -12.624468375521904, timestamp: 2022-08-20 10:58:17.680921\n",
      "resetting env. episode 1454, reward total was -13.0. running mean: -12.628223691766685, timestamp: 2022-08-20 10:58:26.737713\n",
      "resetting env. episode 1455, reward total was -13.0. running mean: -12.631941454849018, timestamp: 2022-08-20 10:58:33.904557\n",
      "resetting env. episode 1456, reward total was -13.0. running mean: -12.635622040300529, timestamp: 2022-08-20 10:58:40.546803\n",
      "resetting env. episode 1457, reward total was -13.0. running mean: -12.639265819897524, timestamp: 2022-08-20 10:58:47.039445\n",
      "resetting env. episode 1458, reward total was -14.0. running mean: -12.65287316169855, timestamp: 2022-08-20 10:58:53.358557\n",
      "resetting env. episode 1459, reward total was -8.0. running mean: -12.606344430081563, timestamp: 2022-08-20 10:59:01.501789\n",
      "resetting env. episode 1460, reward total was -15.0. running mean: -12.630280985780749, timestamp: 2022-08-20 10:59:08.374416\n",
      "resetting env. episode 1461, reward total was -8.0. running mean: -12.58397817592294, timestamp: 2022-08-20 10:59:15.966125\n",
      "resetting env. episode 1462, reward total was -13.0. running mean: -12.588138394163712, timestamp: 2022-08-20 10:59:22.769937\n",
      "resetting env. episode 1463, reward total was -14.0. running mean: -12.602257010222075, timestamp: 2022-08-20 10:59:28.935458\n",
      "resetting env. episode 1464, reward total was -9.0. running mean: -12.566234440119855, timestamp: 2022-08-20 10:59:36.725638\n",
      "resetting env. episode 1465, reward total was -14.0. running mean: -12.580572095718656, timestamp: 2022-08-20 10:59:42.776462\n",
      "resetting env. episode 1466, reward total was -9.0. running mean: -12.544766374761469, timestamp: 2022-08-20 10:59:49.463587\n",
      "resetting env. episode 1467, reward total was -14.0. running mean: -12.559318711013855, timestamp: 2022-08-20 10:59:55.644065\n",
      "resetting env. episode 1468, reward total was -14.0. running mean: -12.573725523903716, timestamp: 2022-08-20 11:00:01.494546\n",
      "resetting env. episode 1469, reward total was -15.0. running mean: -12.597988268664679, timestamp: 2022-08-20 11:00:08.207570\n",
      "resetting env. episode 1470, reward total was -17.0. running mean: -12.642008385978032, timestamp: 2022-08-20 11:00:14.539644\n",
      "resetting env. episode 1471, reward total was -10.0. running mean: -12.615588302118251, timestamp: 2022-08-20 11:00:22.309875\n",
      "resetting env. episode 1472, reward total was -21.0. running mean: -12.69943241909707, timestamp: 2022-08-20 11:00:27.680543\n",
      "resetting env. episode 1473, reward total was -10.0. running mean: -12.672438094906099, timestamp: 2022-08-20 11:00:34.163189\n",
      "resetting env. episode 1474, reward total was -11.0. running mean: -12.655713713957036, timestamp: 2022-08-20 11:00:40.958025\n",
      "resetting env. episode 1475, reward total was -14.0. running mean: -12.669156576817466, timestamp: 2022-08-20 11:00:47.361911\n",
      "resetting env. episode 1476, reward total was -2.0. running mean: -12.56246501104929, timestamp: 2022-08-20 11:00:56.736870\n",
      "resetting env. episode 1477, reward total was -11.0. running mean: -12.546840360938797, timestamp: 2022-08-20 11:01:03.209558\n",
      "resetting env. episode 1478, reward total was -13.0. running mean: -12.55137195732941, timestamp: 2022-08-20 11:01:10.669619\n",
      "resetting env. episode 1479, reward total was -16.0. running mean: -12.585858237756115, timestamp: 2022-08-20 11:01:16.432214\n",
      "resetting env. episode 1480, reward total was -9.0. running mean: -12.549999655378555, timestamp: 2022-08-20 11:01:25.268597\n",
      "resetting env. episode 1481, reward total was -11.0. running mean: -12.53449965882477, timestamp: 2022-08-20 11:01:32.762566\n",
      "resetting env. episode 1482, reward total was -15.0. running mean: -12.559154662236521, timestamp: 2022-08-20 11:01:37.886867\n",
      "resetting env. episode 1483, reward total was -12.0. running mean: -12.553563115614155, timestamp: 2022-08-20 11:01:45.406766\n",
      "resetting env. episode 1484, reward total was -12.0. running mean: -12.548027484458013, timestamp: 2022-08-20 11:01:52.764102\n",
      "resetting env. episode 1485, reward total was -17.0. running mean: -12.592547209613432, timestamp: 2022-08-20 11:01:59.277692\n",
      "resetting env. episode 1486, reward total was -14.0. running mean: -12.606621737517298, timestamp: 2022-08-20 11:02:06.411619\n",
      "resetting env. episode 1487, reward total was -13.0. running mean: -12.610555520142125, timestamp: 2022-08-20 11:02:13.607385\n",
      "resetting env. episode 1488, reward total was -14.0. running mean: -12.624449964940704, timestamp: 2022-08-20 11:02:19.609344\n",
      "resetting env. episode 1489, reward total was -9.0. running mean: -12.588205465291296, timestamp: 2022-08-20 11:02:27.721659\n",
      "resetting env. episode 1490, reward total was -11.0. running mean: -12.572323410638383, timestamp: 2022-08-20 11:02:35.143819\n",
      "resetting env. episode 1491, reward total was -16.0. running mean: -12.606600176531998, timestamp: 2022-08-20 11:02:41.812996\n",
      "resetting env. episode 1492, reward total was -8.0. running mean: -12.560534174766678, timestamp: 2022-08-20 11:02:52.993134\n",
      "resetting env. episode 1493, reward total was -8.0. running mean: -12.514928833019011, timestamp: 2022-08-20 11:03:00.989737\n",
      "resetting env. episode 1494, reward total was -17.0. running mean: -12.559779544688821, timestamp: 2022-08-20 11:03:06.143990\n",
      "resetting env. episode 1495, reward total was -14.0. running mean: -12.574181749241934, timestamp: 2022-08-20 11:03:13.665849\n",
      "resetting env. episode 1496, reward total was -16.0. running mean: -12.608439931749514, timestamp: 2022-08-20 11:03:20.219333\n",
      "resetting env. episode 1497, reward total was -10.0. running mean: -12.582355532432018, timestamp: 2022-08-20 11:03:28.216952\n",
      "resetting env. episode 1498, reward total was -9.0. running mean: -12.546531977107698, timestamp: 2022-08-20 11:03:35.960258\n",
      "resetting env. episode 1499, reward total was -14.0. running mean: -12.561066657336621, timestamp: 2022-08-20 11:03:43.387403\n",
      "resetting env. episode 1500, reward total was -14.0. running mean: -12.575455990763254, timestamp: 2022-08-20 11:03:49.669612\n",
      "resetting env. episode 1501, reward total was -11.0. running mean: -12.559701430855622, timestamp: 2022-08-20 11:03:58.030263\n",
      "resetting env. episode 1502, reward total was -11.0. running mean: -12.544104416547064, timestamp: 2022-08-20 11:04:04.293519\n",
      "resetting env. episode 1503, reward total was -9.0. running mean: -12.508663372381593, timestamp: 2022-08-20 11:04:11.082373\n",
      "resetting env. episode 1504, reward total was -13.0. running mean: -12.513576738657777, timestamp: 2022-08-20 11:04:17.553077\n",
      "resetting env. episode 1505, reward total was -14.0. running mean: -12.5284409712712, timestamp: 2022-08-20 11:04:24.786743\n",
      "resetting env. episode 1506, reward total was -15.0. running mean: -12.553156561558488, timestamp: 2022-08-20 11:04:31.379122\n",
      "resetting env. episode 1507, reward total was -15.0. running mean: -12.577624995942903, timestamp: 2022-08-20 11:04:37.582539\n",
      "resetting env. episode 1508, reward total was -16.0. running mean: -12.611848745983474, timestamp: 2022-08-20 11:04:44.694528\n",
      "resetting env. episode 1509, reward total was -13.0. running mean: -12.61573025852364, timestamp: 2022-08-20 11:04:51.549209\n",
      "resetting env. episode 1510, reward total was -18.0. running mean: -12.669572955938404, timestamp: 2022-08-20 11:04:57.321781\n",
      "resetting env. episode 1511, reward total was -15.0. running mean: -12.692877226379021, timestamp: 2022-08-20 11:05:03.162165\n",
      "resetting env. episode 1512, reward total was -9.0. running mean: -12.655948454115231, timestamp: 2022-08-20 11:05:12.166097\n",
      "resetting env. episode 1513, reward total was -14.0. running mean: -12.66938896957408, timestamp: 2022-08-20 11:05:20.377151\n",
      "resetting env. episode 1514, reward total was -11.0. running mean: -12.652695079878338, timestamp: 2022-08-20 11:05:34.440635\n",
      "resetting env. episode 1515, reward total was -8.0. running mean: -12.606168129079554, timestamp: 2022-08-20 11:05:45.456191\n",
      "resetting env. episode 1516, reward total was -17.0. running mean: -12.650106447788758, timestamp: 2022-08-20 11:05:52.665921\n",
      "resetting env. episode 1517, reward total was -7.0. running mean: -12.593605383310871, timestamp: 2022-08-20 11:06:01.410547\n",
      "resetting env. episode 1518, reward total was -13.0. running mean: -12.597669329477764, timestamp: 2022-08-20 11:06:09.245603\n",
      "resetting env. episode 1519, reward total was -7.0. running mean: -12.541692636182987, timestamp: 2022-08-20 11:06:19.131177\n",
      "resetting env. episode 1520, reward total was -12.0. running mean: -12.536275709821156, timestamp: 2022-08-20 11:06:25.526086\n",
      "resetting env. episode 1521, reward total was -15.0. running mean: -12.560912952722944, timestamp: 2022-08-20 11:06:31.666676\n",
      "resetting env. episode 1522, reward total was -18.0. running mean: -12.615303823195715, timestamp: 2022-08-20 11:06:39.492754\n",
      "resetting env. episode 1523, reward total was -8.0. running mean: -12.569150784963758, timestamp: 2022-08-20 11:06:50.129320\n",
      "resetting env. episode 1524, reward total was -14.0. running mean: -12.583459277114121, timestamp: 2022-08-20 11:06:58.985677\n",
      "resetting env. episode 1525, reward total was -15.0. running mean: -12.60762468434298, timestamp: 2022-08-20 11:07:08.101283\n",
      "resetting env. episode 1526, reward total was -10.0. running mean: -12.58154843749955, timestamp: 2022-08-20 11:07:18.914388\n",
      "resetting env. episode 1527, reward total was -11.0. running mean: -12.565732953124552, timestamp: 2022-08-20 11:07:27.454550\n",
      "resetting env. episode 1528, reward total was -14.0. running mean: -12.580075623593308, timestamp: 2022-08-20 11:07:37.265328\n",
      "resetting env. episode 1529, reward total was -11.0. running mean: -12.564274867357375, timestamp: 2022-08-20 11:07:48.151231\n",
      "resetting env. episode 1530, reward total was -17.0. running mean: -12.6086321186838, timestamp: 2022-08-20 11:07:55.996258\n",
      "resetting env. episode 1531, reward total was -15.0. running mean: -12.632545797496963, timestamp: 2022-08-20 11:08:04.263159\n",
      "resetting env. episode 1532, reward total was -14.0. running mean: -12.646220339521994, timestamp: 2022-08-20 11:08:13.796680\n",
      "resetting env. episode 1533, reward total was -9.0. running mean: -12.609758136126773, timestamp: 2022-08-20 11:08:24.716488\n",
      "resetting env. episode 1534, reward total was -15.0. running mean: -12.633660554765505, timestamp: 2022-08-20 11:08:35.023935\n",
      "resetting env. episode 1535, reward total was -6.0. running mean: -12.567323949217851, timestamp: 2022-08-20 11:08:47.047797\n",
      "resetting env. episode 1536, reward total was -13.0. running mean: -12.571650709725674, timestamp: 2022-08-20 11:08:58.587971\n",
      "resetting env. episode 1537, reward total was -19.0. running mean: -12.635934202628416, timestamp: 2022-08-20 11:09:07.390420\n",
      "resetting env. episode 1538, reward total was -13.0. running mean: -12.639574860602133, timestamp: 2022-08-20 11:09:19.275653\n",
      "resetting env. episode 1539, reward total was -13.0. running mean: -12.643179111996112, timestamp: 2022-08-20 11:09:29.736689\n",
      "resetting env. episode 1540, reward total was -14.0. running mean: -12.656747320876152, timestamp: 2022-08-20 11:09:40.977642\n",
      "resetting env. episode 1541, reward total was -14.0. running mean: -12.67017984766739, timestamp: 2022-08-20 11:09:53.229893\n",
      "resetting env. episode 1542, reward total was -13.0. running mean: -12.673478049190717, timestamp: 2022-08-20 11:10:05.748430\n",
      "resetting env. episode 1543, reward total was -8.0. running mean: -12.62674326869881, timestamp: 2022-08-20 11:10:20.979718\n",
      "resetting env. episode 1544, reward total was -11.0. running mean: -12.61047583601182, timestamp: 2022-08-20 11:10:32.144873\n",
      "resetting env. episode 1545, reward total was -10.0. running mean: -12.584371077651703, timestamp: 2022-08-20 11:10:46.852233\n",
      "resetting env. episode 1546, reward total was -17.0. running mean: -12.628527366875186, timestamp: 2022-08-20 11:10:57.152381\n",
      "resetting env. episode 1547, reward total was -10.0. running mean: -12.602242093206433, timestamp: 2022-08-20 11:11:08.608377\n",
      "resetting env. episode 1548, reward total was -10.0. running mean: -12.576219672274368, timestamp: 2022-08-20 11:11:22.152173\n",
      "resetting env. episode 1549, reward total was -15.0. running mean: -12.600457475551625, timestamp: 2022-08-20 11:11:35.790718\n",
      "resetting env. episode 1550, reward total was -17.0. running mean: -12.644452900796109, timestamp: 2022-08-20 11:11:49.289635\n",
      "resetting env. episode 1551, reward total was -13.0. running mean: -12.64800837178815, timestamp: 2022-08-20 11:12:02.842412\n",
      "resetting env. episode 1552, reward total was -17.0. running mean: -12.691528288070268, timestamp: 2022-08-20 11:12:12.924461\n",
      "resetting env. episode 1553, reward total was -2.0. running mean: -12.584613005189565, timestamp: 2022-08-20 11:12:27.458609\n",
      "resetting env. episode 1554, reward total was -7.0. running mean: -12.52876687513767, timestamp: 2022-08-20 11:12:45.787617\n",
      "resetting env. episode 1555, reward total was -8.0. running mean: -12.483479206386294, timestamp: 2022-08-20 11:12:57.210129\n",
      "resetting env. episode 1556, reward total was -13.0. running mean: -12.488644414322431, timestamp: 2022-08-20 11:13:11.278510\n",
      "resetting env. episode 1557, reward total was -13.0. running mean: -12.493757970179209, timestamp: 2022-08-20 11:13:23.702627\n",
      "resetting env. episode 1558, reward total was -3.0. running mean: -12.398820390477416, timestamp: 2022-08-20 11:13:37.895684\n",
      "resetting env. episode 1559, reward total was -17.0. running mean: -12.444832186572642, timestamp: 2022-08-20 11:13:48.139301\n",
      "resetting env. episode 1560, reward total was -6.0. running mean: -12.380383864706916, timestamp: 2022-08-20 11:14:01.432765\n",
      "resetting env. episode 1561, reward total was 2.0. running mean: -12.236580026059848, timestamp: 2022-08-20 11:14:18.144097\n",
      "resetting env. episode 1562, reward total was 2.0. running mean: -12.094214225799249, timestamp: 2022-08-20 11:14:37.171238\n",
      "resetting env. episode 1563, reward total was -9.0. running mean: -12.063272083541255, timestamp: 2022-08-20 11:14:53.481227\n",
      "resetting env. episode 1564, reward total was -15.0. running mean: -12.092639362705842, timestamp: 2022-08-20 11:15:04.196587\n",
      "resetting env. episode 1565, reward total was -13.0. running mean: -12.101712969078784, timestamp: 2022-08-20 11:15:19.423808\n",
      "resetting env. episode 1566, reward total was -11.0. running mean: -12.090695839387996, timestamp: 2022-08-20 11:15:31.842638\n",
      "resetting env. episode 1567, reward total was -10.0. running mean: -12.069788880994116, timestamp: 2022-08-20 11:15:48.755631\n",
      "resetting env. episode 1568, reward total was -11.0. running mean: -12.059090992184174, timestamp: 2022-08-20 11:16:04.234354\n",
      "resetting env. episode 1569, reward total was -10.0. running mean: -12.038500082262331, timestamp: 2022-08-20 11:16:25.379843\n",
      "resetting env. episode 1570, reward total was -7.0. running mean: -11.988115081439709, timestamp: 2022-08-20 11:16:46.641600\n",
      "resetting env. episode 1571, reward total was -16.0. running mean: -12.028233930625312, timestamp: 2022-08-20 11:17:01.720298\n",
      "resetting env. episode 1572, reward total was -15.0. running mean: -12.057951591319059, timestamp: 2022-08-20 11:17:14.664701\n",
      "resetting env. episode 1573, reward total was -15.0. running mean: -12.087372075405868, timestamp: 2022-08-20 11:17:28.131733\n",
      "resetting env. episode 1574, reward total was -15.0. running mean: -12.11649835465181, timestamp: 2022-08-20 11:17:37.818804\n",
      "resetting env. episode 1575, reward total was -12.0. running mean: -12.115333371105292, timestamp: 2022-08-20 11:17:50.763206\n",
      "resetting env. episode 1576, reward total was -9.0. running mean: -12.084180037394239, timestamp: 2022-08-20 11:18:01.778826\n",
      "resetting env. episode 1577, reward total was -14.0. running mean: -12.103338237020298, timestamp: 2022-08-20 11:18:10.083629\n",
      "resetting env. episode 1578, reward total was -8.0. running mean: -12.062304854650094, timestamp: 2022-08-20 11:18:20.946592\n",
      "resetting env. episode 1579, reward total was -8.0. running mean: -12.021681806103594, timestamp: 2022-08-20 11:18:30.896025\n",
      "resetting env. episode 1580, reward total was -12.0. running mean: -12.021464988042556, timestamp: 2022-08-20 11:18:39.887960\n",
      "resetting env. episode 1581, reward total was -11.0. running mean: -12.01125033816213, timestamp: 2022-08-20 11:18:49.829388\n",
      "resetting env. episode 1582, reward total was -14.0. running mean: -12.031137834780509, timestamp: 2022-08-20 11:18:57.563715\n",
      "resetting env. episode 1583, reward total was -12.0. running mean: -12.030826456432703, timestamp: 2022-08-20 11:19:07.044384\n",
      "resetting env. episode 1584, reward total was -13.0. running mean: -12.040518191868376, timestamp: 2022-08-20 11:19:15.687270\n",
      "resetting env. episode 1585, reward total was -8.0. running mean: -12.000113009949692, timestamp: 2022-08-20 11:19:26.398639\n",
      "resetting env. episode 1586, reward total was -13.0. running mean: -12.010111879850195, timestamp: 2022-08-20 11:19:36.724039\n",
      "resetting env. episode 1587, reward total was -10.0. running mean: -11.990010761051693, timestamp: 2022-08-20 11:19:48.503550\n",
      "resetting env. episode 1588, reward total was -9.0. running mean: -11.960110653441175, timestamp: 2022-08-20 11:19:59.968976\n",
      "resetting env. episode 1589, reward total was -15.0. running mean: -11.990509546906763, timestamp: 2022-08-20 11:20:09.820272\n",
      "resetting env. episode 1590, reward total was -12.0. running mean: -11.990604451437694, timestamp: 2022-08-20 11:20:18.356931\n",
      "resetting env. episode 1591, reward total was -15.0. running mean: -12.020698406923318, timestamp: 2022-08-20 11:20:30.085645\n",
      "resetting env. episode 1592, reward total was -14.0. running mean: -12.040491422854085, timestamp: 2022-08-20 11:20:41.525721\n",
      "resetting env. episode 1593, reward total was -7.0. running mean: -11.990086508625545, timestamp: 2022-08-20 11:20:52.736511\n",
      "resetting env. episode 1594, reward total was -8.0. running mean: -11.950185643539289, timestamp: 2022-08-20 11:21:05.217668\n",
      "resetting env. episode 1595, reward total was -5.0. running mean: -11.880683787103896, timestamp: 2022-08-20 11:21:16.737906\n",
      "resetting env. episode 1596, reward total was -19.0. running mean: -11.951876949232856, timestamp: 2022-08-20 11:21:24.384436\n",
      "resetting env. episode 1597, reward total was -2.0. running mean: -11.852358179740527, timestamp: 2022-08-20 11:21:36.079176\n",
      "resetting env. episode 1598, reward total was -12.0. running mean: -11.853834597943122, timestamp: 2022-08-20 11:21:44.065826\n",
      "resetting env. episode 1599, reward total was -13.0. running mean: -11.865296251963692, timestamp: 2022-08-20 11:21:51.913848\n",
      "resetting env. episode 1600, reward total was -17.0. running mean: -11.916643289444055, timestamp: 2022-08-20 11:22:01.845620\n",
      "resetting env. episode 1601, reward total was -13.0. running mean: -11.927476856549616, timestamp: 2022-08-20 11:22:12.220889\n",
      "resetting env. episode 1602, reward total was -15.0. running mean: -11.95820208798412, timestamp: 2022-08-20 11:22:19.976166\n",
      "resetting env. episode 1603, reward total was -7.0. running mean: -11.908620067104279, timestamp: 2022-08-20 11:22:31.311856\n",
      "resetting env. episode 1604, reward total was -10.0. running mean: -11.889533866433235, timestamp: 2022-08-20 11:22:40.950098\n",
      "resetting env. episode 1605, reward total was -7.0. running mean: -11.840638527768903, timestamp: 2022-08-20 11:22:56.814685\n",
      "resetting env. episode 1606, reward total was -9.0. running mean: -11.812232142491213, timestamp: 2022-08-20 11:23:11.569248\n",
      "resetting env. episode 1607, reward total was -7.0. running mean: -11.7641098210663, timestamp: 2022-08-20 11:23:21.865723\n",
      "resetting env. episode 1608, reward total was 1.0. running mean: -11.636468722855637, timestamp: 2022-08-20 11:23:33.655211\n",
      "resetting env. episode 1609, reward total was -18.0. running mean: -11.700104035627081, timestamp: 2022-08-20 11:23:45.167439\n",
      "resetting env. episode 1610, reward total was -10.0. running mean: -11.68310299527081, timestamp: 2022-08-20 11:23:56.386451\n",
      "resetting env. episode 1611, reward total was -12.0. running mean: -11.6862719653181, timestamp: 2022-08-20 11:24:08.476133\n",
      "resetting env. episode 1612, reward total was -13.0. running mean: -11.69940924566492, timestamp: 2022-08-20 11:24:17.682530\n",
      "resetting env. episode 1613, reward total was -13.0. running mean: -11.712415153208273, timestamp: 2022-08-20 11:24:26.537856\n",
      "resetting env. episode 1614, reward total was -13.0. running mean: -11.72529100167619, timestamp: 2022-08-20 11:24:35.981613\n",
      "resetting env. episode 1615, reward total was -15.0. running mean: -11.75803809165943, timestamp: 2022-08-20 11:24:46.703951\n",
      "resetting env. episode 1616, reward total was -9.0. running mean: -11.730457710742835, timestamp: 2022-08-20 11:24:57.734468\n",
      "resetting env. episode 1617, reward total was -10.0. running mean: -11.713153133635405, timestamp: 2022-08-20 11:25:07.965291\n",
      "resetting env. episode 1618, reward total was -17.0. running mean: -11.76602160229905, timestamp: 2022-08-20 11:25:18.899126\n",
      "resetting env. episode 1619, reward total was -11.0. running mean: -11.758361386276059, timestamp: 2022-08-20 11:25:31.761740\n",
      "resetting env. episode 1620, reward total was -18.0. running mean: -11.820777772413297, timestamp: 2022-08-20 11:25:41.049910\n",
      "resetting env. episode 1621, reward total was -15.0. running mean: -11.852569994689164, timestamp: 2022-08-20 11:25:49.940147\n",
      "resetting env. episode 1622, reward total was -7.0. running mean: -11.804044294742273, timestamp: 2022-08-20 11:26:02.276177\n",
      "resetting env. episode 1623, reward total was -12.0. running mean: -11.806003851794848, timestamp: 2022-08-20 11:26:12.343268\n",
      "resetting env. episode 1624, reward total was -14.0. running mean: -11.827943813276901, timestamp: 2022-08-20 11:26:20.745803\n",
      "resetting env. episode 1625, reward total was -11.0. running mean: -11.819664375144132, timestamp: 2022-08-20 11:26:29.760707\n",
      "resetting env. episode 1626, reward total was -8.0. running mean: -11.781467731392691, timestamp: 2022-08-20 11:26:39.654261\n",
      "resetting env. episode 1627, reward total was -11.0. running mean: -11.773653054078764, timestamp: 2022-08-20 11:26:49.589704\n",
      "resetting env. episode 1628, reward total was -2.0. running mean: -11.675916523537976, timestamp: 2022-08-20 11:27:03.447661\n",
      "resetting env. episode 1629, reward total was -14.0. running mean: -11.699157358302598, timestamp: 2022-08-20 11:27:13.236494\n",
      "resetting env. episode 1630, reward total was -13.0. running mean: -11.712165784719572, timestamp: 2022-08-20 11:27:21.039638\n",
      "resetting env. episode 1631, reward total was -19.0. running mean: -11.785044126872375, timestamp: 2022-08-20 11:27:28.008013\n",
      "resetting env. episode 1632, reward total was -6.0. running mean: -11.727193685603652, timestamp: 2022-08-20 11:27:39.449578\n",
      "resetting env. episode 1633, reward total was -8.0. running mean: -11.689921748747615, timestamp: 2022-08-20 11:27:54.555204\n",
      "resetting env. episode 1634, reward total was -9.0. running mean: -11.663022531260138, timestamp: 2022-08-20 11:28:08.900856\n",
      "resetting env. episode 1635, reward total was -9.0. running mean: -11.636392305947536, timestamp: 2022-08-20 11:28:25.773753\n",
      "resetting env. episode 1636, reward total was -9.0. running mean: -11.61002838288806, timestamp: 2022-08-20 11:28:41.301248\n",
      "resetting env. episode 1637, reward total was -13.0. running mean: -11.623928099059182, timestamp: 2022-08-20 11:28:50.069810\n",
      "resetting env. episode 1638, reward total was -11.0. running mean: -11.617688818068588, timestamp: 2022-08-20 11:29:02.103643\n",
      "resetting env. episode 1639, reward total was -9.0. running mean: -11.591511929887902, timestamp: 2022-08-20 11:29:13.506163\n",
      "resetting env. episode 1640, reward total was -2.0. running mean: -11.495596810589022, timestamp: 2022-08-20 11:29:30.782985\n",
      "resetting env. episode 1641, reward total was -11.0. running mean: -11.490640842483131, timestamp: 2022-08-20 11:29:41.496352\n",
      "resetting env. episode 1642, reward total was -11.0. running mean: -11.4857344340583, timestamp: 2022-08-20 11:29:51.851672\n",
      "resetting env. episode 1643, reward total was -10.0. running mean: -11.470877089717716, timestamp: 2022-08-20 11:30:03.187383\n",
      "resetting env. episode 1644, reward total was -2.0. running mean: -11.376168318820538, timestamp: 2022-08-20 11:30:14.669675\n",
      "resetting env. episode 1645, reward total was 2.0. running mean: -11.242406635632333, timestamp: 2022-08-20 11:30:27.050610\n",
      "resetting env. episode 1646, reward total was -9.0. running mean: -11.21998256927601, timestamp: 2022-08-20 11:30:37.684158\n",
      "resetting env. episode 1647, reward total was -5.0. running mean: -11.15778274358325, timestamp: 2022-08-20 11:30:48.383560\n",
      "resetting env. episode 1648, reward total was -5.0. running mean: -11.096204916147418, timestamp: 2022-08-20 11:30:59.692330\n",
      "resetting env. episode 1649, reward total was -9.0. running mean: -11.075242866985944, timestamp: 2022-08-20 11:31:09.984816\n",
      "resetting env. episode 1650, reward total was -9.0. running mean: -11.054490438316083, timestamp: 2022-08-20 11:31:21.264665\n",
      "resetting env. episode 1651, reward total was -19.0. running mean: -11.133945533932922, timestamp: 2022-08-20 11:31:30.333428\n",
      "resetting env. episode 1652, reward total was -11.0. running mean: -11.132606078593591, timestamp: 2022-08-20 11:31:41.752906\n",
      "resetting env. episode 1653, reward total was -8.0. running mean: -11.101280017807655, timestamp: 2022-08-20 11:31:51.127844\n",
      "resetting env. episode 1654, reward total was -9.0. running mean: -11.080267217629578, timestamp: 2022-08-20 11:32:01.516077\n",
      "resetting env. episode 1655, reward total was -16.0. running mean: -11.129464545453281, timestamp: 2022-08-20 11:32:11.179248\n",
      "resetting env. episode 1656, reward total was -11.0. running mean: -11.128169899998747, timestamp: 2022-08-20 11:32:28.377275\n",
      "resetting env. episode 1657, reward total was -18.0. running mean: -11.19688820099876, timestamp: 2022-08-20 11:32:40.383182\n",
      "resetting env. episode 1658, reward total was -10.0. running mean: -11.184919318988772, timestamp: 2022-08-20 11:32:55.386078\n",
      "resetting env. episode 1659, reward total was -1.0. running mean: -11.083070125798884, timestamp: 2022-08-20 11:33:13.784898\n",
      "resetting env. episode 1660, reward total was -16.0. running mean: -11.132239424540895, timestamp: 2022-08-20 11:33:26.979629\n",
      "resetting env. episode 1661, reward total was -4.0. running mean: -11.060917030295485, timestamp: 2022-08-20 11:33:41.962583\n",
      "resetting env. episode 1662, reward total was -13.0. running mean: -11.080307859992532, timestamp: 2022-08-20 11:33:53.949539\n",
      "resetting env. episode 1663, reward total was -12.0. running mean: -11.089504781392606, timestamp: 2022-08-20 11:34:03.890990\n",
      "resetting env. episode 1664, reward total was -15.0. running mean: -11.12860973357868, timestamp: 2022-08-20 11:34:14.104808\n",
      "resetting env. episode 1665, reward total was -10.0. running mean: -11.117323636242892, timestamp: 2022-08-20 11:34:26.620723\n",
      "resetting env. episode 1666, reward total was -18.0. running mean: -11.186150399880463, timestamp: 2022-08-20 11:34:39.842582\n",
      "resetting env. episode 1667, reward total was -9.0. running mean: -11.164288895881658, timestamp: 2022-08-20 11:34:49.296320\n",
      "resetting env. episode 1668, reward total was -9.0. running mean: -11.14264600692284, timestamp: 2022-08-20 11:34:57.706828\n",
      "resetting env. episode 1669, reward total was -6.0. running mean: -11.091219546853612, timestamp: 2022-08-20 11:35:09.753624\n",
      "resetting env. episode 1670, reward total was 3.0. running mean: -10.950307351385076, timestamp: 2022-08-20 11:35:22.268175\n",
      "resetting env. episode 1671, reward total was -7.0. running mean: -10.910804277871225, timestamp: 2022-08-20 11:35:33.157068\n",
      "resetting env. episode 1672, reward total was -13.0. running mean: -10.931696235092513, timestamp: 2022-08-20 11:35:41.772039\n",
      "resetting env. episode 1673, reward total was -8.0. running mean: -10.902379272741587, timestamp: 2022-08-20 11:35:50.687225\n",
      "resetting env. episode 1674, reward total was -12.0. running mean: -10.91335548001417, timestamp: 2022-08-20 11:35:58.182178\n",
      "resetting env. episode 1675, reward total was -11.0. running mean: -10.914221925214028, timestamp: 2022-08-20 11:36:09.489950\n",
      "resetting env. episode 1676, reward total was -19.0. running mean: -10.995079705961887, timestamp: 2022-08-20 11:36:18.535771\n",
      "resetting env. episode 1677, reward total was -17.0. running mean: -11.055128908902267, timestamp: 2022-08-20 11:36:26.770762\n",
      "resetting env. episode 1678, reward total was -13.0. running mean: -11.074577619813246, timestamp: 2022-08-20 11:36:41.085310\n",
      "resetting env. episode 1679, reward total was -17.0. running mean: -11.133831843615113, timestamp: 2022-08-20 11:36:51.382796\n",
      "resetting env. episode 1680, reward total was -16.0. running mean: -11.18249352517896, timestamp: 2022-08-20 11:37:01.181593\n",
      "resetting env. episode 1681, reward total was -6.0. running mean: -11.130668589927172, timestamp: 2022-08-20 11:37:14.796719\n",
      "resetting env. episode 1682, reward total was -7.0. running mean: -11.0893619040279, timestamp: 2022-08-20 11:37:24.898198\n",
      "resetting env. episode 1683, reward total was -14.0. running mean: -11.118468284987621, timestamp: 2022-08-20 11:37:33.701238\n",
      "resetting env. episode 1684, reward total was -17.0. running mean: -11.177283602137745, timestamp: 2022-08-20 11:37:41.738758\n",
      "resetting env. episode 1685, reward total was -4.0. running mean: -11.105510766116367, timestamp: 2022-08-20 11:37:51.013964\n",
      "resetting env. episode 1686, reward total was -13.0. running mean: -11.124455658455204, timestamp: 2022-08-20 11:38:01.231665\n",
      "resetting env. episode 1687, reward total was -19.0. running mean: -11.203211101870652, timestamp: 2022-08-20 11:38:08.414454\n",
      "resetting env. episode 1688, reward total was -17.0. running mean: -11.261178990851946, timestamp: 2022-08-20 11:38:20.363766\n",
      "resetting env. episode 1689, reward total was -17.0. running mean: -11.318567200943425, timestamp: 2022-08-20 11:38:29.262925\n",
      "resetting env. episode 1690, reward total was -5.0. running mean: -11.255381528933992, timestamp: 2022-08-20 11:38:42.978297\n",
      "resetting env. episode 1691, reward total was -12.0. running mean: -11.26282771364465, timestamp: 2022-08-20 11:38:53.221916\n",
      "resetting env. episode 1692, reward total was -13.0. running mean: -11.280199436508205, timestamp: 2022-08-20 11:39:01.311294\n",
      "resetting env. episode 1693, reward total was -11.0. running mean: -11.277397442143123, timestamp: 2022-08-20 11:39:09.161309\n",
      "resetting env. episode 1694, reward total was -11.0. running mean: -11.27462346772169, timestamp: 2022-08-20 11:39:16.386001\n",
      "resetting env. episode 1695, reward total was -10.0. running mean: -11.261877233044473, timestamp: 2022-08-20 11:39:24.813472\n",
      "resetting env. episode 1696, reward total was -14.0. running mean: -11.289258460714029, timestamp: 2022-08-20 11:39:32.790150\n",
      "resetting env. episode 1697, reward total was -14.0. running mean: -11.31636587610689, timestamp: 2022-08-20 11:39:41.981617\n",
      "resetting env. episode 1698, reward total was -7.0. running mean: -11.27320221734582, timestamp: 2022-08-20 11:39:53.493134\n",
      "resetting env. episode 1699, reward total was -11.0. running mean: -11.270470195172361, timestamp: 2022-08-20 11:40:03.223124\n",
      "resetting env. episode 1700, reward total was -14.0. running mean: -11.297765493220638, timestamp: 2022-08-20 11:40:12.073469\n",
      "resetting env. episode 1701, reward total was -15.0. running mean: -11.334787838288431, timestamp: 2022-08-20 11:40:18.750621\n",
      "resetting env. episode 1702, reward total was -6.0. running mean: -11.281439959905548, timestamp: 2022-08-20 11:40:28.467645\n",
      "resetting env. episode 1703, reward total was -15.0. running mean: -11.318625560306492, timestamp: 2022-08-20 11:40:36.585952\n",
      "resetting env. episode 1704, reward total was -15.0. running mean: -11.355439304703427, timestamp: 2022-08-20 11:40:43.351866\n",
      "resetting env. episode 1705, reward total was -11.0. running mean: -11.351884911656393, timestamp: 2022-08-20 11:40:51.217868\n",
      "resetting env. episode 1706, reward total was -11.0. running mean: -11.348366062539828, timestamp: 2022-08-20 11:41:01.547227\n",
      "resetting env. episode 1707, reward total was 3.0. running mean: -11.20488240191443, timestamp: 2022-08-20 11:41:08.913532\n",
      "resetting env. episode 1708, reward total was -8.0. running mean: -11.172833577895286, timestamp: 2022-08-20 11:41:17.720989\n",
      "resetting env. episode 1709, reward total was -11.0. running mean: -11.171105242116333, timestamp: 2022-08-20 11:41:25.948047\n",
      "resetting env. episode 1710, reward total was -12.0. running mean: -11.179394189695168, timestamp: 2022-08-20 11:41:33.283392\n",
      "resetting env. episode 1711, reward total was -10.0. running mean: -11.167600247798216, timestamp: 2022-08-20 11:41:42.054944\n",
      "resetting env. episode 1712, reward total was -9.0. running mean: -11.145924245320234, timestamp: 2022-08-20 11:41:47.653980\n",
      "resetting env. episode 1713, reward total was -10.0. running mean: -11.134465002867032, timestamp: 2022-08-20 11:41:55.638638\n",
      "resetting env. episode 1714, reward total was -6.0. running mean: -11.083120352838362, timestamp: 2022-08-20 11:42:04.581733\n",
      "resetting env. episode 1715, reward total was -20.0. running mean: -11.172289149309977, timestamp: 2022-08-20 11:42:11.643857\n",
      "resetting env. episode 1716, reward total was -19.0. running mean: -11.250566257816876, timestamp: 2022-08-20 11:42:17.375534\n",
      "resetting env. episode 1717, reward total was -15.0. running mean: -11.288060595238708, timestamp: 2022-08-20 11:42:24.545370\n",
      "resetting env. episode 1718, reward total was -10.0. running mean: -11.27517998928632, timestamp: 2022-08-20 11:42:32.297647\n",
      "resetting env. episode 1719, reward total was -13.0. running mean: -11.292428189393458, timestamp: 2022-08-20 11:42:40.350124\n",
      "resetting env. episode 1720, reward total was -11.0. running mean: -11.289503907499522, timestamp: 2022-08-20 11:42:47.988736\n",
      "resetting env. episode 1721, reward total was -11.0. running mean: -11.286608868424526, timestamp: 2022-08-20 11:42:56.030212\n",
      "resetting env. episode 1722, reward total was -12.0. running mean: -11.29374277974028, timestamp: 2022-08-20 11:43:03.577040\n",
      "resetting env. episode 1723, reward total was -5.0. running mean: -11.230805351942877, timestamp: 2022-08-20 11:43:12.407433\n",
      "resetting env. episode 1724, reward total was -10.0. running mean: -11.218497298423447, timestamp: 2022-08-20 11:43:20.131677\n",
      "resetting env. episode 1725, reward total was -15.0. running mean: -11.256312325439213, timestamp: 2022-08-20 11:43:27.958758\n",
      "resetting env. episode 1726, reward total was -4.0. running mean: -11.183749202184819, timestamp: 2022-08-20 11:43:37.840343\n",
      "resetting env. episode 1727, reward total was -14.0. running mean: -11.21191171016297, timestamp: 2022-08-20 11:43:45.858910\n",
      "resetting env. episode 1728, reward total was -7.0. running mean: -11.169792593061342, timestamp: 2022-08-20 11:43:54.351208\n",
      "resetting env. episode 1729, reward total was -9.0. running mean: -11.148094667130728, timestamp: 2022-08-20 11:44:01.498108\n",
      "resetting env. episode 1730, reward total was -9.0. running mean: -11.12661372045942, timestamp: 2022-08-20 11:44:08.918274\n",
      "resetting env. episode 1731, reward total was -16.0. running mean: -11.175347583254826, timestamp: 2022-08-20 11:44:15.488708\n",
      "resetting env. episode 1732, reward total was -6.0. running mean: -11.123594107422278, timestamp: 2022-08-20 11:44:25.682459\n",
      "resetting env. episode 1733, reward total was -12.0. running mean: -11.132358166348054, timestamp: 2022-08-20 11:44:35.248923\n",
      "resetting env. episode 1734, reward total was -7.0. running mean: -11.091034584684573, timestamp: 2022-08-20 11:44:44.431345\n",
      "resetting env. episode 1735, reward total was -15.0. running mean: -11.130124238837729, timestamp: 2022-08-20 11:44:51.053645\n",
      "resetting env. episode 1736, reward total was -13.0. running mean: -11.148822996449352, timestamp: 2022-08-20 11:44:58.415962\n",
      "resetting env. episode 1737, reward total was -17.0. running mean: -11.207334766484859, timestamp: 2022-08-20 11:45:04.632348\n",
      "resetting env. episode 1738, reward total was -13.0. running mean: -11.225261418820011, timestamp: 2022-08-20 11:45:12.213086\n",
      "resetting env. episode 1739, reward total was -8.0. running mean: -11.19300880463181, timestamp: 2022-08-20 11:45:21.185401\n",
      "resetting env. episode 1740, reward total was -18.0. running mean: -11.261078716585493, timestamp: 2022-08-20 11:45:30.521465\n",
      "resetting env. episode 1741, reward total was -3.0. running mean: -11.178467929419638, timestamp: 2022-08-20 11:45:40.763061\n",
      "resetting env. episode 1742, reward total was -14.0. running mean: -11.206683250125442, timestamp: 2022-08-20 11:45:48.072525\n",
      "resetting env. episode 1743, reward total was -10.0. running mean: -11.194616417624188, timestamp: 2022-08-20 11:45:58.362024\n",
      "resetting env. episode 1744, reward total was -6.0. running mean: -11.142670253447946, timestamp: 2022-08-20 11:46:08.401186\n",
      "resetting env. episode 1745, reward total was -5.0. running mean: -11.081243550913467, timestamp: 2022-08-20 11:46:20.648451\n",
      "resetting env. episode 1746, reward total was -4.0. running mean: -11.01043111540433, timestamp: 2022-08-20 11:46:32.777029\n",
      "resetting env. episode 1747, reward total was -15.0. running mean: -11.050326804250288, timestamp: 2022-08-20 11:46:42.407335\n",
      "resetting env. episode 1748, reward total was -8.0. running mean: -11.019823536207785, timestamp: 2022-08-20 11:46:53.104740\n",
      "resetting env. episode 1749, reward total was -8.0. running mean: -10.989625300845706, timestamp: 2022-08-20 11:47:01.819482\n",
      "resetting env. episode 1750, reward total was -17.0. running mean: -11.049729047837248, timestamp: 2022-08-20 11:47:09.342339\n",
      "resetting env. episode 1751, reward total was -4.0. running mean: -10.979231757358875, timestamp: 2022-08-20 11:47:18.672397\n",
      "resetting env. episode 1752, reward total was -7.0. running mean: -10.939439439785286, timestamp: 2022-08-20 11:47:27.355192\n",
      "resetting env. episode 1753, reward total was -6.0. running mean: -10.890045045387433, timestamp: 2022-08-20 11:47:36.752070\n",
      "resetting env. episode 1754, reward total was -14.0. running mean: -10.921144594933558, timestamp: 2022-08-20 11:47:44.853439\n",
      "resetting env. episode 1755, reward total was -14.0. running mean: -10.951933148984223, timestamp: 2022-08-20 11:47:52.520920\n",
      "resetting env. episode 1756, reward total was -11.0. running mean: -10.95241381749438, timestamp: 2022-08-20 11:47:59.303792\n",
      "resetting env. episode 1757, reward total was -12.0. running mean: -10.962889679319435, timestamp: 2022-08-20 11:48:07.707330\n",
      "resetting env. episode 1758, reward total was -14.0. running mean: -10.99326078252624, timestamp: 2022-08-20 11:48:14.224910\n",
      "resetting env. episode 1759, reward total was -13.0. running mean: -11.013328174700979, timestamp: 2022-08-20 11:48:21.585234\n",
      "resetting env. episode 1760, reward total was -10.0. running mean: -11.00319489295397, timestamp: 2022-08-20 11:48:29.755393\n",
      "resetting env. episode 1761, reward total was -2.0. running mean: -10.913162944024428, timestamp: 2022-08-20 11:48:40.135675\n",
      "resetting env. episode 1762, reward total was -13.0. running mean: -10.934031314584185, timestamp: 2022-08-20 11:48:46.730023\n",
      "resetting env. episode 1763, reward total was -2.0. running mean: -10.844691001438344, timestamp: 2022-08-20 11:48:57.847306\n",
      "resetting env. episode 1764, reward total was -9.0. running mean: -10.82624409142396, timestamp: 2022-08-20 11:49:07.267125\n",
      "resetting env. episode 1765, reward total was -13.0. running mean: -10.847981650509722, timestamp: 2022-08-20 11:49:14.822928\n",
      "resetting env. episode 1766, reward total was -17.0. running mean: -10.909501834004624, timestamp: 2022-08-20 11:49:23.575531\n",
      "resetting env. episode 1767, reward total was -9.0. running mean: -10.890406815664578, timestamp: 2022-08-20 11:49:30.715447\n",
      "resetting env. episode 1768, reward total was -3.0. running mean: -10.811502747507932, timestamp: 2022-08-20 11:49:40.994968\n",
      "resetting env. episode 1769, reward total was -7.0. running mean: -10.773387720032852, timestamp: 2022-08-20 11:49:49.809942\n",
      "resetting env. episode 1770, reward total was -17.0. running mean: -10.835653842832524, timestamp: 2022-08-20 11:49:57.683896\n",
      "resetting env. episode 1771, reward total was -14.0. running mean: -10.8672973044042, timestamp: 2022-08-20 11:50:05.251666\n",
      "resetting env. episode 1772, reward total was -14.0. running mean: -10.898624331360159, timestamp: 2022-08-20 11:50:13.711054\n",
      "resetting env. episode 1773, reward total was -12.0. running mean: -10.909638088046556, timestamp: 2022-08-20 11:50:20.900837\n",
      "resetting env. episode 1774, reward total was -14.0. running mean: -10.94054170716609, timestamp: 2022-08-20 11:50:28.972262\n",
      "resetting env. episode 1775, reward total was -10.0. running mean: -10.931136290094429, timestamp: 2022-08-20 11:50:37.574269\n",
      "resetting env. episode 1776, reward total was -13.0. running mean: -10.951824927193485, timestamp: 2022-08-20 11:50:44.992440\n",
      "resetting env. episode 1777, reward total was -5.0. running mean: -10.892306677921551, timestamp: 2022-08-20 11:50:53.894644\n",
      "resetting env. episode 1778, reward total was -14.0. running mean: -10.923383611142336, timestamp: 2022-08-20 11:51:01.774585\n",
      "resetting env. episode 1779, reward total was -10.0. running mean: -10.914149775030912, timestamp: 2022-08-20 11:51:10.591043\n",
      "resetting env. episode 1780, reward total was -12.0. running mean: -10.925008277280602, timestamp: 2022-08-20 11:51:18.623544\n",
      "resetting env. episode 1781, reward total was -7.0. running mean: -10.885758194507796, timestamp: 2022-08-20 11:51:28.042367\n",
      "resetting env. episode 1782, reward total was -7.0. running mean: -10.846900612562719, timestamp: 2022-08-20 11:51:37.863115\n",
      "resetting env. episode 1783, reward total was -11.0. running mean: -10.84843160643709, timestamp: 2022-08-20 11:51:45.652296\n",
      "resetting env. episode 1784, reward total was -3.0. running mean: -10.769947290372718, timestamp: 2022-08-20 11:51:58.539752\n",
      "resetting env. episode 1785, reward total was -11.0. running mean: -10.77224781746899, timestamp: 2022-08-20 11:52:09.282038\n",
      "resetting env. episode 1786, reward total was -11.0. running mean: -10.7745253392943, timestamp: 2022-08-20 11:52:21.676908\n",
      "resetting env. episode 1787, reward total was -18.0. running mean: -10.846780085901356, timestamp: 2022-08-20 11:52:28.804855\n",
      "resetting env. episode 1788, reward total was -6.0. running mean: -10.798312285042343, timestamp: 2022-08-20 11:52:42.863281\n",
      "resetting env. episode 1789, reward total was -7.0. running mean: -10.760329162191919, timestamp: 2022-08-20 11:52:57.710618\n",
      "resetting env. episode 1790, reward total was -9.0. running mean: -10.74272587057, timestamp: 2022-08-20 11:53:13.143337\n",
      "resetting env. episode 1791, reward total was -21.0. running mean: -10.845298611864301, timestamp: 2022-08-20 11:53:21.158912\n",
      "resetting env. episode 1792, reward total was -7.0. running mean: -10.806845625745659, timestamp: 2022-08-20 11:53:36.850967\n",
      "resetting env. episode 1793, reward total was -15.0. running mean: -10.848777169488203, timestamp: 2022-08-20 11:53:48.801023\n",
      "resetting env. episode 1794, reward total was -13.0. running mean: -10.870289397793321, timestamp: 2022-08-20 11:54:01.016372\n",
      "resetting env. episode 1795, reward total was -14.0. running mean: -10.90158650381539, timestamp: 2022-08-20 11:54:15.304771\n",
      "resetting env. episode 1796, reward total was -9.0. running mean: -10.882570638777235, timestamp: 2022-08-20 11:54:28.217072\n",
      "resetting env. episode 1797, reward total was -18.0. running mean: -10.953744932389462, timestamp: 2022-08-20 11:54:35.142564\n",
      "resetting env. episode 1798, reward total was -14.0. running mean: -10.984207483065568, timestamp: 2022-08-20 11:54:46.120290\n",
      "resetting env. episode 1799, reward total was -6.0. running mean: -10.934365408234912, timestamp: 2022-08-20 11:55:01.309225\n",
      "resetting env. episode 1800, reward total was -18.0. running mean: -11.005021754152562, timestamp: 2022-08-20 11:55:10.704115\n",
      "resetting env. episode 1801, reward total was -6.0. running mean: -10.954971536611037, timestamp: 2022-08-20 11:55:21.506240\n",
      "resetting env. episode 1802, reward total was -5.0. running mean: -10.895421821244927, timestamp: 2022-08-20 11:55:34.496605\n",
      "resetting env. episode 1803, reward total was -17.0. running mean: -10.956467603032477, timestamp: 2022-08-20 11:55:44.276466\n",
      "resetting env. episode 1804, reward total was -12.0. running mean: -10.96690292700215, timestamp: 2022-08-20 11:55:56.382041\n",
      "resetting env. episode 1805, reward total was -18.0. running mean: -11.03723389773213, timestamp: 2022-08-20 11:56:05.211443\n",
      "resetting env. episode 1806, reward total was -1.0. running mean: -10.936861558754808, timestamp: 2022-08-20 11:56:24.098249\n",
      "resetting env. episode 1807, reward total was -3.0. running mean: -10.857492943167259, timestamp: 2022-08-20 11:56:39.018368\n",
      "resetting env. episode 1808, reward total was -12.0. running mean: -10.868918013735586, timestamp: 2022-08-20 11:56:49.202151\n",
      "resetting env. episode 1809, reward total was -14.0. running mean: -10.90022883359823, timestamp: 2022-08-20 11:56:58.567120\n",
      "resetting env. episode 1810, reward total was -6.0. running mean: -10.851226545262248, timestamp: 2022-08-20 11:57:13.381519\n",
      "resetting env. episode 1811, reward total was -15.0. running mean: -10.892714279809626, timestamp: 2022-08-20 11:57:20.424695\n",
      "resetting env. episode 1812, reward total was -13.0. running mean: -10.91378713701153, timestamp: 2022-08-20 11:57:33.003072\n",
      "resetting env. episode 1813, reward total was -9.0. running mean: -10.894649265641414, timestamp: 2022-08-20 11:57:43.591768\n",
      "resetting env. episode 1814, reward total was -9.0. running mean: -10.875702772984999, timestamp: 2022-08-20 11:57:56.758572\n",
      "resetting env. episode 1815, reward total was -9.0. running mean: -10.85694574525515, timestamp: 2022-08-20 11:58:10.138904\n",
      "resetting env. episode 1816, reward total was -9.0. running mean: -10.838376287802598, timestamp: 2022-08-20 11:58:23.750523\n",
      "resetting env. episode 1817, reward total was -9.0. running mean: -10.819992524924572, timestamp: 2022-08-20 11:58:36.407688\n",
      "resetting env. episode 1818, reward total was -1.0. running mean: -10.721792599675327, timestamp: 2022-08-20 11:58:54.474534\n",
      "resetting env. episode 1819, reward total was -8.0. running mean: -10.694574673678574, timestamp: 2022-08-20 11:59:03.913305\n",
      "resetting env. episode 1820, reward total was -19.0. running mean: -10.777628926941787, timestamp: 2022-08-20 11:59:11.599761\n",
      "resetting env. episode 1821, reward total was -2.0. running mean: -10.689852637672368, timestamp: 2022-08-20 11:59:23.803174\n",
      "resetting env. episode 1822, reward total was -12.0. running mean: -10.702954111295643, timestamp: 2022-08-20 11:59:34.123554\n",
      "resetting env. episode 1823, reward total was -12.0. running mean: -10.715924570182686, timestamp: 2022-08-20 11:59:45.615838\n",
      "resetting env. episode 1824, reward total was -11.0. running mean: -10.71876532448086, timestamp: 2022-08-20 11:59:54.298626\n",
      "resetting env. episode 1825, reward total was -13.0. running mean: -10.741577671236051, timestamp: 2022-08-20 12:00:05.584470\n",
      "resetting env. episode 1826, reward total was -17.0. running mean: -10.804161894523691, timestamp: 2022-08-20 12:00:14.088727\n",
      "resetting env. episode 1827, reward total was -13.0. running mean: -10.826120275578456, timestamp: 2022-08-20 12:00:24.038132\n",
      "resetting env. episode 1828, reward total was -12.0. running mean: -10.83785907282267, timestamp: 2022-08-20 12:00:33.536742\n",
      "resetting env. episode 1829, reward total was -5.0. running mean: -10.779480482094444, timestamp: 2022-08-20 12:00:43.701570\n",
      "resetting env. episode 1830, reward total was -10.0. running mean: -10.7716856772735, timestamp: 2022-08-20 12:00:56.601142\n",
      "resetting env. episode 1831, reward total was -5.0. running mean: -10.713968820500765, timestamp: 2022-08-20 12:01:10.518244\n",
      "resetting env. episode 1832, reward total was -14.0. running mean: -10.746829132295758, timestamp: 2022-08-20 12:01:22.789385\n",
      "resetting env. episode 1833, reward total was -6.0. running mean: -10.699360840972801, timestamp: 2022-08-20 12:01:38.553250\n",
      "resetting env. episode 1834, reward total was -6.0. running mean: -10.652367232563073, timestamp: 2022-08-20 12:01:49.845068\n",
      "resetting env. episode 1835, reward total was -5.0. running mean: -10.595843560237443, timestamp: 2022-08-20 12:02:01.668465\n",
      "resetting env. episode 1836, reward total was -1.0. running mean: -10.499885124635068, timestamp: 2022-08-20 12:02:13.562668\n",
      "resetting env. episode 1837, reward total was -11.0. running mean: -10.504886273388717, timestamp: 2022-08-20 12:02:22.684807\n",
      "resetting env. episode 1838, reward total was -10.0. running mean: -10.499837410654829, timestamp: 2022-08-20 12:02:34.965980\n",
      "resetting env. episode 1839, reward total was -17.0. running mean: -10.56483903654828, timestamp: 2022-08-20 12:02:45.174691\n",
      "resetting env. episode 1840, reward total was -6.0. running mean: -10.519190646182798, timestamp: 2022-08-20 12:02:58.774337\n",
      "resetting env. episode 1841, reward total was -5.0. running mean: -10.46399873972097, timestamp: 2022-08-20 12:03:10.174673\n",
      "resetting env. episode 1842, reward total was -12.0. running mean: -10.47935875232376, timestamp: 2022-08-20 12:03:20.616736\n",
      "resetting env. episode 1843, reward total was -17.0. running mean: -10.544565164800522, timestamp: 2022-08-20 12:03:32.696449\n",
      "resetting env. episode 1844, reward total was -13.0. running mean: -10.569119513152518, timestamp: 2022-08-20 12:03:43.373907\n",
      "resetting env. episode 1845, reward total was -7.0. running mean: -10.533428318020993, timestamp: 2022-08-20 12:04:03.714329\n",
      "resetting env. episode 1846, reward total was -8.0. running mean: -10.508094034840783, timestamp: 2022-08-20 12:04:20.156386\n",
      "resetting env. episode 1847, reward total was -9.0. running mean: -10.493013094492374, timestamp: 2022-08-20 12:04:36.241087\n",
      "resetting env. episode 1848, reward total was -13.0. running mean: -10.518082963547451, timestamp: 2022-08-20 12:04:48.099417\n",
      "resetting env. episode 1849, reward total was 4.0. running mean: -10.372902133911978, timestamp: 2022-08-20 12:04:59.451048\n",
      "resetting env. episode 1850, reward total was -4.0. running mean: -10.309173112572857, timestamp: 2022-08-20 12:05:10.549418\n",
      "resetting env. episode 1851, reward total was -11.0. running mean: -10.316081381447129, timestamp: 2022-08-20 12:05:21.687611\n",
      "resetting env. episode 1852, reward total was -4.0. running mean: -10.252920567632657, timestamp: 2022-08-20 12:05:31.807584\n",
      "resetting env. episode 1853, reward total was -7.0. running mean: -10.220391361956331, timestamp: 2022-08-20 12:05:44.058811\n",
      "resetting env. episode 1854, reward total was -11.0. running mean: -10.228187448336767, timestamp: 2022-08-20 12:05:53.179432\n",
      "resetting env. episode 1855, reward total was -5.0. running mean: -10.1759055738534, timestamp: 2022-08-20 12:06:03.765462\n",
      "resetting env. episode 1856, reward total was -11.0. running mean: -10.184146518114867, timestamp: 2022-08-20 12:06:13.757752\n",
      "resetting env. episode 1857, reward total was -9.0. running mean: -10.172305052933718, timestamp: 2022-08-20 12:06:26.793907\n",
      "resetting env. episode 1858, reward total was -13.0. running mean: -10.200582002404381, timestamp: 2022-08-20 12:06:41.676239\n",
      "resetting env. episode 1859, reward total was -9.0. running mean: -10.188576182380338, timestamp: 2022-08-20 12:06:50.845759\n",
      "resetting env. episode 1860, reward total was -9.0. running mean: -10.176690420556534, timestamp: 2022-08-20 12:07:01.989941\n",
      "resetting env. episode 1861, reward total was -17.0. running mean: -10.244923516350969, timestamp: 2022-08-20 12:07:13.043395\n",
      "resetting env. episode 1862, reward total was -16.0. running mean: -10.30247428118746, timestamp: 2022-08-20 12:07:22.852176\n",
      "resetting env. episode 1863, reward total was -15.0. running mean: -10.349449538375584, timestamp: 2022-08-20 12:07:32.289949\n",
      "resetting env. episode 1864, reward total was -9.0. running mean: -10.335955042991827, timestamp: 2022-08-20 12:07:41.744678\n",
      "resetting env. episode 1865, reward total was -9.0. running mean: -10.32259549256191, timestamp: 2022-08-20 12:07:52.529848\n",
      "resetting env. episode 1866, reward total was -10.0. running mean: -10.31936953763629, timestamp: 2022-08-20 12:08:04.817009\n",
      "resetting env. episode 1867, reward total was -11.0. running mean: -10.326175842259925, timestamp: 2022-08-20 12:08:15.053642\n",
      "resetting env. episode 1868, reward total was -14.0. running mean: -10.362914083837326, timestamp: 2022-08-20 12:08:24.760695\n",
      "resetting env. episode 1869, reward total was -13.0. running mean: -10.389284942998954, timestamp: 2022-08-20 12:08:32.605726\n",
      "resetting env. episode 1870, reward total was -12.0. running mean: -10.405392093568963, timestamp: 2022-08-20 12:08:42.700743\n",
      "resetting env. episode 1871, reward total was -4.0. running mean: -10.341338172633273, timestamp: 2022-08-20 12:08:56.703313\n",
      "resetting env. episode 1872, reward total was -17.0. running mean: -10.407924790906941, timestamp: 2022-08-20 12:09:07.768737\n",
      "resetting env. episode 1873, reward total was 1.0. running mean: -10.293845542997872, timestamp: 2022-08-20 12:09:21.725429\n",
      "resetting env. episode 1874, reward total was -9.0. running mean: -10.280907087567893, timestamp: 2022-08-20 12:09:32.109700\n",
      "resetting env. episode 1875, reward total was -14.0. running mean: -10.318098016692215, timestamp: 2022-08-20 12:09:40.135220\n",
      "resetting env. episode 1876, reward total was -15.0. running mean: -10.364917036525293, timestamp: 2022-08-20 12:09:50.733891\n",
      "resetting env. episode 1877, reward total was -8.0. running mean: -10.34126786616004, timestamp: 2022-08-20 12:10:03.835867\n",
      "resetting env. episode 1878, reward total was -11.0. running mean: -10.347855187498439, timestamp: 2022-08-20 12:10:13.002368\n",
      "resetting env. episode 1879, reward total was -11.0. running mean: -10.354376635623455, timestamp: 2022-08-20 12:10:25.304566\n",
      "resetting env. episode 1880, reward total was -9.0. running mean: -10.340832869267219, timestamp: 2022-08-20 12:10:36.209332\n",
      "resetting env. episode 1881, reward total was -14.0. running mean: -10.377424540574546, timestamp: 2022-08-20 12:10:46.340253\n",
      "resetting env. episode 1882, reward total was -6.0. running mean: -10.333650295168802, timestamp: 2022-08-20 12:10:57.720831\n",
      "resetting env. episode 1883, reward total was -5.0. running mean: -10.280313792217115, timestamp: 2022-08-20 12:11:13.641276\n",
      "resetting env. episode 1884, reward total was -3.0. running mean: -10.207510654294943, timestamp: 2022-08-20 12:11:26.278502\n",
      "resetting env. episode 1885, reward total was -6.0. running mean: -10.165435547751994, timestamp: 2022-08-20 12:11:37.925364\n",
      "resetting env. episode 1886, reward total was -2.0. running mean: -10.083781192274474, timestamp: 2022-08-20 12:11:50.503746\n",
      "resetting env. episode 1887, reward total was -13.0. running mean: -10.11294338035173, timestamp: 2022-08-20 12:11:58.280954\n",
      "resetting env. episode 1888, reward total was -14.0. running mean: -10.151813946548213, timestamp: 2022-08-20 12:12:08.432851\n",
      "resetting env. episode 1889, reward total was -16.0. running mean: -10.210295807082732, timestamp: 2022-08-20 12:12:16.467342\n",
      "resetting env. episode 1890, reward total was -9.0. running mean: -10.198192849011905, timestamp: 2022-08-20 12:12:27.551719\n",
      "resetting env. episode 1891, reward total was -1.0. running mean: -10.106210920521786, timestamp: 2022-08-20 12:12:40.823240\n",
      "resetting env. episode 1892, reward total was -9.0. running mean: -10.095148811316568, timestamp: 2022-08-20 12:12:51.624395\n",
      "resetting env. episode 1893, reward total was -4.0. running mean: -10.0341973232034, timestamp: 2022-08-20 12:13:03.468708\n",
      "resetting env. episode 1894, reward total was -2.0. running mean: -9.953855349971366, timestamp: 2022-08-20 12:13:16.759192\n",
      "resetting env. episode 1895, reward total was -6.0. running mean: -9.914316796471653, timestamp: 2022-08-20 12:13:28.172674\n",
      "resetting env. episode 1896, reward total was -13.0. running mean: -9.945173628506936, timestamp: 2022-08-20 12:13:38.277664\n",
      "resetting env. episode 1897, reward total was -16.0. running mean: -10.005721892221867, timestamp: 2022-08-20 12:13:47.077141\n",
      "resetting env. episode 1898, reward total was -20.0. running mean: -10.105664673299648, timestamp: 2022-08-20 12:13:55.489659\n",
      "resetting env. episode 1899, reward total was -4.0. running mean: -10.044608026566651, timestamp: 2022-08-20 12:14:10.331996\n",
      "resetting env. episode 1900, reward total was -7.0. running mean: -10.014161946300986, timestamp: 2022-08-20 12:14:24.436280\n",
      "resetting env. episode 1901, reward total was 3.0. running mean: -9.884020326837977, timestamp: 2022-08-20 12:14:37.081482\n",
      "resetting env. episode 1902, reward total was -10.0. running mean: -9.885180123569597, timestamp: 2022-08-20 12:14:51.923815\n",
      "resetting env. episode 1903, reward total was -9.0. running mean: -9.876328322333901, timestamp: 2022-08-20 12:15:03.271475\n",
      "resetting env. episode 1904, reward total was -15.0. running mean: -9.927565039110561, timestamp: 2022-08-20 12:15:12.894751\n",
      "resetting env. episode 1905, reward total was -4.0. running mean: -9.868289388719456, timestamp: 2022-08-20 12:15:30.327611\n",
      "resetting env. episode 1906, reward total was -15.0. running mean: -9.919606494832262, timestamp: 2022-08-20 12:15:39.418314\n",
      "resetting env. episode 1907, reward total was -15.0. running mean: -9.97041042988394, timestamp: 2022-08-20 12:15:49.991058\n",
      "resetting env. episode 1908, reward total was 7.0. running mean: -9.8007063255851, timestamp: 2022-08-20 12:16:01.702746\n",
      "resetting env. episode 1909, reward total was -7.0. running mean: -9.77269926232925, timestamp: 2022-08-20 12:16:13.596954\n",
      "resetting env. episode 1910, reward total was -8.0. running mean: -9.754972269705958, timestamp: 2022-08-20 12:16:25.857183\n",
      "resetting env. episode 1911, reward total was -9.0. running mean: -9.747422547008897, timestamp: 2022-08-20 12:16:39.378042\n",
      "resetting env. episode 1912, reward total was -16.0. running mean: -9.809948321538808, timestamp: 2022-08-20 12:16:50.467401\n",
      "resetting env. episode 1913, reward total was -11.0. running mean: -9.821848838323419, timestamp: 2022-08-20 12:17:00.889702\n",
      "resetting env. episode 1914, reward total was -12.0. running mean: -9.843630349940184, timestamp: 2022-08-20 12:17:13.178409\n",
      "resetting env. episode 1915, reward total was -8.0. running mean: -9.825194046440783, timestamp: 2022-08-20 12:17:24.852203\n",
      "resetting env. episode 1916, reward total was -15.0. running mean: -9.876942105976376, timestamp: 2022-08-20 12:17:36.748807\n",
      "resetting env. episode 1917, reward total was -17.0. running mean: -9.948172684916612, timestamp: 2022-08-20 12:17:44.638716\n",
      "resetting env. episode 1918, reward total was -6.0. running mean: -9.908690958067446, timestamp: 2022-08-20 12:18:01.227376\n",
      "resetting env. episode 1919, reward total was -9.0. running mean: -9.899604048486772, timestamp: 2022-08-20 12:18:21.897125\n",
      "resetting env. episode 1920, reward total was -11.0. running mean: -9.910608008001903, timestamp: 2022-08-20 12:18:36.728482\n",
      "resetting env. episode 1921, reward total was -8.0. running mean: -9.891501927921883, timestamp: 2022-08-20 12:18:51.453391\n",
      "resetting env. episode 1922, reward total was -8.0. running mean: -9.872586908642665, timestamp: 2022-08-20 12:19:06.499174\n",
      "resetting env. episode 1923, reward total was -3.0. running mean: -9.803861039556239, timestamp: 2022-08-20 12:19:29.175566\n",
      "resetting env. episode 1924, reward total was -9.0. running mean: -9.795822429160676, timestamp: 2022-08-20 12:19:44.714918\n",
      "resetting env. episode 1925, reward total was -15.0. running mean: -9.847864204869069, timestamp: 2022-08-20 12:19:57.719149\n",
      "resetting env. episode 1926, reward total was -12.0. running mean: -9.869385562820376, timestamp: 2022-08-20 12:20:10.552837\n",
      "resetting env. episode 1927, reward total was -11.0. running mean: -9.880691707192172, timestamp: 2022-08-20 12:20:25.974615\n",
      "resetting env. episode 1928, reward total was -12.0. running mean: -9.90188479012025, timestamp: 2022-08-20 12:20:45.359903\n",
      "resetting env. episode 1929, reward total was -8.0. running mean: -9.882865942219047, timestamp: 2022-08-20 12:21:09.725654\n",
      "resetting env. episode 1930, reward total was -5.0. running mean: -9.834037282796857, timestamp: 2022-08-20 12:21:40.284121\n",
      "resetting env. episode 1931, reward total was -13.0. running mean: -9.86569690996889, timestamp: 2022-08-20 12:21:59.005347\n",
      "resetting env. episode 1932, reward total was -13.0. running mean: -9.8970399408692, timestamp: 2022-08-20 12:22:16.070736\n",
      "resetting env. episode 1933, reward total was -18.0. running mean: -9.978069541460508, timestamp: 2022-08-20 12:22:30.205006\n",
      "resetting env. episode 1934, reward total was -5.0. running mean: -9.928288846045904, timestamp: 2022-08-20 12:22:50.311204\n",
      "resetting env. episode 1935, reward total was -14.0. running mean: -9.969005957585445, timestamp: 2022-08-20 12:23:06.484009\n",
      "resetting env. episode 1936, reward total was -15.0. running mean: -10.01931589800959, timestamp: 2022-08-20 12:23:20.649112\n",
      "resetting env. episode 1937, reward total was -3.0. running mean: -9.949122739029495, timestamp: 2022-08-20 12:23:39.503713\n",
      "resetting env. episode 1938, reward total was -9.0. running mean: -9.939631511639199, timestamp: 2022-08-20 12:23:53.809475\n",
      "resetting env. episode 1939, reward total was -11.0. running mean: -9.950235196522806, timestamp: 2022-08-20 12:24:08.725604\n",
      "resetting env. episode 1940, reward total was -13.0. running mean: -9.98073284455758, timestamp: 2022-08-20 12:24:23.553970\n",
      "resetting env. episode 1941, reward total was -11.0. running mean: -9.990925516112002, timestamp: 2022-08-20 12:24:39.418560\n",
      "resetting env. episode 1942, reward total was -4.0. running mean: -9.93101626095088, timestamp: 2022-08-20 12:24:59.989596\n",
      "resetting env. episode 1943, reward total was -11.0. running mean: -9.94170609834137, timestamp: 2022-08-20 12:25:14.075925\n",
      "resetting env. episode 1944, reward total was -17.0. running mean: -10.012289037357956, timestamp: 2022-08-20 12:25:26.542600\n",
      "resetting env. episode 1945, reward total was -13.0. running mean: -10.042166146984377, timestamp: 2022-08-20 12:25:39.666518\n",
      "resetting env. episode 1946, reward total was -11.0. running mean: -10.051744485514533, timestamp: 2022-08-20 12:25:57.738223\n",
      "resetting env. episode 1947, reward total was -5.0. running mean: -10.001227040659389, timestamp: 2022-08-20 12:26:18.813879\n",
      "resetting env. episode 1948, reward total was -14.0. running mean: -10.041214770252795, timestamp: 2022-08-20 12:26:35.117307\n",
      "resetting env. episode 1949, reward total was -15.0. running mean: -10.090802622550267, timestamp: 2022-08-20 12:26:47.911100\n",
      "resetting env. episode 1950, reward total was -8.0. running mean: -10.069894596324765, timestamp: 2022-08-20 12:27:02.529039\n",
      "resetting env. episode 1951, reward total was -10.0. running mean: -10.069195650361516, timestamp: 2022-08-20 12:27:21.668865\n",
      "resetting env. episode 1952, reward total was -11.0. running mean: -10.0785036938579, timestamp: 2022-08-20 12:27:34.801803\n",
      "resetting env. episode 1953, reward total was -10.0. running mean: -10.077718656919322, timestamp: 2022-08-20 12:27:49.246158\n",
      "resetting env. episode 1954, reward total was -16.0. running mean: -10.136941470350129, timestamp: 2022-08-20 12:28:01.991094\n",
      "resetting env. episode 1955, reward total was -15.0. running mean: -10.185572055646627, timestamp: 2022-08-20 12:28:15.454097\n",
      "resetting env. episode 1956, reward total was -14.0. running mean: -10.223716335090161, timestamp: 2022-08-20 12:28:29.336990\n",
      "resetting env. episode 1957, reward total was -15.0. running mean: -10.27147917173926, timestamp: 2022-08-20 12:28:43.824268\n",
      "resetting env. episode 1958, reward total was -3.0. running mean: -10.198764380021867, timestamp: 2022-08-20 12:29:06.037901\n",
      "resetting env. episode 1959, reward total was -9.0. running mean: -10.186776736221647, timestamp: 2022-08-20 12:29:21.858598\n",
      "resetting env. episode 1960, reward total was -10.0. running mean: -10.18490896885943, timestamp: 2022-08-20 12:29:37.000125\n",
      "resetting env. episode 1961, reward total was -10.0. running mean: -10.183059879170836, timestamp: 2022-08-20 12:29:51.916255\n",
      "resetting env. episode 1962, reward total was -3.0. running mean: -10.111229280379126, timestamp: 2022-08-20 12:30:10.911481\n",
      "resetting env. episode 1963, reward total was -5.0. running mean: -10.060116987575336, timestamp: 2022-08-20 12:30:27.682668\n",
      "resetting env. episode 1964, reward total was -13.0. running mean: -10.089515817699583, timestamp: 2022-08-20 12:30:40.060648\n",
      "resetting env. episode 1965, reward total was -5.0. running mean: -10.038620659522588, timestamp: 2022-08-20 12:30:53.654314\n",
      "resetting env. episode 1966, reward total was -13.0. running mean: -10.068234452927362, timestamp: 2022-08-20 12:31:04.401584\n",
      "resetting env. episode 1967, reward total was -11.0. running mean: -10.077552108398088, timestamp: 2022-08-20 12:31:14.879607\n",
      "resetting env. episode 1968, reward total was -11.0. running mean: -10.086776587314105, timestamp: 2022-08-20 12:31:27.543727\n",
      "resetting env. episode 1969, reward total was -15.0. running mean: -10.135908821440964, timestamp: 2022-08-20 12:31:39.141723\n",
      "resetting env. episode 1970, reward total was -9.0. running mean: -10.124549733226553, timestamp: 2022-08-20 12:31:51.341115\n",
      "resetting env. episode 1971, reward total was -11.0. running mean: -10.133304235894286, timestamp: 2022-08-20 12:32:02.721697\n",
      "resetting env. episode 1972, reward total was -16.0. running mean: -10.191971193535343, timestamp: 2022-08-20 12:32:14.156130\n",
      "resetting env. episode 1973, reward total was -10.0. running mean: -10.190051481599989, timestamp: 2022-08-20 12:32:25.608518\n",
      "resetting env. episode 1974, reward total was -13.0. running mean: -10.21815096678399, timestamp: 2022-08-20 12:32:38.535964\n",
      "resetting env. episode 1975, reward total was -11.0. running mean: -10.225969457116149, timestamp: 2022-08-20 12:32:52.147579\n",
      "resetting env. episode 1976, reward total was -10.0. running mean: -10.223709762544987, timestamp: 2022-08-20 12:33:05.000226\n",
      "resetting env. episode 1977, reward total was -9.0. running mean: -10.211472664919537, timestamp: 2022-08-20 12:33:18.012441\n",
      "resetting env. episode 1978, reward total was -8.0. running mean: -10.18935793827034, timestamp: 2022-08-20 12:33:31.507369\n",
      "resetting env. episode 1979, reward total was -6.0. running mean: -10.147464358887637, timestamp: 2022-08-20 12:33:46.817445\n",
      "resetting env. episode 1980, reward total was -11.0. running mean: -10.15598971529876, timestamp: 2022-08-20 12:34:00.428066\n",
      "resetting env. episode 1981, reward total was -11.0. running mean: -10.164429818145772, timestamp: 2022-08-20 12:34:13.560962\n",
      "resetting env. episode 1982, reward total was -13.0. running mean: -10.192785519964314, timestamp: 2022-08-20 12:34:26.127374\n",
      "resetting env. episode 1983, reward total was -5.0. running mean: -10.140857664764672, timestamp: 2022-08-20 12:34:40.996626\n",
      "resetting env. episode 1984, reward total was -11.0. running mean: -10.149449088117025, timestamp: 2022-08-20 12:34:53.485243\n",
      "resetting env. episode 1985, reward total was -10.0. running mean: -10.147954597235854, timestamp: 2022-08-20 12:35:06.518406\n",
      "resetting env. episode 1986, reward total was -14.0. running mean: -10.186475051263496, timestamp: 2022-08-20 12:35:15.842486\n",
      "resetting env. episode 1987, reward total was -19.0. running mean: -10.274610300750862, timestamp: 2022-08-20 12:35:26.140981\n",
      "resetting env. episode 1988, reward total was -10.0. running mean: -10.271864197743353, timestamp: 2022-08-20 12:35:40.953314\n",
      "resetting env. episode 1989, reward total was -7.0. running mean: -10.23914555576592, timestamp: 2022-08-20 12:35:59.245415\n",
      "resetting env. episode 1990, reward total was -11.0. running mean: -10.246754100208259, timestamp: 2022-08-20 12:36:12.049198\n",
      "resetting env. episode 1991, reward total was -7.0. running mean: -10.214286559206176, timestamp: 2022-08-20 12:36:24.183758\n",
      "resetting env. episode 1992, reward total was -12.0. running mean: -10.232143693614113, timestamp: 2022-08-20 12:36:35.389802\n",
      "resetting env. episode 1993, reward total was -14.0. running mean: -10.269822256677973, timestamp: 2022-08-20 12:36:47.443590\n",
      "resetting env. episode 1994, reward total was -16.0. running mean: -10.327124034111193, timestamp: 2022-08-20 12:36:58.769310\n",
      "resetting env. episode 1995, reward total was -15.0. running mean: -10.373852793770082, timestamp: 2022-08-20 12:37:12.837705\n",
      "resetting env. episode 1996, reward total was -15.0. running mean: -10.420114265832382, timestamp: 2022-08-20 12:37:21.848617\n",
      "resetting env. episode 1997, reward total was -6.0. running mean: -10.375913123174058, timestamp: 2022-08-20 12:37:34.232514\n",
      "resetting env. episode 1998, reward total was -10.0. running mean: -10.372153991942318, timestamp: 2022-08-20 12:37:48.254036\n",
      "resetting env. episode 1999, reward total was -2.0. running mean: -10.288432452022894, timestamp: 2022-08-20 12:38:03.088416\n",
      "resetting env. episode 2000, reward total was -13.0. running mean: -10.315548127502666, timestamp: 2022-08-20 12:38:16.644177\n",
      "resetting env. episode 2001, reward total was -3.0. running mean: -10.24239264622764, timestamp: 2022-08-20 12:38:32.654385\n",
      "resetting env. episode 2002, reward total was -12.0. running mean: -10.259968719765363, timestamp: 2022-08-20 12:38:43.431547\n",
      "resetting env. episode 2003, reward total was -19.0. running mean: -10.347369032567709, timestamp: 2022-08-20 12:38:53.992316\n",
      "resetting env. episode 2004, reward total was -9.0. running mean: -10.333895342242032, timestamp: 2022-08-20 12:39:10.052391\n",
      "resetting env. episode 2005, reward total was -12.0. running mean: -10.35055638881961, timestamp: 2022-08-20 12:39:22.835222\n",
      "resetting env. episode 2006, reward total was -10.0. running mean: -10.347050824931413, timestamp: 2022-08-20 12:39:36.905612\n",
      "resetting env. episode 2007, reward total was -14.0. running mean: -10.3835803166821, timestamp: 2022-08-20 12:39:56.175395\n",
      "resetting env. episode 2008, reward total was -9.0. running mean: -10.369744513515279, timestamp: 2022-08-20 12:40:09.525708\n",
      "resetting env. episode 2009, reward total was -2.0. running mean: -10.286047068380126, timestamp: 2022-08-20 12:40:29.859356\n",
      "resetting env. episode 2010, reward total was -12.0. running mean: -10.303186597696325, timestamp: 2022-08-20 12:40:43.876363\n",
      "resetting env. episode 2011, reward total was -12.0. running mean: -10.320154731719361, timestamp: 2022-08-20 12:40:58.544157\n",
      "resetting env. episode 2012, reward total was -17.0. running mean: -10.386953184402167, timestamp: 2022-08-20 12:41:11.735896\n",
      "resetting env. episode 2013, reward total was -3.0. running mean: -10.313083652558143, timestamp: 2022-08-20 12:41:31.012395\n",
      "resetting env. episode 2014, reward total was -11.0. running mean: -10.319952816032561, timestamp: 2022-08-20 12:41:48.204417\n",
      "resetting env. episode 2015, reward total was -17.0. running mean: -10.386753287872235, timestamp: 2022-08-20 12:42:02.537115\n",
      "resetting env. episode 2016, reward total was -10.0. running mean: -10.382885754993513, timestamp: 2022-08-20 12:42:24.644024\n",
      "resetting env. episode 2017, reward total was -13.0. running mean: -10.409056897443579, timestamp: 2022-08-20 12:42:39.043527\n",
      "resetting env. episode 2018, reward total was -8.0. running mean: -10.384966328469142, timestamp: 2022-08-20 12:42:59.878828\n",
      "resetting env. episode 2019, reward total was -14.0. running mean: -10.421116665184451, timestamp: 2022-08-20 12:43:13.376748\n",
      "resetting env. episode 2020, reward total was -12.0. running mean: -10.436905498532605, timestamp: 2022-08-20 12:43:30.633626\n",
      "resetting env. episode 2021, reward total was -18.0. running mean: -10.512536443547278, timestamp: 2022-08-20 12:43:43.209008\n",
      "resetting env. episode 2022, reward total was -6.0. running mean: -10.467411079111805, timestamp: 2022-08-20 12:44:04.339526\n",
      "resetting env. episode 2023, reward total was -13.0. running mean: -10.492736968320688, timestamp: 2022-08-20 12:44:18.566500\n",
      "resetting env. episode 2024, reward total was -3.0. running mean: -10.417809598637481, timestamp: 2022-08-20 12:44:39.844622\n",
      "resetting env. episode 2025, reward total was -3.0. running mean: -10.343631502651105, timestamp: 2022-08-20 12:44:58.643389\n",
      "resetting env. episode 2026, reward total was -16.0. running mean: -10.400195187624593, timestamp: 2022-08-20 12:45:12.133319\n",
      "resetting env. episode 2027, reward total was -16.0. running mean: -10.456193235748348, timestamp: 2022-08-20 12:45:25.677112\n",
      "resetting env. episode 2028, reward total was -15.0. running mean: -10.501631303390864, timestamp: 2022-08-20 12:45:41.526745\n",
      "resetting env. episode 2029, reward total was -6.0. running mean: -10.456614990356956, timestamp: 2022-08-20 12:45:59.107777\n",
      "resetting env. episode 2030, reward total was -1.0. running mean: -10.362048840453387, timestamp: 2022-08-20 12:46:20.467658\n",
      "resetting env. episode 2031, reward total was -10.0. running mean: -10.358428352048852, timestamp: 2022-08-20 12:46:37.391423\n",
      "resetting env. episode 2032, reward total was -15.0. running mean: -10.404844068528364, timestamp: 2022-08-20 12:46:54.209501\n",
      "resetting env. episode 2033, reward total was -15.0. running mean: -10.45079562784308, timestamp: 2022-08-20 12:47:06.069772\n",
      "resetting env. episode 2034, reward total was -8.0. running mean: -10.42628767156465, timestamp: 2022-08-20 12:47:27.988177\n",
      "resetting env. episode 2035, reward total was -5.0. running mean: -10.372024794849004, timestamp: 2022-08-20 12:47:46.611401\n",
      "resetting env. episode 2036, reward total was -3.0. running mean: -10.298304546900514, timestamp: 2022-08-20 12:48:12.170078\n",
      "resetting env. episode 2037, reward total was -7.0. running mean: -10.26532150143151, timestamp: 2022-08-20 12:48:32.081858\n",
      "resetting env. episode 2038, reward total was -8.0. running mean: -10.242668286417194, timestamp: 2022-08-20 12:48:54.090025\n",
      "resetting env. episode 2039, reward total was -9.0. running mean: -10.230241603553022, timestamp: 2022-08-20 12:49:12.167705\n",
      "resetting env. episode 2040, reward total was -3.0. running mean: -10.157939187517492, timestamp: 2022-08-20 12:49:36.760994\n",
      "resetting env. episode 2041, reward total was -9.0. running mean: -10.146359795642317, timestamp: 2022-08-20 12:49:55.206665\n",
      "resetting env. episode 2042, reward total was -15.0. running mean: -10.194896197685894, timestamp: 2022-08-20 12:50:08.948032\n",
      "resetting env. episode 2043, reward total was 1.0. running mean: -10.082947235709035, timestamp: 2022-08-20 12:50:32.778334\n",
      "resetting env. episode 2044, reward total was -13.0. running mean: -10.112117763351945, timestamp: 2022-08-20 12:50:47.953775\n",
      "resetting env. episode 2045, reward total was -11.0. running mean: -10.120996585718425, timestamp: 2022-08-20 12:51:07.579322\n",
      "resetting env. episode 2046, reward total was -11.0. running mean: -10.129786619861239, timestamp: 2022-08-20 12:51:26.088611\n",
      "resetting env. episode 2047, reward total was -8.0. running mean: -10.108488753662627, timestamp: 2022-08-20 12:51:42.912161\n",
      "resetting env. episode 2048, reward total was -9.0. running mean: -10.097403866126001, timestamp: 2022-08-20 12:51:58.491580\n",
      "resetting env. episode 2049, reward total was -5.0. running mean: -10.046429827464742, timestamp: 2022-08-20 12:52:16.051643\n",
      "resetting env. episode 2050, reward total was -15.0. running mean: -10.095965529190094, timestamp: 2022-08-20 12:52:32.896615\n",
      "resetting env. episode 2051, reward total was -15.0. running mean: -10.145005873898194, timestamp: 2022-08-20 12:52:46.423464\n",
      "resetting env. episode 2052, reward total was -9.0. running mean: -10.133555815159212, timestamp: 2022-08-20 12:53:04.497755\n",
      "resetting env. episode 2053, reward total was -6.0. running mean: -10.092220257007622, timestamp: 2022-08-20 12:53:25.403900\n",
      "resetting env. episode 2054, reward total was -15.0. running mean: -10.141298054437545, timestamp: 2022-08-20 12:53:41.234548\n",
      "resetting env. episode 2055, reward total was -4.0. running mean: -10.079885073893168, timestamp: 2022-08-20 12:54:00.152980\n",
      "resetting env. episode 2056, reward total was -9.0. running mean: -10.069086223154237, timestamp: 2022-08-20 12:54:17.790835\n",
      "resetting env. episode 2057, reward total was -7.0. running mean: -10.038395360922694, timestamp: 2022-08-20 12:54:37.168040\n",
      "resetting env. episode 2058, reward total was -12.0. running mean: -10.058011407313465, timestamp: 2022-08-20 12:54:56.014665\n",
      "resetting env. episode 2059, reward total was -12.0. running mean: -10.07743129324033, timestamp: 2022-08-20 12:55:12.911055\n",
      "resetting env. episode 2060, reward total was -15.0. running mean: -10.126656980307928, timestamp: 2022-08-20 12:55:29.905763\n",
      "resetting env. episode 2061, reward total was -6.0. running mean: -10.085390410504848, timestamp: 2022-08-20 12:55:47.023714\n",
      "resetting env. episode 2062, reward total was -10.0. running mean: -10.084536506399798, timestamp: 2022-08-20 12:56:07.026243\n",
      "resetting env. episode 2063, reward total was -6.0. running mean: -10.0436911413358, timestamp: 2022-08-20 12:56:29.356559\n",
      "resetting env. episode 2064, reward total was -10.0. running mean: -10.043254229922443, timestamp: 2022-08-20 12:56:50.500046\n",
      "resetting env. episode 2065, reward total was -15.0. running mean: -10.09282168762322, timestamp: 2022-08-20 12:57:05.299509\n",
      "resetting env. episode 2066, reward total was -11.0. running mean: -10.101893470746987, timestamp: 2022-08-20 12:57:22.135478\n",
      "resetting env. episode 2067, reward total was -8.0. running mean: -10.080874536039516, timestamp: 2022-08-20 12:57:42.339473\n",
      "resetting env. episode 2068, reward total was -15.0. running mean: -10.130065790679122, timestamp: 2022-08-20 12:57:58.777531\n",
      "resetting env. episode 2069, reward total was -11.0. running mean: -10.138765132772331, timestamp: 2022-08-20 12:58:16.881155\n",
      "resetting env. episode 2070, reward total was -9.0. running mean: -10.127377481444608, timestamp: 2022-08-20 12:58:38.217110\n",
      "resetting env. episode 2071, reward total was -13.0. running mean: -10.156103706630162, timestamp: 2022-08-20 12:58:54.169473\n",
      "resetting env. episode 2072, reward total was -3.0. running mean: -10.08454266956386, timestamp: 2022-08-20 12:59:16.874788\n",
      "resetting env. episode 2073, reward total was -9.0. running mean: -10.07369724286822, timestamp: 2022-08-20 12:59:38.428167\n",
      "resetting env. episode 2074, reward total was -8.0. running mean: -10.052960270439538, timestamp: 2022-08-20 12:59:57.417413\n",
      "resetting env. episode 2075, reward total was -13.0. running mean: -10.082430667735144, timestamp: 2022-08-20 13:00:15.603795\n",
      "resetting env. episode 2076, reward total was -4.0. running mean: -10.021606361057792, timestamp: 2022-08-20 13:00:35.836712\n",
      "resetting env. episode 2077, reward total was -10.0. running mean: -10.021390297447214, timestamp: 2022-08-20 13:00:52.798375\n",
      "resetting env. episode 2078, reward total was -1.0. running mean: -9.931176394472741, timestamp: 2022-08-20 13:01:24.028895\n",
      "resetting env. episode 2079, reward total was -7.0. running mean: -9.901864630528014, timestamp: 2022-08-20 13:01:48.041715\n",
      "resetting env. episode 2080, reward total was -15.0. running mean: -9.952845984222733, timestamp: 2022-08-20 13:02:05.760346\n",
      "resetting env. episode 2081, reward total was -14.0. running mean: -9.993317524380506, timestamp: 2022-08-20 13:02:20.065117\n",
      "resetting env. episode 2082, reward total was -2.0. running mean: -9.9133843491367, timestamp: 2022-08-20 13:02:51.007521\n",
      "resetting env. episode 2083, reward total was 4.0. running mean: -9.774250505645334, timestamp: 2022-08-20 13:03:17.389019\n",
      "resetting env. episode 2084, reward total was -13.0. running mean: -9.806508000588881, timestamp: 2022-08-20 13:03:35.737972\n",
      "resetting env. episode 2085, reward total was -6.0. running mean: -9.768442920582993, timestamp: 2022-08-20 13:04:02.217185\n",
      "resetting env. episode 2086, reward total was -10.0. running mean: -9.770758491377164, timestamp: 2022-08-20 13:04:23.562179\n",
      "resetting env. episode 2087, reward total was -2.0. running mean: -9.693050906463391, timestamp: 2022-08-20 13:04:43.797068\n",
      "resetting env. episode 2088, reward total was -17.0. running mean: -9.766120397398756, timestamp: 2022-08-20 13:05:00.944233\n",
      "resetting env. episode 2089, reward total was -15.0. running mean: -9.818459193424768, timestamp: 2022-08-20 13:05:18.694786\n",
      "resetting env. episode 2090, reward total was -5.0. running mean: -9.770274601490522, timestamp: 2022-08-20 13:05:42.029411\n",
      "resetting env. episode 2091, reward total was -13.0. running mean: -9.802571855475618, timestamp: 2022-08-20 13:06:00.388339\n",
      "resetting env. episode 2092, reward total was -8.0. running mean: -9.784546136920861, timestamp: 2022-08-20 13:06:21.751247\n",
      "resetting env. episode 2093, reward total was -2.0. running mean: -9.706700675551652, timestamp: 2022-08-20 13:06:45.059939\n",
      "resetting env. episode 2094, reward total was -12.0. running mean: -9.729633668796135, timestamp: 2022-08-20 13:07:01.884958\n",
      "resetting env. episode 2095, reward total was -13.0. running mean: -9.762337332108174, timestamp: 2022-08-20 13:07:22.905770\n",
      "resetting env. episode 2096, reward total was -12.0. running mean: -9.784713958787092, timestamp: 2022-08-20 13:07:39.316903\n",
      "resetting env. episode 2097, reward total was -10.0. running mean: -9.78686681919922, timestamp: 2022-08-20 13:07:57.037060\n",
      "resetting env. episode 2098, reward total was -15.0. running mean: -9.838998151007228, timestamp: 2022-08-20 13:08:14.191223\n",
      "resetting env. episode 2099, reward total was -6.0. running mean: -9.800608169497156, timestamp: 2022-08-20 13:08:35.520205\n",
      "resetting env. episode 2100, reward total was -3.0. running mean: -9.732602087802183, timestamp: 2022-08-20 13:09:01.775026\n",
      "resetting env. episode 2101, reward total was -11.0. running mean: -9.745276066924161, timestamp: 2022-08-20 13:09:23.905859\n",
      "resetting env. episode 2102, reward total was -9.0. running mean: -9.737823306254919, timestamp: 2022-08-20 13:09:42.200957\n",
      "resetting env. episode 2103, reward total was -10.0. running mean: -9.740445073192369, timestamp: 2022-08-20 13:10:06.622698\n",
      "resetting env. episode 2104, reward total was -9.0. running mean: -9.733040622460445, timestamp: 2022-08-20 13:10:24.517971\n",
      "resetting env. episode 2105, reward total was -11.0. running mean: -9.74571021623584, timestamp: 2022-08-20 13:10:44.067708\n",
      "resetting env. episode 2106, reward total was -15.0. running mean: -9.798253114073482, timestamp: 2022-08-20 13:11:01.099182\n",
      "resetting env. episode 2107, reward total was -11.0. running mean: -9.810270582932747, timestamp: 2022-08-20 13:11:19.321478\n",
      "resetting env. episode 2108, reward total was -2.0. running mean: -9.732167877103418, timestamp: 2022-08-20 13:11:38.154136\n",
      "resetting env. episode 2109, reward total was -7.0. running mean: -9.704846198332383, timestamp: 2022-08-20 13:11:57.469539\n",
      "resetting env. episode 2110, reward total was -4.0. running mean: -9.647797736349059, timestamp: 2022-08-20 13:12:19.240311\n",
      "resetting env. episode 2111, reward total was -5.0. running mean: -9.60131975898557, timestamp: 2022-08-20 13:12:42.703611\n",
      "resetting env. episode 2112, reward total was -6.0. running mean: -9.565306561395714, timestamp: 2022-08-20 13:13:05.549527\n",
      "resetting env. episode 2113, reward total was 10.0. running mean: -9.369653495781757, timestamp: 2022-08-20 13:13:20.343983\n",
      "resetting env. episode 2114, reward total was -10.0. running mean: -9.375956960823938, timestamp: 2022-08-20 13:13:37.441287\n",
      "resetting env. episode 2115, reward total was -9.0. running mean: -9.372197391215698, timestamp: 2022-08-20 13:14:00.182494\n",
      "resetting env. episode 2116, reward total was -9.0. running mean: -9.368475417303541, timestamp: 2022-08-20 13:14:24.293044\n",
      "resetting env. episode 2117, reward total was -9.0. running mean: -9.364790663130506, timestamp: 2022-08-20 13:14:44.748375\n",
      "resetting env. episode 2118, reward total was -13.0. running mean: -9.401142756499201, timestamp: 2022-08-20 13:15:03.477327\n",
      "resetting env. episode 2119, reward total was -9.0. running mean: -9.39713132893421, timestamp: 2022-08-20 13:15:27.015389\n",
      "resetting env. episode 2120, reward total was -4.0. running mean: -9.343160015644866, timestamp: 2022-08-20 13:15:56.607288\n",
      "resetting env. episode 2121, reward total was -6.0. running mean: -9.309728415488419, timestamp: 2022-08-20 13:16:17.130437\n",
      "resetting env. episode 2122, reward total was -5.0. running mean: -9.266631131333535, timestamp: 2022-08-20 13:16:39.219386\n",
      "resetting env. episode 2123, reward total was -13.0. running mean: -9.3039648200202, timestamp: 2022-08-20 13:16:57.926403\n",
      "resetting env. episode 2124, reward total was -17.0. running mean: -9.380925171819998, timestamp: 2022-08-20 13:17:11.578890\n",
      "resetting env. episode 2125, reward total was -7.0. running mean: -9.357115920101798, timestamp: 2022-08-20 13:17:30.951123\n",
      "resetting env. episode 2126, reward total was -5.0. running mean: -9.31354476090078, timestamp: 2022-08-20 13:17:53.744182\n",
      "resetting env. episode 2127, reward total was -17.0. running mean: -9.390409313291773, timestamp: 2022-08-20 13:18:08.614435\n",
      "resetting env. episode 2128, reward total was -9.0. running mean: -9.386505220158854, timestamp: 2022-08-20 13:18:28.294836\n",
      "resetting env. episode 2129, reward total was -3.0. running mean: -9.322640167957266, timestamp: 2022-08-20 13:18:50.868493\n",
      "resetting env. episode 2130, reward total was -5.0. running mean: -9.279413766277694, timestamp: 2022-08-20 13:19:17.991988\n",
      "resetting env. episode 2131, reward total was -4.0. running mean: -9.226619628614916, timestamp: 2022-08-20 13:19:41.142622\n",
      "resetting env. episode 2132, reward total was -12.0. running mean: -9.254353432328767, timestamp: 2022-08-20 13:19:58.856285\n",
      "resetting env. episode 2133, reward total was -14.0. running mean: -9.30180989800548, timestamp: 2022-08-20 13:20:17.933350\n",
      "resetting env. episode 2134, reward total was -8.0. running mean: -9.288791799025425, timestamp: 2022-08-20 13:20:37.720487\n",
      "resetting env. episode 2135, reward total was -11.0. running mean: -9.30590388103517, timestamp: 2022-08-20 13:20:53.216044\n",
      "resetting env. episode 2136, reward total was -3.0. running mean: -9.242844842224818, timestamp: 2022-08-20 13:21:17.912025\n",
      "resetting env. episode 2137, reward total was -6.0. running mean: -9.21041639380257, timestamp: 2022-08-20 13:21:38.724396\n",
      "resetting env. episode 2138, reward total was -1.0. running mean: -9.128312229864544, timestamp: 2022-08-20 13:22:00.332653\n",
      "resetting env. episode 2139, reward total was -13.0. running mean: -9.1670291075659, timestamp: 2022-08-20 13:22:19.929254\n",
      "resetting env. episode 2140, reward total was -7.0. running mean: -9.145358816490242, timestamp: 2022-08-20 13:22:39.756261\n",
      "resetting env. episode 2141, reward total was -11.0. running mean: -9.163905228325339, timestamp: 2022-08-20 13:23:02.667041\n",
      "resetting env. episode 2142, reward total was -13.0. running mean: -9.202266176042086, timestamp: 2022-08-20 13:23:23.448486\n",
      "resetting env. episode 2143, reward total was -15.0. running mean: -9.260243514281665, timestamp: 2022-08-20 13:23:39.841703\n",
      "resetting env. episode 2144, reward total was -11.0. running mean: -9.277641079138848, timestamp: 2022-08-20 13:23:56.536075\n",
      "resetting env. episode 2145, reward total was -9.0. running mean: -9.274864668347458, timestamp: 2022-08-20 13:24:14.054217\n",
      "resetting env. episode 2146, reward total was -11.0. running mean: -9.292116021663983, timestamp: 2022-08-20 13:24:32.914806\n",
      "resetting env. episode 2147, reward total was -5.0. running mean: -9.249194861447343, timestamp: 2022-08-20 13:24:58.564436\n",
      "resetting env. episode 2148, reward total was -19.0. running mean: -9.34670291283287, timestamp: 2022-08-20 13:25:13.197307\n",
      "resetting env. episode 2149, reward total was -14.0. running mean: -9.393235883704541, timestamp: 2022-08-20 13:25:30.534968\n",
      "resetting env. episode 2150, reward total was -3.0. running mean: -9.329303524867495, timestamp: 2022-08-20 13:25:52.694807\n",
      "resetting env. episode 2151, reward total was -8.0. running mean: -9.31601048961882, timestamp: 2022-08-20 13:26:12.879854\n",
      "resetting env. episode 2152, reward total was -6.0. running mean: -9.282850384722632, timestamp: 2022-08-20 13:26:34.213834\n",
      "resetting env. episode 2153, reward total was -12.0. running mean: -9.310021880875405, timestamp: 2022-08-20 13:26:55.279532\n",
      "resetting env. episode 2154, reward total was -12.0. running mean: -9.33692166206665, timestamp: 2022-08-20 13:27:19.687277\n",
      "resetting env. episode 2155, reward total was -7.0. running mean: -9.313552445445984, timestamp: 2022-08-20 13:27:43.840724\n",
      "resetting env. episode 2156, reward total was -2.0. running mean: -9.240416920991523, timestamp: 2022-08-20 13:28:07.036067\n",
      "resetting env. episode 2157, reward total was -16.0. running mean: -9.308012751781607, timestamp: 2022-08-20 13:28:22.600470\n",
      "resetting env. episode 2158, reward total was -15.0. running mean: -9.364932624263792, timestamp: 2022-08-20 13:28:41.887907\n",
      "resetting env. episode 2159, reward total was -16.0. running mean: -9.431283298021155, timestamp: 2022-08-20 13:28:59.815003\n",
      "resetting env. episode 2160, reward total was -17.0. running mean: -9.506970465040943, timestamp: 2022-08-20 13:29:14.745080\n",
      "resetting env. episode 2161, reward total was 10.0. running mean: -9.311900760390534, timestamp: 2022-08-20 13:29:33.322434\n",
      "resetting env. episode 2162, reward total was -8.0. running mean: -9.29878175278663, timestamp: 2022-08-20 13:29:53.236193\n",
      "resetting env. episode 2163, reward total was -4.0. running mean: -9.245793935258762, timestamp: 2022-08-20 13:30:11.585149\n",
      "resetting env. episode 2164, reward total was -11.0. running mean: -9.263335995906173, timestamp: 2022-08-20 13:30:31.226645\n",
      "resetting env. episode 2165, reward total was -5.0. running mean: -9.220702635947113, timestamp: 2022-08-20 13:30:52.506781\n",
      "resetting env. episode 2166, reward total was -9.0. running mean: -9.218495609587642, timestamp: 2022-08-20 13:31:12.874899\n",
      "resetting env. episode 2167, reward total was -3.0. running mean: -9.156310653491765, timestamp: 2022-08-20 13:31:36.834860\n",
      "resetting env. episode 2168, reward total was -12.0. running mean: -9.184747546956846, timestamp: 2022-08-20 13:31:54.590396\n",
      "resetting env. episode 2169, reward total was -5.0. running mean: -9.142900071487277, timestamp: 2022-08-20 13:32:19.265446\n",
      "resetting env. episode 2170, reward total was -9.0. running mean: -9.141471070772404, timestamp: 2022-08-20 13:32:42.394611\n",
      "resetting env. episode 2171, reward total was -11.0. running mean: -9.16005636006468, timestamp: 2022-08-20 13:33:02.693365\n",
      "resetting env. episode 2172, reward total was -5.0. running mean: -9.118455796464033, timestamp: 2022-08-20 13:33:26.996390\n",
      "resetting env. episode 2173, reward total was -12.0. running mean: -9.147271238499393, timestamp: 2022-08-20 13:33:42.269577\n",
      "resetting env. episode 2174, reward total was -8.0. running mean: -9.1357985261144, timestamp: 2022-08-20 13:34:03.158728\n",
      "resetting env. episode 2175, reward total was -13.0. running mean: -9.174440540853256, timestamp: 2022-08-20 13:34:20.560748\n",
      "resetting env. episode 2176, reward total was -13.0. running mean: -9.212696135444725, timestamp: 2022-08-20 13:34:40.111520\n",
      "resetting env. episode 2177, reward total was -14.0. running mean: -9.260569174090278, timestamp: 2022-08-20 13:34:57.852100\n",
      "resetting env. episode 2178, reward total was -9.0. running mean: -9.257963482349375, timestamp: 2022-08-20 13:35:22.387516\n",
      "resetting env. episode 2179, reward total was -7.0. running mean: -9.235383847525881, timestamp: 2022-08-20 13:35:42.683280\n",
      "resetting env. episode 2180, reward total was -17.0. running mean: -9.313030009050623, timestamp: 2022-08-20 13:36:00.692137\n",
      "resetting env. episode 2181, reward total was -13.0. running mean: -9.349899708960118, timestamp: 2022-08-20 13:36:20.870210\n",
      "resetting env. episode 2182, reward total was -5.0. running mean: -9.306400711870518, timestamp: 2022-08-20 13:36:45.166247\n",
      "resetting env. episode 2183, reward total was -13.0. running mean: -9.343336704751813, timestamp: 2022-08-20 13:37:02.040144\n",
      "resetting env. episode 2184, reward total was -7.0. running mean: -9.319903337704295, timestamp: 2022-08-20 13:37:24.936940\n",
      "resetting env. episode 2185, reward total was -11.0. running mean: -9.33670430432725, timestamp: 2022-08-20 13:37:42.043222\n",
      "resetting env. episode 2186, reward total was -21.0. running mean: -9.453337261283979, timestamp: 2022-08-20 13:37:54.474002\n",
      "resetting env. episode 2187, reward total was -3.0. running mean: -9.388803888671138, timestamp: 2022-08-20 13:38:19.037330\n",
      "resetting env. episode 2188, reward total was -11.0. running mean: -9.404915849784425, timestamp: 2022-08-20 13:38:35.420539\n",
      "resetting env. episode 2189, reward total was -17.0. running mean: -9.48086669128658, timestamp: 2022-08-20 13:38:51.597303\n",
      "resetting env. episode 2190, reward total was 3.0. running mean: -9.356058024373715, timestamp: 2022-08-20 13:39:13.616004\n",
      "resetting env. episode 2191, reward total was -5.0. running mean: -9.312497444129978, timestamp: 2022-08-20 13:39:36.028104\n",
      "resetting env. episode 2192, reward total was 1.0. running mean: -9.209372469688677, timestamp: 2022-08-20 13:40:08.287501\n",
      "resetting env. episode 2193, reward total was -13.0. running mean: -9.24727874499179, timestamp: 2022-08-20 13:40:26.455940\n",
      "resetting env. episode 2194, reward total was -10.0. running mean: -9.254805957541873, timestamp: 2022-08-20 13:40:47.938514\n",
      "resetting env. episode 2195, reward total was -7.0. running mean: -9.232257897966454, timestamp: 2022-08-20 13:41:12.373100\n",
      "resetting env. episode 2196, reward total was -2.0. running mean: -9.159935318986788, timestamp: 2022-08-20 13:41:36.900536\n",
      "resetting env. episode 2197, reward total was -5.0. running mean: -9.118335965796922, timestamp: 2022-08-20 13:42:01.291419\n",
      "resetting env. episode 2198, reward total was -12.0. running mean: -9.14715260613895, timestamp: 2022-08-20 13:42:16.307286\n",
      "resetting env. episode 2199, reward total was -12.0. running mean: -9.17568108007756, timestamp: 2022-08-20 13:42:31.125673\n",
      "resetting env. episode 2200, reward total was -15.0. running mean: -9.233924269276784, timestamp: 2022-08-20 13:42:52.392825\n",
      "resetting env. episode 2201, reward total was -10.0. running mean: -9.241585026584016, timestamp: 2022-08-20 13:43:10.126424\n",
      "resetting env. episode 2202, reward total was -3.0. running mean: -9.179169176318174, timestamp: 2022-08-20 13:43:30.587737\n",
      "resetting env. episode 2203, reward total was 8.0. running mean: -9.007377484554992, timestamp: 2022-08-20 13:43:46.038431\n",
      "resetting env. episode 2204, reward total was -3.0. running mean: -8.947303709709441, timestamp: 2022-08-20 13:44:06.987434\n",
      "resetting env. episode 2205, reward total was -15.0. running mean: -9.007830672612346, timestamp: 2022-08-20 13:44:25.493968\n",
      "resetting env. episode 2206, reward total was -17.0. running mean: -9.087752365886223, timestamp: 2022-08-20 13:44:38.154131\n",
      "resetting env. episode 2207, reward total was -12.0. running mean: -9.116874842227359, timestamp: 2022-08-20 13:44:53.830223\n",
      "resetting env. episode 2208, reward total was -1.0. running mean: -9.035706093805086, timestamp: 2022-08-20 13:45:19.245294\n",
      "resetting env. episode 2209, reward total was -14.0. running mean: -9.085349032867036, timestamp: 2022-08-20 13:45:35.668396\n",
      "resetting env. episode 2210, reward total was -6.0. running mean: -9.054495542538366, timestamp: 2022-08-20 13:45:54.040287\n",
      "resetting env. episode 2211, reward total was -6.0. running mean: -9.023950587112981, timestamp: 2022-08-20 13:46:16.310777\n",
      "resetting env. episode 2212, reward total was -5.0. running mean: -8.983711081241852, timestamp: 2022-08-20 13:46:34.783390\n",
      "resetting env. episode 2213, reward total was -8.0. running mean: -8.973873970429434, timestamp: 2022-08-20 13:46:53.190181\n",
      "resetting env. episode 2214, reward total was -15.0. running mean: -9.03413523072514, timestamp: 2022-08-20 13:47:06.827740\n",
      "resetting env. episode 2215, reward total was -1.0. running mean: -8.953793878417887, timestamp: 2022-08-20 13:47:31.999442\n",
      "resetting env. episode 2216, reward total was -11.0. running mean: -8.974255939633707, timestamp: 2022-08-20 13:47:50.304513\n",
      "resetting env. episode 2217, reward total was -1.0. running mean: -8.89451338023737, timestamp: 2022-08-20 13:48:12.970934\n",
      "resetting env. episode 2218, reward total was -11.0. running mean: -8.915568246434995, timestamp: 2022-08-20 13:48:33.761353\n",
      "resetting env. episode 2219, reward total was -16.0. running mean: -8.986412563970646, timestamp: 2022-08-20 13:48:45.249644\n",
      "resetting env. episode 2220, reward total was -1.0. running mean: -8.906548438330939, timestamp: 2022-08-20 13:49:06.890797\n",
      "resetting env. episode 2221, reward total was -7.0. running mean: -8.887482953947629, timestamp: 2022-08-20 13:49:27.990398\n",
      "resetting env. episode 2222, reward total was -11.0. running mean: -8.908608124408152, timestamp: 2022-08-20 13:49:46.577730\n",
      "resetting env. episode 2223, reward total was -12.0. running mean: -8.93952204316407, timestamp: 2022-08-20 13:50:03.981260\n",
      "resetting env. episode 2224, reward total was -1.0. running mean: -8.860126822732429, timestamp: 2022-08-20 13:50:29.571793\n",
      "resetting env. episode 2225, reward total was -6.0. running mean: -8.831525554505106, timestamp: 2022-08-20 13:50:51.284753\n",
      "resetting env. episode 2226, reward total was -16.0. running mean: -8.903210298960055, timestamp: 2022-08-20 13:51:08.641361\n",
      "resetting env. episode 2227, reward total was -3.0. running mean: -8.844178195970454, timestamp: 2022-08-20 13:51:29.181454\n",
      "resetting env. episode 2228, reward total was -6.0. running mean: -8.815736414010749, timestamp: 2022-08-20 13:51:49.752466\n",
      "resetting env. episode 2229, reward total was 7.0. running mean: -8.657579049870641, timestamp: 2022-08-20 13:52:09.127679\n",
      "resetting env. episode 2230, reward total was -13.0. running mean: -8.701003259371936, timestamp: 2022-08-20 13:52:24.206372\n",
      "resetting env. episode 2231, reward total was -8.0. running mean: -8.693993226778217, timestamp: 2022-08-20 13:52:46.200586\n",
      "resetting env. episode 2232, reward total was -9.0. running mean: -8.697053294510434, timestamp: 2022-08-20 13:53:03.221085\n",
      "resetting env. episode 2233, reward total was -9.0. running mean: -8.70008276156533, timestamp: 2022-08-20 13:53:23.041161\n",
      "resetting env. episode 2234, reward total was -3.0. running mean: -8.643081933949675, timestamp: 2022-08-20 13:53:47.626415\n",
      "resetting env. episode 2235, reward total was -6.0. running mean: -8.616651114610178, timestamp: 2022-08-20 13:54:04.764606\n",
      "resetting env. episode 2236, reward total was -19.0. running mean: -8.720484603464076, timestamp: 2022-08-20 13:54:18.873893\n",
      "resetting env. episode 2237, reward total was -12.0. running mean: -8.753279757429434, timestamp: 2022-08-20 13:54:37.579974\n",
      "resetting env. episode 2238, reward total was -10.0. running mean: -8.76574695985514, timestamp: 2022-08-20 13:54:56.915285\n",
      "resetting env. episode 2239, reward total was -15.0. running mean: -8.828089490256588, timestamp: 2022-08-20 13:55:08.085416\n",
      "resetting env. episode 2240, reward total was -4.0. running mean: -8.779808595354021, timestamp: 2022-08-20 13:55:33.274215\n",
      "resetting env. episode 2241, reward total was -9.0. running mean: -8.78201050940048, timestamp: 2022-08-20 13:55:52.933682\n",
      "resetting env. episode 2242, reward total was -3.0. running mean: -8.724190404306475, timestamp: 2022-08-20 13:56:15.358723\n",
      "resetting env. episode 2243, reward total was -11.0. running mean: -8.746948500263409, timestamp: 2022-08-20 13:56:30.269875\n",
      "resetting env. episode 2244, reward total was -3.0. running mean: -8.689479015260774, timestamp: 2022-08-20 13:56:53.415995\n",
      "resetting env. episode 2245, reward total was -15.0. running mean: -8.752584225108167, timestamp: 2022-08-20 13:57:09.025278\n",
      "resetting env. episode 2246, reward total was -12.0. running mean: -8.785058382857084, timestamp: 2022-08-20 13:57:23.728970\n",
      "resetting env. episode 2247, reward total was -12.0. running mean: -8.817207799028512, timestamp: 2022-08-20 13:57:41.028733\n",
      "resetting env. episode 2248, reward total was -6.0. running mean: -8.789035721038228, timestamp: 2022-08-20 13:58:02.065496\n",
      "resetting env. episode 2249, reward total was -11.0. running mean: -8.811145363827846, timestamp: 2022-08-20 13:58:19.186733\n",
      "resetting env. episode 2250, reward total was -5.0. running mean: -8.773033910189568, timestamp: 2022-08-20 13:58:39.084570\n",
      "resetting env. episode 2251, reward total was -3.0. running mean: -8.715303571087672, timestamp: 2022-08-20 13:59:01.485675\n",
      "resetting env. episode 2252, reward total was -5.0. running mean: -8.678150535376796, timestamp: 2022-08-20 13:59:22.342920\n",
      "resetting env. episode 2253, reward total was -8.0. running mean: -8.671369030023028, timestamp: 2022-08-20 13:59:37.942218\n",
      "resetting env. episode 2254, reward total was -5.0. running mean: -8.634655339722798, timestamp: 2022-08-20 14:00:00.272537\n",
      "resetting env. episode 2255, reward total was -15.0. running mean: -8.69830878632557, timestamp: 2022-08-20 14:00:15.680344\n",
      "resetting env. episode 2256, reward total was -3.0. running mean: -8.641325698462314, timestamp: 2022-08-20 14:00:36.838787\n",
      "resetting env. episode 2257, reward total was -9.0. running mean: -8.64491244147769, timestamp: 2022-08-20 14:00:57.684068\n",
      "resetting env. episode 2258, reward total was -5.0. running mean: -8.608463317062913, timestamp: 2022-08-20 14:01:18.609133\n",
      "resetting env. episode 2259, reward total was -11.0. running mean: -8.632378683892284, timestamp: 2022-08-20 14:01:37.730026\n",
      "resetting env. episode 2260, reward total was -15.0. running mean: -8.696054897053362, timestamp: 2022-08-20 14:01:53.663438\n",
      "resetting env. episode 2261, reward total was -12.0. running mean: -8.729094348082826, timestamp: 2022-08-20 14:02:09.231822\n",
      "resetting env. episode 2262, reward total was -14.0. running mean: -8.781803404601998, timestamp: 2022-08-20 14:02:22.816510\n",
      "resetting env. episode 2263, reward total was -7.0. running mean: -8.763985370555979, timestamp: 2022-08-20 14:02:42.256544\n",
      "resetting env. episode 2264, reward total was -11.0. running mean: -8.786345516850417, timestamp: 2022-08-20 14:02:58.976853\n",
      "resetting env. episode 2265, reward total was -3.0. running mean: -8.728482061681913, timestamp: 2022-08-20 14:03:21.394929\n",
      "resetting env. episode 2266, reward total was -11.0. running mean: -8.751197241065093, timestamp: 2022-08-20 14:03:39.318022\n",
      "resetting env. episode 2267, reward total was -13.0. running mean: -8.793685268654443, timestamp: 2022-08-20 14:04:01.376591\n",
      "resetting env. episode 2268, reward total was -3.0. running mean: -8.735748415967898, timestamp: 2022-08-20 14:04:30.185434\n",
      "resetting env. episode 2269, reward total was -17.0. running mean: -8.818390931808219, timestamp: 2022-08-20 14:04:41.601922\n",
      "resetting env. episode 2270, reward total was -5.0. running mean: -8.780207022490137, timestamp: 2022-08-20 14:05:04.023977\n",
      "resetting env. episode 2271, reward total was -7.0. running mean: -8.762404952265236, timestamp: 2022-08-20 14:05:22.664153\n",
      "resetting env. episode 2272, reward total was -2.0. running mean: -8.694780902742583, timestamp: 2022-08-20 14:05:40.963241\n",
      "resetting env. episode 2273, reward total was -3.0. running mean: -8.637833093715155, timestamp: 2022-08-20 14:06:08.090727\n",
      "resetting env. episode 2274, reward total was -1.0. running mean: -8.561454762778004, timestamp: 2022-08-20 14:06:29.245188\n",
      "resetting env. episode 2275, reward total was -3.0. running mean: -8.505840215150222, timestamp: 2022-08-20 14:06:49.622711\n",
      "resetting env. episode 2276, reward total was -13.0. running mean: -8.550781812998721, timestamp: 2022-08-20 14:07:09.020860\n",
      "resetting env. episode 2277, reward total was -13.0. running mean: -8.595273994868736, timestamp: 2022-08-20 14:07:27.264130\n",
      "resetting env. episode 2278, reward total was -11.0. running mean: -8.619321254920047, timestamp: 2022-08-20 14:07:42.852431\n",
      "resetting env. episode 2279, reward total was 3.0. running mean: -8.503128042370847, timestamp: 2022-08-20 14:08:07.062713\n",
      "resetting env. episode 2280, reward total was -14.0. running mean: -8.558096761947139, timestamp: 2022-08-20 14:08:24.612803\n",
      "resetting env. episode 2281, reward total was -13.0. running mean: -8.602515794327669, timestamp: 2022-08-20 14:08:41.920559\n",
      "resetting env. episode 2282, reward total was -12.0. running mean: -8.63649063638439, timestamp: 2022-08-20 14:08:55.857304\n",
      "resetting env. episode 2283, reward total was -13.0. running mean: -8.680125730020547, timestamp: 2022-08-20 14:09:15.933648\n",
      "resetting env. episode 2284, reward total was -2.0. running mean: -8.613324472720342, timestamp: 2022-08-20 14:09:37.956755\n",
      "resetting env. episode 2285, reward total was -12.0. running mean: -8.647191227993137, timestamp: 2022-08-20 14:09:56.502749\n",
      "resetting env. episode 2286, reward total was -16.0. running mean: -8.720719315713206, timestamp: 2022-08-20 14:10:09.995682\n",
      "resetting env. episode 2287, reward total was -5.0. running mean: -8.683512122556074, timestamp: 2022-08-20 14:10:31.237903\n",
      "resetting env. episode 2288, reward total was -7.0. running mean: -8.666677001330514, timestamp: 2022-08-20 14:10:48.994455\n",
      "resetting env. episode 2289, reward total was -1.0. running mean: -8.590010231317208, timestamp: 2022-08-20 14:11:12.898545\n",
      "resetting env. episode 2290, reward total was -9.0. running mean: -8.594110129004036, timestamp: 2022-08-20 14:11:32.891111\n",
      "resetting env. episode 2291, reward total was -13.0. running mean: -8.638169027713996, timestamp: 2022-08-20 14:11:48.335834\n",
      "resetting env. episode 2292, reward total was -7.0. running mean: -8.621787337436857, timestamp: 2022-08-20 14:12:07.812758\n",
      "resetting env. episode 2293, reward total was -6.0. running mean: -8.59556946406249, timestamp: 2022-08-20 14:12:28.376790\n",
      "resetting env. episode 2294, reward total was -2.0. running mean: -8.529613769421864, timestamp: 2022-08-20 14:12:52.395604\n",
      "resetting env. episode 2295, reward total was -9.0. running mean: -8.534317631727646, timestamp: 2022-08-20 14:13:12.052057\n",
      "resetting env. episode 2296, reward total was -7.0. running mean: -8.518974455410369, timestamp: 2022-08-20 14:13:31.356483\n",
      "resetting env. episode 2297, reward total was -15.0. running mean: -8.583784710856266, timestamp: 2022-08-20 14:13:47.124300\n",
      "resetting env. episode 2298, reward total was -12.0. running mean: -8.617946863747703, timestamp: 2022-08-20 14:14:06.936349\n",
      "resetting env. episode 2299, reward total was -15.0. running mean: -8.681767395110226, timestamp: 2022-08-20 14:14:22.119760\n",
      "resetting env. episode 2300, reward total was -10.0. running mean: -8.694949721159123, timestamp: 2022-08-20 14:14:40.701104\n",
      "resetting env. episode 2301, reward total was -8.0. running mean: -8.688000223947531, timestamp: 2022-08-20 14:14:56.962619\n",
      "resetting env. episode 2302, reward total was -10.0. running mean: -8.701120221708056, timestamp: 2022-08-20 14:15:16.407648\n",
      "resetting env. episode 2303, reward total was -14.0. running mean: -8.754109019490976, timestamp: 2022-08-20 14:15:30.308486\n",
      "resetting env. episode 2304, reward total was -6.0. running mean: -8.726567929296067, timestamp: 2022-08-20 14:15:50.587283\n",
      "resetting env. episode 2305, reward total was -7.0. running mean: -8.709302250003107, timestamp: 2022-08-20 14:16:10.057237\n",
      "resetting env. episode 2306, reward total was -8.0. running mean: -8.702209227503076, timestamp: 2022-08-20 14:16:29.111309\n",
      "resetting env. episode 2307, reward total was -12.0. running mean: -8.735187135228044, timestamp: 2022-08-20 14:16:44.476244\n",
      "resetting env. episode 2308, reward total was -13.0. running mean: -8.777835263875764, timestamp: 2022-08-20 14:17:01.974628\n",
      "resetting env. episode 2309, reward total was 1.0. running mean: -8.680056911237006, timestamp: 2022-08-20 14:17:24.230075\n",
      "resetting env. episode 2310, reward total was -13.0. running mean: -8.723256342124637, timestamp: 2022-08-20 14:17:37.578409\n",
      "resetting env. episode 2311, reward total was -8.0. running mean: -8.716023778703391, timestamp: 2022-08-20 14:17:56.144769\n",
      "resetting env. episode 2312, reward total was -14.0. running mean: -8.768863540916358, timestamp: 2022-08-20 14:18:16.252019\n",
      "resetting env. episode 2313, reward total was -3.0. running mean: -8.711174905507194, timestamp: 2022-08-20 14:18:39.050984\n",
      "resetting env. episode 2314, reward total was -13.0. running mean: -8.754063156452123, timestamp: 2022-08-20 14:18:58.594728\n",
      "resetting env. episode 2315, reward total was -12.0. running mean: -8.7865225248876, timestamp: 2022-08-20 14:19:17.024463\n",
      "resetting env. episode 2316, reward total was -13.0. running mean: -8.828657299638724, timestamp: 2022-08-20 14:19:34.378083\n",
      "resetting env. episode 2317, reward total was -2.0. running mean: -8.760370726642336, timestamp: 2022-08-20 14:19:56.419161\n",
      "resetting env. episode 2318, reward total was -1.0. running mean: -8.682767019375913, timestamp: 2022-08-20 14:20:19.182337\n",
      "resetting env. episode 2319, reward total was -11.0. running mean: -8.705939349182152, timestamp: 2022-08-20 14:20:40.312833\n",
      "resetting env. episode 2320, reward total was -3.0. running mean: -8.64887995569033, timestamp: 2022-08-20 14:21:05.338967\n",
      "resetting env. episode 2321, reward total was -8.0. running mean: -8.642391156133426, timestamp: 2022-08-20 14:21:26.459482\n",
      "resetting env. episode 2322, reward total was -12.0. running mean: -8.67596724457209, timestamp: 2022-08-20 14:21:44.049466\n",
      "resetting env. episode 2323, reward total was -7.0. running mean: -8.65920757212637, timestamp: 2022-08-20 14:22:02.512116\n",
      "resetting env. episode 2324, reward total was -4.0. running mean: -8.612615496405105, timestamp: 2022-08-20 14:22:27.069473\n",
      "resetting env. episode 2325, reward total was -6.0. running mean: -8.586489341441055, timestamp: 2022-08-20 14:22:47.983572\n",
      "resetting env. episode 2326, reward total was -15.0. running mean: -8.650624448026646, timestamp: 2022-08-20 14:23:02.794990\n",
      "resetting env. episode 2327, reward total was -14.0. running mean: -8.704118203546379, timestamp: 2022-08-20 14:23:16.610050\n",
      "resetting env. episode 2328, reward total was -7.0. running mean: -8.687077021510916, timestamp: 2022-08-20 14:23:40.613889\n",
      "resetting env. episode 2329, reward total was 1.0. running mean: -8.590206251295808, timestamp: 2022-08-20 14:24:04.986740\n",
      "resetting env. episode 2330, reward total was -17.0. running mean: -8.67430418878285, timestamp: 2022-08-20 14:24:18.948427\n",
      "resetting env. episode 2331, reward total was -14.0. running mean: -8.727561146895022, timestamp: 2022-08-20 14:24:35.539098\n",
      "resetting env. episode 2332, reward total was -9.0. running mean: -8.730285535426072, timestamp: 2022-08-20 14:24:54.473180\n",
      "resetting env. episode 2333, reward total was -11.0. running mean: -8.75298268007181, timestamp: 2022-08-20 14:25:10.294875\n",
      "resetting env. episode 2334, reward total was -7.0. running mean: -8.735452853271092, timestamp: 2022-08-20 14:25:37.093241\n",
      "resetting env. episode 2335, reward total was -6.0. running mean: -8.708098324738382, timestamp: 2022-08-20 14:26:00.951475\n",
      "resetting env. episode 2336, reward total was -6.0. running mean: -8.681017341490998, timestamp: 2022-08-20 14:26:24.038760\n",
      "resetting env. episode 2337, reward total was -10.0. running mean: -8.694207168076087, timestamp: 2022-08-20 14:26:44.525003\n",
      "resetting env. episode 2338, reward total was -3.0. running mean: -8.637265096395325, timestamp: 2022-08-20 14:27:13.013849\n",
      "resetting env. episode 2339, reward total was -8.0. running mean: -8.630892445431371, timestamp: 2022-08-20 14:27:38.425946\n",
      "resetting env. episode 2340, reward total was 4.0. running mean: -8.504583520977059, timestamp: 2022-08-20 14:28:08.831650\n",
      "resetting env. episode 2341, reward total was -10.0. running mean: -8.519537685767288, timestamp: 2022-08-20 14:28:26.360792\n",
      "resetting env. episode 2342, reward total was -11.0. running mean: -8.544342308909615, timestamp: 2022-08-20 14:28:41.085458\n",
      "resetting env. episode 2343, reward total was 7.0. running mean: -8.388898885820518, timestamp: 2022-08-20 14:29:00.481599\n",
      "resetting env. episode 2344, reward total was -8.0. running mean: -8.385009896962313, timestamp: 2022-08-20 14:29:21.089500\n",
      "resetting env. episode 2345, reward total was -4.0. running mean: -8.341159797992688, timestamp: 2022-08-20 14:29:44.820085\n",
      "resetting env. episode 2346, reward total was -10.0. running mean: -8.357748200012761, timestamp: 2022-08-20 14:30:06.987813\n",
      "resetting env. episode 2347, reward total was -7.0. running mean: -8.344170718012634, timestamp: 2022-08-20 14:30:29.962804\n",
      "resetting env. episode 2348, reward total was -11.0. running mean: -8.370729010832507, timestamp: 2022-08-20 14:30:46.726997\n",
      "resetting env. episode 2349, reward total was -9.0. running mean: -8.377021720724182, timestamp: 2022-08-20 14:31:07.031704\n",
      "resetting env. episode 2350, reward total was -6.0. running mean: -8.35325150351694, timestamp: 2022-08-20 14:31:27.770279\n",
      "resetting env. episode 2351, reward total was -13.0. running mean: -8.399718988481771, timestamp: 2022-08-20 14:31:48.079991\n",
      "resetting env. episode 2352, reward total was -12.0. running mean: -8.435721798596953, timestamp: 2022-08-20 14:32:06.041969\n",
      "resetting env. episode 2353, reward total was 9.0. running mean: -8.261364580610984, timestamp: 2022-08-20 14:32:23.543188\n",
      "resetting env. episode 2354, reward total was -6.0. running mean: -8.238750934804875, timestamp: 2022-08-20 14:32:46.272481\n",
      "resetting env. episode 2355, reward total was 1.0. running mean: -8.146363425456826, timestamp: 2022-08-20 14:33:06.595160\n",
      "resetting env. episode 2356, reward total was -9.0. running mean: -8.154899791202258, timestamp: 2022-08-20 14:33:29.055126\n",
      "resetting env. episode 2357, reward total was -1.0. running mean: -8.083350793290235, timestamp: 2022-08-20 14:33:54.509086\n",
      "resetting env. episode 2358, reward total was -7.0. running mean: -8.072517285357332, timestamp: 2022-08-20 14:34:14.708096\n",
      "resetting env. episode 2359, reward total was -7.0. running mean: -8.061792112503758, timestamp: 2022-08-20 14:34:34.215947\n",
      "resetting env. episode 2360, reward total was -13.0. running mean: -8.111174191378721, timestamp: 2022-08-20 14:34:50.939247\n",
      "resetting env. episode 2361, reward total was -13.0. running mean: -8.160062449464935, timestamp: 2022-08-20 14:35:08.442460\n",
      "resetting env. episode 2362, reward total was 3.0. running mean: -8.048461824970286, timestamp: 2022-08-20 14:35:29.955962\n",
      "resetting env. episode 2363, reward total was 5.0. running mean: -7.9179772067205825, timestamp: 2022-08-20 14:35:49.667270\n",
      "resetting env. episode 2364, reward total was -12.0. running mean: -7.958797434653377, timestamp: 2022-08-20 14:36:11.480010\n",
      "resetting env. episode 2365, reward total was -7.0. running mean: -7.949209460306843, timestamp: 2022-08-20 14:36:33.066289\n",
      "resetting env. episode 2366, reward total was -8.0. running mean: -7.9497173657037745, timestamp: 2022-08-20 14:36:49.009643\n",
      "resetting env. episode 2367, reward total was -12.0. running mean: -7.990220192046737, timestamp: 2022-08-20 14:37:08.412785\n",
      "resetting env. episode 2368, reward total was -14.0. running mean: -8.050317990126269, timestamp: 2022-08-20 14:37:26.011736\n",
      "resetting env. episode 2369, reward total was -13.0. running mean: -8.099814810225006, timestamp: 2022-08-20 14:37:40.337447\n",
      "resetting env. episode 2370, reward total was -8.0. running mean: -8.098816662122756, timestamp: 2022-08-20 14:38:00.215312\n",
      "resetting env. episode 2371, reward total was -11.0. running mean: -8.127828495501527, timestamp: 2022-08-20 14:38:15.256110\n",
      "resetting env. episode 2372, reward total was -15.0. running mean: -8.196550210546512, timestamp: 2022-08-20 14:38:34.783909\n",
      "resetting env. episode 2373, reward total was -10.0. running mean: -8.214584708441047, timestamp: 2022-08-20 14:38:57.901122\n",
      "resetting env. episode 2374, reward total was -7.0. running mean: -8.202438861356637, timestamp: 2022-08-20 14:39:16.386759\n",
      "resetting env. episode 2375, reward total was -9.0. running mean: -8.21041447274307, timestamp: 2022-08-20 14:39:33.697439\n",
      "resetting env. episode 2376, reward total was -10.0. running mean: -8.228310328015638, timestamp: 2022-08-20 14:39:54.447000\n",
      "resetting env. episode 2377, reward total was -3.0. running mean: -8.176027224735481, timestamp: 2022-08-20 14:40:20.948165\n",
      "resetting env. episode 2378, reward total was -3.0. running mean: -8.124266952488126, timestamp: 2022-08-20 14:40:44.181062\n",
      "resetting env. episode 2379, reward total was 2.0. running mean: -8.023024282963245, timestamp: 2022-08-20 14:41:08.370405\n",
      "resetting env. episode 2380, reward total was 6.0. running mean: -7.882794040133613, timestamp: 2022-08-20 14:41:30.597989\n",
      "resetting env. episode 2381, reward total was 2.0. running mean: -7.783966099732278, timestamp: 2022-08-20 14:41:54.004429\n",
      "resetting env. episode 2382, reward total was -13.0. running mean: -7.836126438734955, timestamp: 2022-08-20 14:42:11.072799\n",
      "resetting env. episode 2383, reward total was -9.0. running mean: -7.847765174347606, timestamp: 2022-08-20 14:42:34.358558\n",
      "resetting env. episode 2384, reward total was -12.0. running mean: -7.889287522604129, timestamp: 2022-08-20 14:42:54.139681\n",
      "resetting env. episode 2385, reward total was -10.0. running mean: -7.910394647378087, timestamp: 2022-08-20 14:43:09.511611\n",
      "resetting env. episode 2386, reward total was -10.0. running mean: -7.931290700904306, timestamp: 2022-08-20 14:43:29.105914\n",
      "resetting env. episode 2387, reward total was -12.0. running mean: -7.971977793895263, timestamp: 2022-08-20 14:43:49.171284\n",
      "resetting env. episode 2388, reward total was -15.0. running mean: -8.04225801595631, timestamp: 2022-08-20 14:44:01.842413\n",
      "resetting env. episode 2389, reward total was -11.0. running mean: -8.071835435796746, timestamp: 2022-08-20 14:44:17.268184\n",
      "resetting env. episode 2390, reward total was -9.0. running mean: -8.081117081438778, timestamp: 2022-08-20 14:44:35.899378\n",
      "resetting env. episode 2391, reward total was -8.0. running mean: -8.080305910624391, timestamp: 2022-08-20 14:44:53.696977\n",
      "resetting env. episode 2392, reward total was -19.0. running mean: -8.189502851518148, timestamp: 2022-08-20 14:45:05.698735\n",
      "resetting env. episode 2393, reward total was -3.0. running mean: -8.137607823002966, timestamp: 2022-08-20 14:45:26.783365\n",
      "resetting env. episode 2394, reward total was -5.0. running mean: -8.106231744772938, timestamp: 2022-08-20 14:45:47.576791\n",
      "resetting env. episode 2395, reward total was -2.0. running mean: -8.045169427325208, timestamp: 2022-08-20 14:46:10.043730\n",
      "resetting env. episode 2396, reward total was -10.0. running mean: -8.064717733051955, timestamp: 2022-08-20 14:46:26.158663\n",
      "resetting env. episode 2397, reward total was -3.0. running mean: -8.014070555721435, timestamp: 2022-08-20 14:46:53.286288\n",
      "resetting env. episode 2398, reward total was -8.0. running mean: -8.01392985016422, timestamp: 2022-08-20 14:47:10.357657\n",
      "resetting env. episode 2399, reward total was -5.0. running mean: -7.983790551662579, timestamp: 2022-08-20 14:47:30.388126\n",
      "resetting env. episode 2400, reward total was -11.0. running mean: -8.013952646145952, timestamp: 2022-08-20 14:47:50.378703\n",
      "resetting env. episode 2401, reward total was -4.0. running mean: -7.973813119684493, timestamp: 2022-08-20 14:48:13.379202\n",
      "resetting env. episode 2402, reward total was -6.0. running mean: -7.954074988487648, timestamp: 2022-08-20 14:48:36.362765\n",
      "resetting env. episode 2403, reward total was -10.0. running mean: -7.974534238602771, timestamp: 2022-08-20 14:48:56.439104\n",
      "resetting env. episode 2404, reward total was -9.0. running mean: -7.984788896216743, timestamp: 2022-08-20 14:49:18.951924\n",
      "resetting env. episode 2405, reward total was -11.0. running mean: -8.014941007254574, timestamp: 2022-08-20 14:49:37.298881\n",
      "resetting env. episode 2406, reward total was 5.0. running mean: -7.884791597182028, timestamp: 2022-08-20 14:49:53.434753\n",
      "resetting env. episode 2407, reward total was -9.0. running mean: -7.895943681210207, timestamp: 2022-08-20 14:50:13.170996\n",
      "resetting env. episode 2408, reward total was -12.0. running mean: -7.936984244398105, timestamp: 2022-08-20 14:50:28.904021\n",
      "resetting env. episode 2409, reward total was -11.0. running mean: -7.967614401954124, timestamp: 2022-08-20 14:50:48.667116\n",
      "resetting env. episode 2410, reward total was -15.0. running mean: -8.037938257934583, timestamp: 2022-08-20 14:51:04.729209\n",
      "resetting env. episode 2411, reward total was -7.0. running mean: -8.027558875355236, timestamp: 2022-08-20 14:51:26.073128\n",
      "resetting env. episode 2412, reward total was -1.0. running mean: -7.957283286601684, timestamp: 2022-08-20 14:51:53.055006\n",
      "resetting env. episode 2413, reward total was -4.0. running mean: -7.917710453735667, timestamp: 2022-08-20 14:52:19.149255\n",
      "resetting env. episode 2414, reward total was -10.0. running mean: -7.93853334919831, timestamp: 2022-08-20 14:52:34.276827\n",
      "resetting env. episode 2415, reward total was -8.0. running mean: -7.939148015706327, timestamp: 2022-08-20 14:52:54.099890\n",
      "resetting env. episode 2416, reward total was -2.0. running mean: -7.879756535549263, timestamp: 2022-08-20 14:53:13.818218\n",
      "resetting env. episode 2417, reward total was -7.0. running mean: -7.8709589701937706, timestamp: 2022-08-20 14:53:33.017832\n",
      "resetting env. episode 2418, reward total was 1.0. running mean: -7.782249380491833, timestamp: 2022-08-20 14:53:54.016704\n",
      "resetting env. episode 2419, reward total was -10.0. running mean: -7.804426886686914, timestamp: 2022-08-20 14:54:13.128622\n",
      "resetting env. episode 2420, reward total was -13.0. running mean: -7.856382617820045, timestamp: 2022-08-20 14:54:28.662103\n",
      "resetting env. episode 2421, reward total was -7.0. running mean: -7.847818791641845, timestamp: 2022-08-20 14:54:54.652622\n",
      "resetting env. episode 2422, reward total was -5.0. running mean: -7.819340603725426, timestamp: 2022-08-20 14:55:14.718990\n",
      "resetting env. episode 2423, reward total was -5.0. running mean: -7.791147197688171, timestamp: 2022-08-20 14:55:33.436953\n",
      "resetting env. episode 2424, reward total was -9.0. running mean: -7.803235725711289, timestamp: 2022-08-20 14:55:53.948127\n",
      "resetting env. episode 2425, reward total was 9.0. running mean: -7.635203368454176, timestamp: 2022-08-20 14:56:12.371880\n",
      "resetting env. episode 2426, reward total was -5.0. running mean: -7.608851334769634, timestamp: 2022-08-20 14:56:32.580862\n",
      "resetting env. episode 2427, reward total was -7.0. running mean: -7.602762821421938, timestamp: 2022-08-20 14:56:53.834055\n",
      "resetting env. episode 2428, reward total was -7.0. running mean: -7.596735193207719, timestamp: 2022-08-20 14:57:13.368835\n",
      "resetting env. episode 2429, reward total was 5.0. running mean: -7.4707678412756415, timestamp: 2022-08-20 14:57:41.090733\n",
      "resetting env. episode 2430, reward total was -1.0. running mean: -7.406060162862885, timestamp: 2022-08-20 14:58:08.660053\n",
      "resetting env. episode 2431, reward total was -12.0. running mean: -7.451999561234256, timestamp: 2022-08-20 14:58:23.830490\n",
      "resetting env. episode 2432, reward total was -7.0. running mean: -7.447479565621914, timestamp: 2022-08-20 14:58:43.301444\n",
      "resetting env. episode 2433, reward total was -9.0. running mean: -7.4630047699656945, timestamp: 2022-08-20 14:59:05.806291\n",
      "resetting env. episode 2434, reward total was -9.0. running mean: -7.478374722266038, timestamp: 2022-08-20 14:59:22.373007\n",
      "resetting env. episode 2435, reward total was -8.0. running mean: -7.483590975043377, timestamp: 2022-08-20 14:59:37.287140\n",
      "resetting env. episode 2436, reward total was -11.0. running mean: -7.518755065292944, timestamp: 2022-08-20 14:59:55.636097\n",
      "resetting env. episode 2437, reward total was -4.0. running mean: -7.483567514640015, timestamp: 2022-08-20 15:00:16.674857\n",
      "resetting env. episode 2438, reward total was -6.0. running mean: -7.468731839493614, timestamp: 2022-08-20 15:00:38.442678\n",
      "resetting env. episode 2439, reward total was -17.0. running mean: -7.564044521098678, timestamp: 2022-08-20 15:00:55.360450\n",
      "resetting env. episode 2440, reward total was -5.0. running mean: -7.538404075887691, timestamp: 2022-08-20 15:01:21.752914\n",
      "resetting env. episode 2441, reward total was -10.0. running mean: -7.5630200351288135, timestamp: 2022-08-20 15:01:39.454587\n",
      "resetting env. episode 2442, reward total was -17.0. running mean: -7.6573898347775256, timestamp: 2022-08-20 15:01:54.986071\n",
      "resetting env. episode 2443, reward total was -4.0. running mean: -7.62081593642975, timestamp: 2022-08-20 15:02:21.645814\n",
      "resetting env. episode 2444, reward total was -5.0. running mean: -7.594607777065453, timestamp: 2022-08-20 15:02:40.988107\n",
      "resetting env. episode 2445, reward total was -5.0. running mean: -7.568661699294798, timestamp: 2022-08-20 15:03:01.147226\n",
      "resetting env. episode 2446, reward total was 4.0. running mean: -7.452975082301849, timestamp: 2022-08-20 15:03:18.212622\n",
      "resetting env. episode 2447, reward total was -7.0. running mean: -7.448445331478831, timestamp: 2022-08-20 15:03:36.962499\n",
      "resetting env. episode 2448, reward total was -16.0. running mean: -7.533960878164042, timestamp: 2022-08-20 15:03:52.615663\n",
      "resetting env. episode 2449, reward total was 5.0. running mean: -7.408621269382402, timestamp: 2022-08-20 15:04:13.205610\n",
      "resetting env. episode 2450, reward total was -5.0. running mean: -7.384535056688578, timestamp: 2022-08-20 15:04:36.811514\n",
      "resetting env. episode 2451, reward total was -9.0. running mean: -7.400689706121692, timestamp: 2022-08-20 15:04:52.066757\n",
      "resetting env. episode 2452, reward total was -2.0. running mean: -7.346682809060474, timestamp: 2022-08-20 15:05:17.012054\n",
      "resetting env. episode 2453, reward total was -1.0. running mean: -7.283215980969869, timestamp: 2022-08-20 15:05:38.381935\n",
      "resetting env. episode 2454, reward total was -15.0. running mean: -7.360383821160171, timestamp: 2022-08-20 15:05:57.315331\n",
      "resetting env. episode 2455, reward total was -1.0. running mean: -7.296779982948569, timestamp: 2022-08-20 15:06:20.015646\n",
      "resetting env. episode 2456, reward total was -6.0. running mean: -7.283812183119083, timestamp: 2022-08-20 15:06:37.713356\n",
      "resetting env. episode 2457, reward total was -7.0. running mean: -7.280974061287892, timestamp: 2022-08-20 15:06:58.041003\n",
      "resetting env. episode 2458, reward total was -6.0. running mean: -7.268164320675012, timestamp: 2022-08-20 15:07:18.138301\n",
      "resetting env. episode 2459, reward total was -1.0. running mean: -7.205482677468262, timestamp: 2022-08-20 15:07:46.573289\n",
      "resetting env. episode 2460, reward total was -2.0. running mean: -7.1534278506935784, timestamp: 2022-08-20 15:08:08.421880\n",
      "resetting env. episode 2461, reward total was -6.0. running mean: -7.141893572186643, timestamp: 2022-08-20 15:08:28.538105\n",
      "resetting env. episode 2462, reward total was 1.0. running mean: -7.060474636464776, timestamp: 2022-08-20 15:08:55.680565\n",
      "resetting env. episode 2463, reward total was -12.0. running mean: -7.1098698901001285, timestamp: 2022-08-20 15:09:14.452375\n",
      "resetting env. episode 2464, reward total was -9.0. running mean: -7.128771191199127, timestamp: 2022-08-20 15:09:33.291023\n",
      "resetting env. episode 2465, reward total was -11.0. running mean: -7.167483479287136, timestamp: 2022-08-20 15:09:53.372474\n",
      "resetting env. episode 2466, reward total was -7.0. running mean: -7.165808644494265, timestamp: 2022-08-20 15:10:15.113376\n",
      "resetting env. episode 2467, reward total was -11.0. running mean: -7.204150558049323, timestamp: 2022-08-20 15:10:33.678735\n",
      "resetting env. episode 2468, reward total was -9.0. running mean: -7.222109052468829, timestamp: 2022-08-20 15:10:50.332221\n",
      "resetting env. episode 2469, reward total was 3.0. running mean: -7.119887961944141, timestamp: 2022-08-20 15:11:12.365327\n",
      "resetting env. episode 2470, reward total was 9.0. running mean: -6.9586890823247, timestamp: 2022-08-20 15:11:29.785769\n",
      "resetting env. episode 2471, reward total was -13.0. running mean: -7.019102191501452, timestamp: 2022-08-20 15:11:53.516330\n",
      "resetting env. episode 2472, reward total was -15.0. running mean: -7.0989111695864375, timestamp: 2022-08-20 15:12:12.871597\n",
      "resetting env. episode 2473, reward total was -8.0. running mean: -7.1079220578905735, timestamp: 2022-08-20 15:12:31.615493\n",
      "resetting env. episode 2474, reward total was 6.0. running mean: -6.976842837311668, timestamp: 2022-08-20 15:12:56.311489\n",
      "resetting env. episode 2475, reward total was -6.0. running mean: -6.967074408938552, timestamp: 2022-08-20 15:13:17.686357\n",
      "resetting env. episode 2476, reward total was -13.0. running mean: -7.027403664849166, timestamp: 2022-08-20 15:13:30.921979\n",
      "resetting env. episode 2477, reward total was -7.0. running mean: -7.027129628200674, timestamp: 2022-08-20 15:13:55.227006\n",
      "resetting env. episode 2478, reward total was -5.0. running mean: -7.006858331918667, timestamp: 2022-08-20 15:14:19.106193\n",
      "resetting env. episode 2479, reward total was -9.0. running mean: -7.02678974859948, timestamp: 2022-08-20 15:14:38.603061\n",
      "resetting env. episode 2480, reward total was -3.0. running mean: -6.986521851113485, timestamp: 2022-08-20 15:15:01.030113\n",
      "resetting env. episode 2481, reward total was -7.0. running mean: -6.986656632602351, timestamp: 2022-08-20 15:15:21.796609\n",
      "resetting env. episode 2482, reward total was -9.0. running mean: -7.006790066276327, timestamp: 2022-08-20 15:15:40.566436\n",
      "resetting env. episode 2483, reward total was -5.0. running mean: -6.986722165613564, timestamp: 2022-08-20 15:16:02.331258\n",
      "resetting env. episode 2484, reward total was -5.0. running mean: -6.966854943957428, timestamp: 2022-08-20 15:16:29.280226\n",
      "resetting env. episode 2485, reward total was 2.0. running mean: -6.877186394517855, timestamp: 2022-08-20 15:16:50.388707\n",
      "resetting env. episode 2486, reward total was -15.0. running mean: -6.958414530572677, timestamp: 2022-08-20 15:17:05.372667\n",
      "resetting env. episode 2487, reward total was 7.0. running mean: -6.81883038526695, timestamp: 2022-08-20 15:17:27.393457\n",
      "resetting env. episode 2488, reward total was -9.0. running mean: -6.8406420814142805, timestamp: 2022-08-20 15:17:47.156633\n",
      "resetting env. episode 2489, reward total was -7.0. running mean: -6.842235660600138, timestamp: 2022-08-20 15:18:05.374934\n",
      "resetting env. episode 2490, reward total was 1.0. running mean: -6.763813303994136, timestamp: 2022-08-20 15:18:32.833562\n",
      "resetting env. episode 2491, reward total was 4.0. running mean: -6.656175170954195, timestamp: 2022-08-20 15:18:53.036535\n",
      "resetting env. episode 2492, reward total was -15.0. running mean: -6.7396134192446535, timestamp: 2022-08-20 15:19:12.451639\n",
      "resetting env. episode 2493, reward total was -15.0. running mean: -6.822217285052207, timestamp: 2022-08-20 15:19:32.383167\n",
      "resetting env. episode 2494, reward total was -4.0. running mean: -6.793995112201685, timestamp: 2022-08-20 15:19:59.158595\n",
      "resetting env. episode 2495, reward total was -10.0. running mean: -6.826055161079668, timestamp: 2022-08-20 15:20:15.648066\n",
      "resetting env. episode 2496, reward total was -11.0. running mean: -6.867794609468872, timestamp: 2022-08-20 15:20:34.695175\n",
      "resetting env. episode 2497, reward total was -13.0. running mean: -6.929116663374183, timestamp: 2022-08-20 15:20:53.624097\n",
      "resetting env. episode 2498, reward total was -1.0. running mean: -6.86982549674044, timestamp: 2022-08-20 15:21:16.109992\n",
      "resetting env. episode 2499, reward total was -6.0. running mean: -6.861127241773035, timestamp: 2022-08-20 15:21:38.719561\n",
      "resetting env. episode 2500, reward total was -7.0. running mean: -6.862515969355305, timestamp: 2022-08-20 15:21:56.843119\n",
      "resetting env. episode 2501, reward total was -13.0. running mean: -6.923890809661751, timestamp: 2022-08-20 15:22:16.575384\n",
      "resetting env. episode 2502, reward total was -10.0. running mean: -6.954651901565133, timestamp: 2022-08-20 15:22:31.265104\n",
      "resetting env. episode 2503, reward total was -5.0. running mean: -6.935105382549481, timestamp: 2022-08-20 15:22:54.765283\n",
      "resetting env. episode 2504, reward total was -9.0. running mean: -6.9557543287239865, timestamp: 2022-08-20 15:23:16.482743\n",
      "resetting env. episode 2505, reward total was -14.0. running mean: -7.026196785436746, timestamp: 2022-08-20 15:23:29.818108\n",
      "resetting env. episode 2506, reward total was 4.0. running mean: -6.915934817582379, timestamp: 2022-08-20 15:23:53.094882\n",
      "resetting env. episode 2507, reward total was -3.0. running mean: -6.876775469406555, timestamp: 2022-08-20 15:24:16.517272\n",
      "resetting env. episode 2508, reward total was -15.0. running mean: -6.9580077147124895, timestamp: 2022-08-20 15:24:30.430082\n",
      "resetting env. episode 2509, reward total was 5.0. running mean: -6.838427637565364, timestamp: 2022-08-20 15:24:54.328354\n",
      "resetting env. episode 2510, reward total was -10.0. running mean: -6.87004336118971, timestamp: 2022-08-20 15:25:09.869810\n",
      "resetting env. episode 2511, reward total was -7.0. running mean: -6.871342927577813, timestamp: 2022-08-20 15:25:30.471741\n",
      "resetting env. episode 2512, reward total was -7.0. running mean: -6.872629498302035, timestamp: 2022-08-20 15:25:51.398806\n",
      "resetting env. episode 2513, reward total was -7.0. running mean: -6.873903203319015, timestamp: 2022-08-20 15:26:11.995996\n",
      "resetting env. episode 2514, reward total was -7.0. running mean: -6.875164171285825, timestamp: 2022-08-20 15:26:34.343260\n",
      "resetting env. episode 2515, reward total was -9.0. running mean: -6.896412529572967, timestamp: 2022-08-20 15:26:51.897328\n",
      "resetting env. episode 2516, reward total was -15.0. running mean: -6.977448404277237, timestamp: 2022-08-20 15:27:07.044864\n",
      "resetting env. episode 2517, reward total was -10.0. running mean: -7.007673920234464, timestamp: 2022-08-20 15:27:26.295380\n",
      "resetting env. episode 2518, reward total was -14.0. running mean: -7.077597181032119, timestamp: 2022-08-20 15:27:44.795935\n",
      "resetting env. episode 2519, reward total was -5.0. running mean: -7.0568212092217975, timestamp: 2022-08-20 15:28:07.584015\n",
      "resetting env. episode 2520, reward total was -8.0. running mean: -7.066252997129579, timestamp: 2022-08-20 15:28:26.330905\n",
      "resetting env. episode 2521, reward total was -5.0. running mean: -7.045590467158283, timestamp: 2022-08-20 15:28:52.199762\n",
      "resetting env. episode 2522, reward total was 2.0. running mean: -6.955134562486701, timestamp: 2022-08-20 15:29:13.916714\n",
      "resetting env. episode 2523, reward total was -8.0. running mean: -6.965583216861834, timestamp: 2022-08-20 15:29:33.494376\n",
      "resetting env. episode 2524, reward total was -16.0. running mean: -7.055927384693216, timestamp: 2022-08-20 15:29:50.396209\n",
      "resetting env. episode 2525, reward total was -11.0. running mean: -7.095368110846284, timestamp: 2022-08-20 15:30:05.948637\n",
      "resetting env. episode 2526, reward total was -10.0. running mean: -7.12441442973782, timestamp: 2022-08-20 15:30:22.871395\n",
      "resetting env. episode 2527, reward total was -5.0. running mean: -7.1031702854404415, timestamp: 2022-08-20 15:30:41.873599\n",
      "resetting env. episode 2528, reward total was -16.0. running mean: -7.192138582586037, timestamp: 2022-08-20 15:30:56.569327\n",
      "resetting env. episode 2529, reward total was -9.0. running mean: -7.210217196760176, timestamp: 2022-08-20 15:31:15.020017\n",
      "resetting env. episode 2530, reward total was -11.0. running mean: -7.2481150247925745, timestamp: 2022-08-20 15:31:37.736279\n",
      "resetting env. episode 2531, reward total was -6.0. running mean: -7.235633874544648, timestamp: 2022-08-20 15:32:00.283015\n",
      "resetting env. episode 2532, reward total was -7.0. running mean: -7.233277535799202, timestamp: 2022-08-20 15:32:18.254990\n",
      "resetting env. episode 2533, reward total was -7.0. running mean: -7.23094476044121, timestamp: 2022-08-20 15:32:40.458624\n",
      "resetting env. episode 2534, reward total was 6.0. running mean: -7.098635312836798, timestamp: 2022-08-20 15:33:02.451836\n",
      "resetting env. episode 2535, reward total was -15.0. running mean: -7.1776489597084305, timestamp: 2022-08-20 15:33:20.235298\n",
      "resetting env. episode 2536, reward total was -10.0. running mean: -7.205872470111346, timestamp: 2022-08-20 15:33:40.054324\n",
      "resetting env. episode 2537, reward total was -8.0. running mean: -7.213813745410232, timestamp: 2022-08-20 15:33:57.997388\n",
      "resetting env. episode 2538, reward total was 4.0. running mean: -7.10167560795613, timestamp: 2022-08-20 15:34:18.439722\n",
      "resetting env. episode 2539, reward total was -12.0. running mean: -7.150658851876569, timestamp: 2022-08-20 15:34:36.198260\n",
      "resetting env. episode 2540, reward total was 4.0. running mean: -7.039152263357804, timestamp: 2022-08-20 15:35:00.085401\n",
      "resetting env. episode 2541, reward total was 2.0. running mean: -6.948760740724226, timestamp: 2022-08-20 15:35:21.657744\n",
      "resetting env. episode 2542, reward total was -8.0. running mean: -6.959273133316984, timestamp: 2022-08-20 15:35:42.756344\n",
      "resetting env. episode 2543, reward total was -10.0. running mean: -6.989680401983813, timestamp: 2022-08-20 15:36:01.242459\n",
      "resetting env. episode 2544, reward total was 5.0. running mean: -6.869783597963975, timestamp: 2022-08-20 15:36:22.941466\n",
      "resetting env. episode 2545, reward total was -14.0. running mean: -6.941085761984335, timestamp: 2022-08-20 15:36:38.485907\n",
      "resetting env. episode 2546, reward total was -13.0. running mean: -7.001674904364492, timestamp: 2022-08-20 15:36:53.582542\n",
      "resetting env. episode 2547, reward total was -15.0. running mean: -7.081658155320848, timestamp: 2022-08-20 15:37:09.995665\n",
      "resetting env. episode 2548, reward total was 4.0. running mean: -6.970841573767639, timestamp: 2022-08-20 15:37:34.181018\n",
      "resetting env. episode 2549, reward total was -5.0. running mean: -6.951133158029962, timestamp: 2022-08-20 15:37:54.379031\n",
      "resetting env. episode 2550, reward total was -1.0. running mean: -6.891621826449662, timestamp: 2022-08-20 15:38:13.848986\n",
      "resetting env. episode 2551, reward total was -15.0. running mean: -6.972705608185166, timestamp: 2022-08-20 15:38:25.710338\n",
      "resetting env. episode 2552, reward total was -11.0. running mean: -7.0129785521033146, timestamp: 2022-08-20 15:38:46.149701\n",
      "resetting env. episode 2553, reward total was 3.0. running mean: -6.9128487665822815, timestamp: 2022-08-20 15:39:06.540198\n",
      "resetting env. episode 2554, reward total was 12.0. running mean: -6.723720278916459, timestamp: 2022-08-20 15:39:23.951670\n",
      "resetting env. episode 2555, reward total was -6.0. running mean: -6.716483076127294, timestamp: 2022-08-20 15:39:45.191883\n",
      "resetting env. episode 2556, reward total was -3.0. running mean: -6.679318245366021, timestamp: 2022-08-20 15:40:08.458981\n",
      "resetting env. episode 2557, reward total was -1.0. running mean: -6.622525062912361, timestamp: 2022-08-20 15:40:34.601101\n",
      "resetting env. episode 2558, reward total was -7.0. running mean: -6.6262998122832375, timestamp: 2022-08-20 15:40:54.192737\n",
      "resetting env. episode 2559, reward total was -11.0. running mean: -6.670036814160405, timestamp: 2022-08-20 15:41:14.492472\n",
      "resetting env. episode 2560, reward total was 8.0. running mean: -6.523336446018801, timestamp: 2022-08-20 15:41:29.504344\n",
      "resetting env. episode 2561, reward total was -9.0. running mean: -6.5481030815586125, timestamp: 2022-08-20 15:41:51.037788\n",
      "resetting env. episode 2562, reward total was 7.0. running mean: -6.4126220507430265, timestamp: 2022-08-20 15:42:12.393733\n",
      "resetting env. episode 2563, reward total was 4.0. running mean: -6.3084958302355965, timestamp: 2022-08-20 15:42:32.022233\n",
      "resetting env. episode 2564, reward total was -6.0. running mean: -6.3054108719332405, timestamp: 2022-08-20 15:42:57.279721\n",
      "resetting env. episode 2565, reward total was -3.0. running mean: -6.272356763213908, timestamp: 2022-08-20 15:43:18.106055\n",
      "resetting env. episode 2566, reward total was -14.0. running mean: -6.349633195581769, timestamp: 2022-08-20 15:43:32.999247\n",
      "resetting env. episode 2567, reward total was -9.0. running mean: -6.376136863625951, timestamp: 2022-08-20 15:43:53.611162\n",
      "resetting env. episode 2568, reward total was -8.0. running mean: -6.392375494989691, timestamp: 2022-08-20 15:44:10.188847\n",
      "resetting env. episode 2569, reward total was 6.0. running mean: -6.2684517400397946, timestamp: 2022-08-20 15:44:32.157123\n",
      "resetting env. episode 2570, reward total was -7.0. running mean: -6.275767222639397, timestamp: 2022-08-20 15:44:51.379731\n",
      "resetting env. episode 2571, reward total was 1.0. running mean: -6.203009550413003, timestamp: 2022-08-20 15:45:19.519520\n",
      "resetting env. episode 2572, reward total was -9.0. running mean: -6.230979454908873, timestamp: 2022-08-20 15:45:37.479520\n",
      "resetting env. episode 2573, reward total was -16.0. running mean: -6.328669660359784, timestamp: 2022-08-20 15:45:54.441171\n",
      "resetting env. episode 2574, reward total was -10.0. running mean: -6.365382963756186, timestamp: 2022-08-20 15:46:14.193373\n",
      "resetting env. episode 2575, reward total was -4.0. running mean: -6.341729134118625, timestamp: 2022-08-20 15:46:36.117791\n",
      "resetting env. episode 2576, reward total was -10.0. running mean: -6.378311842777438, timestamp: 2022-08-20 15:46:52.209765\n",
      "resetting env. episode 2577, reward total was -10.0. running mean: -6.414528724349664, timestamp: 2022-08-20 15:47:09.347959\n",
      "resetting env. episode 2578, reward total was -10.0. running mean: -6.450383437106167, timestamp: 2022-08-20 15:47:29.627746\n",
      "resetting env. episode 2579, reward total was -7.0. running mean: -6.455879602735106, timestamp: 2022-08-20 15:47:51.828399\n",
      "resetting env. episode 2580, reward total was -9.0. running mean: -6.481320806707754, timestamp: 2022-08-20 15:48:11.208619\n",
      "resetting env. episode 2581, reward total was -7.0. running mean: -6.486507598640677, timestamp: 2022-08-20 15:48:34.954119\n",
      "resetting env. episode 2582, reward total was -13.0. running mean: -6.55164252265427, timestamp: 2022-08-20 15:48:51.586669\n",
      "resetting env. episode 2583, reward total was -7.0. running mean: -6.556126097427728, timestamp: 2022-08-20 15:49:11.081552\n",
      "resetting env. episode 2584, reward total was -6.0. running mean: -6.55056483645345, timestamp: 2022-08-20 15:49:34.091043\n",
      "resetting env. episode 2585, reward total was -4.0. running mean: -6.525059188088916, timestamp: 2022-08-20 15:49:59.869154\n",
      "resetting env. episode 2586, reward total was 3.0. running mean: -6.4298085962080265, timestamp: 2022-08-20 15:50:22.625315\n",
      "resetting env. episode 2587, reward total was -12.0. running mean: -6.4855105102459465, timestamp: 2022-08-20 15:50:40.390085\n",
      "resetting env. episode 2588, reward total was -17.0. running mean: -6.590655405143487, timestamp: 2022-08-20 15:50:56.500031\n",
      "resetting env. episode 2589, reward total was 4.0. running mean: -6.484748851092052, timestamp: 2022-08-20 15:51:16.965312\n",
      "resetting env. episode 2590, reward total was -12.0. running mean: -6.539901362581131, timestamp: 2022-08-20 15:51:33.398383\n",
      "resetting env. episode 2591, reward total was -6.0. running mean: -6.534502348955319, timestamp: 2022-08-20 15:51:56.131617\n",
      "resetting env. episode 2592, reward total was -5.0. running mean: -6.519157325465765, timestamp: 2022-08-20 15:52:18.123834\n",
      "resetting env. episode 2593, reward total was -9.0. running mean: -6.543965752211108, timestamp: 2022-08-20 15:52:37.607751\n",
      "resetting env. episode 2594, reward total was 3.0. running mean: -6.448526094688996, timestamp: 2022-08-20 15:53:02.028489\n",
      "resetting env. episode 2595, reward total was -13.0. running mean: -6.514040833742106, timestamp: 2022-08-20 15:53:16.049997\n",
      "resetting env. episode 2596, reward total was -11.0. running mean: -6.558900425404685, timestamp: 2022-08-20 15:53:35.042244\n",
      "resetting env. episode 2597, reward total was -8.0. running mean: -6.573311421150638, timestamp: 2022-08-20 15:54:00.121202\n",
      "resetting env. episode 2598, reward total was -15.0. running mean: -6.657578306939132, timestamp: 2022-08-20 15:54:20.086892\n",
      "resetting env. episode 2599, reward total was 2.0. running mean: -6.571002523869741, timestamp: 2022-08-20 15:54:49.486241\n",
      "resetting env. episode 2600, reward total was -21.0. running mean: -6.715292498631044, timestamp: 2022-08-20 15:55:03.878768\n",
      "resetting env. episode 2601, reward total was -2.0. running mean: -6.668139573644733, timestamp: 2022-08-20 15:55:31.854988\n",
      "resetting env. episode 2602, reward total was -3.0. running mean: -6.631458177908286, timestamp: 2022-08-20 15:55:54.336897\n",
      "resetting env. episode 2603, reward total was -11.0. running mean: -6.675143596129203, timestamp: 2022-08-20 15:56:12.332791\n",
      "resetting env. episode 2604, reward total was -15.0. running mean: -6.7583921601679116, timestamp: 2022-08-20 15:56:27.647854\n",
      "resetting env. episode 2605, reward total was -13.0. running mean: -6.820808238566232, timestamp: 2022-08-20 15:56:48.491140\n",
      "resetting env. episode 2606, reward total was -9.0. running mean: -6.84260015618057, timestamp: 2022-08-20 15:57:07.594131\n",
      "resetting env. episode 2607, reward total was -2.0. running mean: -6.794174154618764, timestamp: 2022-08-20 15:57:31.114262\n",
      "resetting env. episode 2608, reward total was -6.0. running mean: -6.786232413072576, timestamp: 2022-08-20 15:57:52.097179\n",
      "resetting env. episode 2609, reward total was -1.0. running mean: -6.72837008894185, timestamp: 2022-08-20 15:58:16.420168\n",
      "resetting env. episode 2610, reward total was -16.0. running mean: -6.821086388052432, timestamp: 2022-08-20 15:58:32.143143\n",
      "resetting env. episode 2611, reward total was -6.0. running mean: -6.812875524171908, timestamp: 2022-08-20 15:58:52.507696\n",
      "resetting env. episode 2612, reward total was -13.0. running mean: -6.874746768930188, timestamp: 2022-08-20 15:59:09.819422\n",
      "resetting env. episode 2613, reward total was -9.0. running mean: -6.895999301240886, timestamp: 2022-08-20 15:59:29.384134\n",
      "resetting env. episode 2614, reward total was -13.0. running mean: -6.957039308228477, timestamp: 2022-08-20 15:59:47.687209\n",
      "resetting env. episode 2615, reward total was 7.0. running mean: -6.8174689151461925, timestamp: 2022-08-20 16:00:04.491287\n",
      "resetting env. episode 2616, reward total was -10.0. running mean: -6.84929422599473, timestamp: 2022-08-20 16:00:23.290041\n",
      "resetting env. episode 2617, reward total was -8.0. running mean: -6.860801283734783, timestamp: 2022-08-20 16:00:42.893634\n",
      "resetting env. episode 2618, reward total was -8.0. running mean: -6.8721932708974345, timestamp: 2022-08-20 16:01:05.480261\n",
      "resetting env. episode 2619, reward total was -7.0. running mean: -6.873471338188461, timestamp: 2022-08-20 16:01:27.828533\n",
      "resetting env. episode 2620, reward total was -6.0. running mean: -6.864736624806576, timestamp: 2022-08-20 16:01:50.281646\n",
      "resetting env. episode 2621, reward total was -15.0. running mean: -6.94608925855851, timestamp: 2022-08-20 16:02:08.621637\n",
      "resetting env. episode 2622, reward total was -6.0. running mean: -6.936628365972925, timestamp: 2022-08-20 16:02:29.159731\n",
      "resetting env. episode 2623, reward total was -6.0. running mean: -6.927262082313195, timestamp: 2022-08-20 16:02:53.030917\n",
      "resetting env. episode 2624, reward total was 1.0. running mean: -6.847989461490063, timestamp: 2022-08-20 16:03:16.071330\n",
      "resetting env. episode 2625, reward total was -17.0. running mean: -6.949509566875163, timestamp: 2022-08-20 16:03:33.754064\n",
      "resetting env. episode 2626, reward total was -14.0. running mean: -7.0200144712064105, timestamp: 2022-08-20 16:03:50.554159\n",
      "resetting env. episode 2627, reward total was -8.0. running mean: -7.029814326494346, timestamp: 2022-08-20 16:04:11.509144\n",
      "resetting env. episode 2628, reward total was -6.0. running mean: -7.019516183229403, timestamp: 2022-08-20 16:04:32.278634\n",
      "resetting env. episode 2629, reward total was 4.0. running mean: -6.909321021397108, timestamp: 2022-08-20 16:04:52.524511\n",
      "resetting env. episode 2630, reward total was -9.0. running mean: -6.930227811183137, timestamp: 2022-08-20 16:05:16.838899\n",
      "resetting env. episode 2631, reward total was -16.0. running mean: -7.020925533071305, timestamp: 2022-08-20 16:05:36.849404\n",
      "resetting env. episode 2632, reward total was 4.0. running mean: -6.910716277740592, timestamp: 2022-08-20 16:05:59.194678\n",
      "resetting env. episode 2633, reward total was -15.0. running mean: -6.991609114963187, timestamp: 2022-08-20 16:06:16.968165\n",
      "resetting env. episode 2634, reward total was -10.0. running mean: -7.021693023813555, timestamp: 2022-08-20 16:06:35.802828\n",
      "resetting env. episode 2635, reward total was -5.0. running mean: -7.001476093575419, timestamp: 2022-08-20 16:06:58.985853\n",
      "resetting env. episode 2636, reward total was -4.0. running mean: -6.971461332639665, timestamp: 2022-08-20 16:07:21.515633\n",
      "resetting env. episode 2637, reward total was -13.0. running mean: -7.031746719313268, timestamp: 2022-08-20 16:07:38.025502\n",
      "resetting env. episode 2638, reward total was -15.0. running mean: -7.111429252120136, timestamp: 2022-08-20 16:07:54.460569\n",
      "resetting env. episode 2639, reward total was -3.0. running mean: -7.070314959598934, timestamp: 2022-08-20 16:08:17.683567\n",
      "resetting env. episode 2640, reward total was -14.0. running mean: -7.139611810002944, timestamp: 2022-08-20 16:08:37.770877\n",
      "resetting env. episode 2641, reward total was 1.0. running mean: -7.058215691902915, timestamp: 2022-08-20 16:09:01.805622\n",
      "resetting env. episode 2642, reward total was -15.0. running mean: -7.137633534983887, timestamp: 2022-08-20 16:09:16.723326\n",
      "resetting env. episode 2643, reward total was -9.0. running mean: -7.156257199634047, timestamp: 2022-08-20 16:09:33.217237\n",
      "resetting env. episode 2644, reward total was -11.0. running mean: -7.194694627637707, timestamp: 2022-08-20 16:09:56.139961\n",
      "resetting env. episode 2645, reward total was -9.0. running mean: -7.21274768136133, timestamp: 2022-08-20 16:10:18.752528\n",
      "resetting env. episode 2646, reward total was -7.0. running mean: -7.210620204547717, timestamp: 2022-08-20 16:10:37.057591\n",
      "resetting env. episode 2647, reward total was -15.0. running mean: -7.28851400250224, timestamp: 2022-08-20 16:10:54.628635\n",
      "resetting env. episode 2648, reward total was -1.0. running mean: -7.225628862477218, timestamp: 2022-08-20 16:11:19.743495\n",
      "resetting env. episode 2649, reward total was 4.0. running mean: -7.113372573852446, timestamp: 2022-08-20 16:11:45.061814\n",
      "resetting env. episode 2650, reward total was -3.0. running mean: -7.072238848113921, timestamp: 2022-08-20 16:12:05.971920\n",
      "resetting env. episode 2651, reward total was -7.0. running mean: -7.0715164596327815, timestamp: 2022-08-20 16:12:28.149641\n",
      "resetting env. episode 2652, reward total was -13.0. running mean: -7.130801295036454, timestamp: 2022-08-20 16:12:49.332032\n",
      "resetting env. episode 2653, reward total was -6.0. running mean: -7.119493282086089, timestamp: 2022-08-20 16:13:14.902670\n",
      "resetting env. episode 2654, reward total was -8.0. running mean: -7.128298349265228, timestamp: 2022-08-20 16:13:36.186787\n",
      "resetting env. episode 2655, reward total was -15.0. running mean: -7.207015365772576, timestamp: 2022-08-20 16:13:51.272452\n",
      "resetting env. episode 2656, reward total was -17.0. running mean: -7.30494521211485, timestamp: 2022-08-20 16:14:08.279992\n",
      "resetting env. episode 2657, reward total was -3.0. running mean: -7.261895759993702, timestamp: 2022-08-20 16:14:28.999622\n",
      "resetting env. episode 2658, reward total was -12.0. running mean: -7.309276802393764, timestamp: 2022-08-20 16:14:45.102570\n",
      "resetting env. episode 2659, reward total was -5.0. running mean: -7.2861840343698265, timestamp: 2022-08-20 16:15:05.640667\n",
      "resetting env. episode 2660, reward total was -18.0. running mean: -7.393322194026128, timestamp: 2022-08-20 16:15:20.906892\n",
      "resetting env. episode 2661, reward total was -11.0. running mean: -7.429388972085867, timestamp: 2022-08-20 16:15:39.843242\n",
      "resetting env. episode 2662, reward total was -9.0. running mean: -7.445095082365008, timestamp: 2022-08-20 16:15:57.725458\n",
      "resetting env. episode 2663, reward total was -4.0. running mean: -7.410644131541359, timestamp: 2022-08-20 16:16:18.029172\n",
      "resetting env. episode 2664, reward total was -14.0. running mean: -7.476537690225944, timestamp: 2022-08-20 16:16:33.047051\n",
      "resetting env. episode 2665, reward total was 3.0. running mean: -7.371772313323684, timestamp: 2022-08-20 16:16:54.112719\n",
      "resetting env. episode 2666, reward total was -10.0. running mean: -7.398054590190447, timestamp: 2022-08-20 16:17:10.820063\n",
      "resetting env. episode 2667, reward total was -3.0. running mean: -7.354074044288542, timestamp: 2022-08-20 16:17:33.758745\n",
      "resetting env. episode 2668, reward total was -5.0. running mean: -7.330533303845657, timestamp: 2022-08-20 16:17:55.131620\n",
      "resetting env. episode 2669, reward total was -12.0. running mean: -7.3772279708072, timestamp: 2022-08-20 16:18:14.400112\n",
      "resetting env. episode 2670, reward total was -13.0. running mean: -7.4334556910991285, timestamp: 2022-08-20 16:18:28.662396\n",
      "resetting env. episode 2671, reward total was -13.0. running mean: -7.489121134188137, timestamp: 2022-08-20 16:18:47.595382\n",
      "resetting env. episode 2672, reward total was -11.0. running mean: -7.5242299228462555, timestamp: 2022-08-20 16:19:05.437799\n",
      "resetting env. episode 2673, reward total was -6.0. running mean: -7.508987623617792, timestamp: 2022-08-20 16:19:24.527764\n",
      "resetting env. episode 2674, reward total was -11.0. running mean: -7.543897747381615, timestamp: 2022-08-20 16:19:45.946501\n",
      "resetting env. episode 2675, reward total was 8.0. running mean: -7.388458769907799, timestamp: 2022-08-20 16:20:03.615273\n",
      "resetting env. episode 2676, reward total was -1.0. running mean: -7.32457418220872, timestamp: 2022-08-20 16:20:35.328503\n",
      "resetting env. episode 2677, reward total was -14.0. running mean: -7.391328440386633, timestamp: 2022-08-20 16:20:51.101345\n",
      "resetting env. episode 2678, reward total was -13.0. running mean: -7.447415155982766, timestamp: 2022-08-20 16:21:08.116861\n",
      "resetting env. episode 2679, reward total was -12.0. running mean: -7.4929410044229385, timestamp: 2022-08-20 16:21:25.742751\n",
      "resetting env. episode 2680, reward total was -12.0. running mean: -7.538011594378709, timestamp: 2022-08-20 16:21:47.718008\n",
      "resetting env. episode 2681, reward total was -19.0. running mean: -7.652631478434922, timestamp: 2022-08-20 16:22:02.538390\n",
      "resetting env. episode 2682, reward total was -6.0. running mean: -7.636105163650573, timestamp: 2022-08-20 16:22:26.122353\n",
      "resetting env. episode 2683, reward total was -4.0. running mean: -7.599744112014067, timestamp: 2022-08-20 16:22:49.059044\n",
      "resetting env. episode 2684, reward total was -3.0. running mean: -7.5537466708939265, timestamp: 2022-08-20 16:23:13.576519\n",
      "resetting env. episode 2685, reward total was 11.0. running mean: -7.3682092041849865, timestamp: 2022-08-20 16:23:32.362319\n",
      "resetting env. episode 2686, reward total was 3.0. running mean: -7.264527112143137, timestamp: 2022-08-20 16:23:57.514060\n",
      "resetting env. episode 2687, reward total was 3.0. running mean: -7.161881841021705, timestamp: 2022-08-20 16:24:19.970253\n",
      "resetting env. episode 2688, reward total was -3.0. running mean: -7.120263022611488, timestamp: 2022-08-20 16:24:44.020957\n",
      "resetting env. episode 2689, reward total was -3.0. running mean: -7.079060392385373, timestamp: 2022-08-20 16:25:08.174400\n",
      "resetting env. episode 2690, reward total was -7.0. running mean: -7.07826978846152, timestamp: 2022-08-20 16:25:27.597489\n",
      "resetting env. episode 2691, reward total was -1.0. running mean: -7.017487090576904, timestamp: 2022-08-20 16:25:49.458058\n",
      "resetting env. episode 2692, reward total was 7.0. running mean: -6.8773122196711345, timestamp: 2022-08-20 16:26:08.341581\n",
      "resetting env. episode 2693, reward total was -13.0. running mean: -6.938539097474423, timestamp: 2022-08-20 16:26:28.384027\n",
      "resetting env. episode 2694, reward total was -10.0. running mean: -6.969153706499678, timestamp: 2022-08-20 16:26:43.219339\n",
      "resetting env. episode 2695, reward total was -3.0. running mean: -6.929462169434681, timestamp: 2022-08-20 16:27:06.487141\n",
      "resetting env. episode 2696, reward total was -10.0. running mean: -6.960167547740334, timestamp: 2022-08-20 16:27:28.361668\n",
      "resetting env. episode 2697, reward total was -10.0. running mean: -6.9905658722629305, timestamp: 2022-08-20 16:27:45.225592\n",
      "resetting env. episode 2698, reward total was -3.0. running mean: -6.950660213540301, timestamp: 2022-08-20 16:28:12.487720\n",
      "resetting env. episode 2699, reward total was -11.0. running mean: -6.991153611404898, timestamp: 2022-08-20 16:28:32.740588\n",
      "resetting env. episode 2700, reward total was -9.0. running mean: -7.011242075290848, timestamp: 2022-08-20 16:28:54.875425\n",
      "resetting env. episode 2701, reward total was -1.0. running mean: -6.951129654537939, timestamp: 2022-08-20 16:29:16.075747\n",
      "resetting env. episode 2702, reward total was -6.0. running mean: -6.941618357992559, timestamp: 2022-08-20 16:29:35.133805\n",
      "resetting env. episode 2703, reward total was -11.0. running mean: -6.982202174412634, timestamp: 2022-08-20 16:29:53.836827\n",
      "resetting env. episode 2704, reward total was -6.0. running mean: -6.9723801526685065, timestamp: 2022-08-20 16:30:14.468666\n",
      "resetting env. episode 2705, reward total was -4.0. running mean: -6.942656351141822, timestamp: 2022-08-20 16:30:34.191949\n",
      "resetting env. episode 2706, reward total was -7.0. running mean: -6.943229787630404, timestamp: 2022-08-20 16:30:55.581769\n",
      "resetting env. episode 2707, reward total was 8.0. running mean: -6.7937974897541, timestamp: 2022-08-20 16:31:16.674389\n",
      "resetting env. episode 2708, reward total was -13.0. running mean: -6.855859514856559, timestamp: 2022-08-20 16:31:30.008752\n",
      "resetting env. episode 2709, reward total was -8.0. running mean: -6.8673009197079935, timestamp: 2022-08-20 16:31:53.153882\n",
      "resetting env. episode 2710, reward total was -3.0. running mean: -6.828627910510914, timestamp: 2022-08-20 16:32:13.188329\n",
      "resetting env. episode 2711, reward total was -11.0. running mean: -6.870341631405805, timestamp: 2022-08-20 16:32:34.073509\n",
      "resetting env. episode 2712, reward total was 1.0. running mean: -6.791638215091747, timestamp: 2022-08-20 16:32:58.256860\n",
      "resetting env. episode 2713, reward total was -6.0. running mean: -6.783721832940829, timestamp: 2022-08-20 16:33:14.619135\n",
      "resetting env. episode 2714, reward total was -12.0. running mean: -6.8358846146114205, timestamp: 2022-08-20 16:33:31.702459\n",
      "resetting env. episode 2715, reward total was -9.0. running mean: -6.857525768465306, timestamp: 2022-08-20 16:33:51.485587\n",
      "resetting env. episode 2716, reward total was -17.0. running mean: -6.958950510780653, timestamp: 2022-08-20 16:34:03.918350\n",
      "resetting env. episode 2717, reward total was -4.0. running mean: -6.929361005672846, timestamp: 2022-08-20 16:34:28.309665\n",
      "resetting env. episode 2718, reward total was -15.0. running mean: -7.010067395616118, timestamp: 2022-08-20 16:34:44.928757\n",
      "resetting env. episode 2719, reward total was -13.0. running mean: -7.069966721659957, timestamp: 2022-08-20 16:35:03.802311\n",
      "resetting env. episode 2720, reward total was -3.0. running mean: -7.029267054443357, timestamp: 2022-08-20 16:35:25.077468\n",
      "resetting env. episode 2721, reward total was -9.0. running mean: -7.048974383898924, timestamp: 2022-08-20 16:35:49.280757\n",
      "resetting env. episode 2722, reward total was -16.0. running mean: -7.138484640059935, timestamp: 2022-08-20 16:36:03.824870\n",
      "resetting env. episode 2723, reward total was -3.0. running mean: -7.097099793659336, timestamp: 2022-08-20 16:36:30.695057\n",
      "resetting env. episode 2724, reward total was -13.0. running mean: -7.156128795722742, timestamp: 2022-08-20 16:36:50.048312\n",
      "resetting env. episode 2725, reward total was -15.0. running mean: -7.234567507765514, timestamp: 2022-08-20 16:37:04.696159\n",
      "resetting env. episode 2726, reward total was -1.0. running mean: -7.172221832687859, timestamp: 2022-08-20 16:37:31.558371\n",
      "resetting env. episode 2727, reward total was 9.0. running mean: -7.01049961436098, timestamp: 2022-08-20 16:37:47.989624\n",
      "resetting env. episode 2728, reward total was -2.0. running mean: -6.96039461821737, timestamp: 2022-08-20 16:38:11.800389\n",
      "resetting env. episode 2729, reward total was 3.0. running mean: -6.860790672035196, timestamp: 2022-08-20 16:38:34.933665\n",
      "resetting env. episode 2730, reward total was -6.0. running mean: -6.8521827653148435, timestamp: 2022-08-20 16:38:55.287256\n",
      "resetting env. episode 2731, reward total was -7.0. running mean: -6.8536609376616955, timestamp: 2022-08-20 16:39:13.407822\n",
      "resetting env. episode 2732, reward total was -9.0. running mean: -6.875124328285079, timestamp: 2022-08-20 16:39:33.743462\n",
      "resetting env. episode 2733, reward total was -7.0. running mean: -6.876373085002228, timestamp: 2022-08-20 16:39:56.263267\n",
      "resetting env. episode 2734, reward total was -7.0. running mean: -6.877609354152207, timestamp: 2022-08-20 16:40:18.669381\n",
      "resetting env. episode 2735, reward total was -4.0. running mean: -6.848833260610685, timestamp: 2022-08-20 16:40:46.844064\n",
      "resetting env. episode 2736, reward total was 1.0. running mean: -6.770344928004579, timestamp: 2022-08-20 16:41:06.264152\n",
      "resetting env. episode 2737, reward total was -15.0. running mean: -6.852641478724533, timestamp: 2022-08-20 16:41:24.519359\n",
      "resetting env. episode 2738, reward total was -7.0. running mean: -6.854115063937288, timestamp: 2022-08-20 16:41:37.502663\n",
      "resetting env. episode 2739, reward total was -11.0. running mean: -6.895573913297915, timestamp: 2022-08-20 16:42:01.784752\n",
      "resetting env. episode 2740, reward total was -12.0. running mean: -6.946618174164936, timestamp: 2022-08-20 16:42:21.776393\n",
      "resetting env. episode 2741, reward total was -15.0. running mean: -7.027151992423287, timestamp: 2022-08-20 16:42:37.890329\n",
      "resetting env. episode 2742, reward total was -15.0. running mean: -7.106880472499054, timestamp: 2022-08-20 16:42:55.600982\n",
      "resetting env. episode 2743, reward total was -9.0. running mean: -7.125811667774063, timestamp: 2022-08-20 16:43:14.395743\n",
      "resetting env. episode 2744, reward total was -3.0. running mean: -7.084553551096323, timestamp: 2022-08-20 16:43:40.131965\n",
      "resetting env. episode 2745, reward total was -2.0. running mean: -7.0337080155853595, timestamp: 2022-08-20 16:44:02.179023\n",
      "resetting env. episode 2746, reward total was -5.0. running mean: -7.0133709354295055, timestamp: 2022-08-20 16:44:25.937509\n",
      "resetting env. episode 2747, reward total was -7.0. running mean: -7.01323722607521, timestamp: 2022-08-20 16:44:48.872208\n",
      "resetting env. episode 2748, reward total was -3.0. running mean: -6.973104853814458, timestamp: 2022-08-20 16:45:11.095800\n",
      "resetting env. episode 2749, reward total was -13.0. running mean: -7.033373805276313, timestamp: 2022-08-20 16:45:25.958081\n",
      "resetting env. episode 2750, reward total was -3.0. running mean: -6.99304006722355, timestamp: 2022-08-20 16:45:50.376804\n",
      "resetting env. episode 2751, reward total was -10.0. running mean: -7.023109666551314, timestamp: 2022-08-20 16:46:09.806267\n",
      "resetting env. episode 2752, reward total was -17.0. running mean: -7.122878569885801, timestamp: 2022-08-20 16:46:24.314490\n",
      "resetting env. episode 2753, reward total was -14.0. running mean: -7.191649784186942, timestamp: 2022-08-20 16:46:40.313724\n",
      "resetting env. episode 2754, reward total was -7.0. running mean: -7.1897332863450725, timestamp: 2022-08-20 16:47:04.425276\n",
      "resetting env. episode 2755, reward total was 13.0. running mean: -6.987835953481622, timestamp: 2022-08-20 16:47:21.316127\n",
      "resetting env. episode 2756, reward total was -5.0. running mean: -6.967957593946806, timestamp: 2022-08-20 16:47:45.309003\n",
      "resetting env. episode 2757, reward total was -13.0. running mean: -7.028278018007337, timestamp: 2022-08-20 16:47:59.485106\n",
      "resetting env. episode 2758, reward total was -14.0. running mean: -7.097995237827264, timestamp: 2022-08-20 16:48:18.313767\n",
      "resetting env. episode 2759, reward total was -17.0. running mean: -7.197015285448991, timestamp: 2022-08-20 16:48:30.629857\n",
      "resetting env. episode 2760, reward total was -7.0. running mean: -7.195045132594501, timestamp: 2022-08-20 16:48:55.936209\n",
      "resetting env. episode 2761, reward total was -11.0. running mean: -7.233094681268557, timestamp: 2022-08-20 16:49:16.261905\n",
      "resetting env. episode 2762, reward total was -4.0. running mean: -7.200763734455871, timestamp: 2022-08-20 16:49:38.637081\n",
      "resetting env. episode 2763, reward total was -6.0. running mean: -7.188756097111312, timestamp: 2022-08-20 16:50:05.640885\n",
      "resetting env. episode 2764, reward total was 7.0. running mean: -7.046868536140198, timestamp: 2022-08-20 16:50:25.349211\n",
      "resetting env. episode 2765, reward total was -11.0. running mean: -7.086399850778796, timestamp: 2022-08-20 16:50:44.284589\n",
      "resetting env. episode 2766, reward total was -5.0. running mean: -7.065535852271008, timestamp: 2022-08-20 16:51:07.380856\n",
      "resetting env. episode 2767, reward total was -7.0. running mean: -7.064880493748298, timestamp: 2022-08-20 16:51:28.538297\n",
      "resetting env. episode 2768, reward total was -7.0. running mean: -7.064231688810815, timestamp: 2022-08-20 16:51:48.520882\n",
      "resetting env. episode 2769, reward total was 2.0. running mean: -6.973589371922707, timestamp: 2022-08-20 16:52:11.894407\n",
      "resetting env. episode 2770, reward total was -5.0. running mean: -6.95385347820348, timestamp: 2022-08-20 16:52:35.152281\n",
      "resetting env. episode 2771, reward total was -11.0. running mean: -6.994314943421445, timestamp: 2022-08-20 16:52:54.578339\n",
      "resetting env. episode 2772, reward total was -9.0. running mean: -7.01437179398723, timestamp: 2022-08-20 16:53:13.664826\n",
      "resetting env. episode 2773, reward total was -3.0. running mean: -6.974228076047358, timestamp: 2022-08-20 16:53:39.369116\n",
      "resetting env. episode 2774, reward total was -7.0. running mean: -6.974485795286885, timestamp: 2022-08-20 16:54:02.276897\n",
      "resetting env. episode 2775, reward total was 2.0. running mean: -6.8847409373340165, timestamp: 2022-08-20 16:54:26.737510\n",
      "resetting env. episode 2776, reward total was -6.0. running mean: -6.875893527960676, timestamp: 2022-08-20 16:54:47.361381\n",
      "resetting env. episode 2777, reward total was -10.0. running mean: -6.907134592681069, timestamp: 2022-08-20 16:55:11.438016\n",
      "resetting env. episode 2778, reward total was -6.0. running mean: -6.8980632467542575, timestamp: 2022-08-20 16:55:33.033304\n",
      "resetting env. episode 2779, reward total was -3.0. running mean: -6.859082614286715, timestamp: 2022-08-20 16:55:59.585320\n",
      "resetting env. episode 2780, reward total was -9.0. running mean: -6.880491788143847, timestamp: 2022-08-20 16:56:18.918655\n",
      "resetting env. episode 2781, reward total was -5.0. running mean: -6.8616868702624085, timestamp: 2022-08-20 16:56:44.917145\n",
      "resetting env. episode 2782, reward total was -3.0. running mean: -6.823070001559785, timestamp: 2022-08-20 16:57:09.619116\n",
      "resetting env. episode 2783, reward total was -7.0. running mean: -6.824839301544187, timestamp: 2022-08-20 16:57:29.361353\n",
      "resetting env. episode 2784, reward total was -11.0. running mean: -6.866590908528745, timestamp: 2022-08-20 16:57:49.272123\n",
      "resetting env. episode 2785, reward total was -11.0. running mean: -6.907924999443458, timestamp: 2022-08-20 16:58:10.022659\n",
      "resetting env. episode 2786, reward total was -6.0. running mean: -6.898845749449023, timestamp: 2022-08-20 16:58:30.790146\n",
      "resetting env. episode 2787, reward total was -12.0. running mean: -6.949857291954533, timestamp: 2022-08-20 16:58:53.051647\n",
      "resetting env. episode 2788, reward total was -5.0. running mean: -6.930358719034988, timestamp: 2022-08-20 16:59:15.691125\n",
      "resetting env. episode 2789, reward total was -11.0. running mean: -6.971055131844638, timestamp: 2022-08-20 16:59:37.577627\n",
      "resetting env. episode 2790, reward total was -14.0. running mean: -7.041344580526191, timestamp: 2022-08-20 16:59:56.476108\n",
      "resetting env. episode 2791, reward total was -16.0. running mean: -7.130931134720929, timestamp: 2022-08-20 17:00:12.461387\n",
      "resetting env. episode 2792, reward total was -3.0. running mean: -7.08962182337372, timestamp: 2022-08-20 17:00:36.316643\n",
      "resetting env. episode 2793, reward total was -2.0. running mean: -7.038725605139982, timestamp: 2022-08-20 17:00:59.693691\n",
      "resetting env. episode 2794, reward total was -3.0. running mean: -6.998338349088582, timestamp: 2022-08-20 17:01:25.617402\n",
      "resetting env. episode 2795, reward total was 7.0. running mean: -6.858354965597696, timestamp: 2022-08-20 17:01:47.141860\n",
      "resetting env. episode 2796, reward total was -3.0. running mean: -6.8197714159417195, timestamp: 2022-08-20 17:02:11.573556\n",
      "resetting env. episode 2797, reward total was -6.0. running mean: -6.811573701782302, timestamp: 2022-08-20 17:02:35.460734\n",
      "resetting env. episode 2798, reward total was -6.0. running mean: -6.803457964764478, timestamp: 2022-08-20 17:03:04.212849\n",
      "resetting env. episode 2799, reward total was -15.0. running mean: -6.885423385116834, timestamp: 2022-08-20 17:03:21.661219\n",
      "resetting env. episode 2800, reward total was 4.0. running mean: -6.776569151265665, timestamp: 2022-08-20 17:03:40.100922\n",
      "resetting env. episode 2801, reward total was 1.0. running mean: -6.698803459753009, timestamp: 2022-08-20 17:04:09.862371\n",
      "resetting env. episode 2802, reward total was -8.0. running mean: -6.7118154251554785, timestamp: 2022-08-20 17:04:32.431043\n",
      "resetting env. episode 2803, reward total was -14.0. running mean: -6.784697270903924, timestamp: 2022-08-20 17:04:52.606125\n",
      "resetting env. episode 2804, reward total was -11.0. running mean: -6.826850298194885, timestamp: 2022-08-20 17:05:17.862869\n",
      "resetting env. episode 2805, reward total was -1.0. running mean: -6.768581795212936, timestamp: 2022-08-20 17:05:40.707797\n",
      "resetting env. episode 2806, reward total was -11.0. running mean: -6.810895977260807, timestamp: 2022-08-20 17:05:53.796807\n",
      "resetting env. episode 2807, reward total was -1.0. running mean: -6.752787017488198, timestamp: 2022-08-20 17:06:19.649706\n",
      "resetting env. episode 2808, reward total was -10.0. running mean: -6.785259147313316, timestamp: 2022-08-20 17:06:38.419531\n",
      "resetting env. episode 2809, reward total was -13.0. running mean: -6.847406555840183, timestamp: 2022-08-20 17:06:56.911112\n",
      "resetting env. episode 2810, reward total was -12.0. running mean: -6.898932490281782, timestamp: 2022-08-20 17:07:13.623441\n",
      "resetting env. episode 2811, reward total was -15.0. running mean: -6.979943165378964, timestamp: 2022-08-20 17:07:30.188159\n",
      "resetting env. episode 2812, reward total was 5.0. running mean: -6.860143733725175, timestamp: 2022-08-20 17:07:51.120231\n",
      "resetting env. episode 2813, reward total was -3.0. running mean: -6.821542296387923, timestamp: 2022-08-20 17:08:17.975422\n",
      "resetting env. episode 2814, reward total was -5.0. running mean: -6.8033268734240435, timestamp: 2022-08-20 17:08:41.902462\n",
      "resetting env. episode 2815, reward total was -8.0. running mean: -6.815293604689803, timestamp: 2022-08-20 17:09:02.642026\n",
      "resetting env. episode 2816, reward total was -9.0. running mean: -6.837140668642904, timestamp: 2022-08-20 17:09:21.629785\n",
      "resetting env. episode 2817, reward total was -9.0. running mean: -6.858769261956475, timestamp: 2022-08-20 17:09:41.297752\n",
      "resetting env. episode 2818, reward total was -9.0. running mean: -6.8801815693369095, timestamp: 2022-08-20 17:10:04.431911\n",
      "resetting env. episode 2819, reward total was -7.0. running mean: -6.88137975364354, timestamp: 2022-08-20 17:10:30.173103\n",
      "resetting env. episode 2820, reward total was -5.0. running mean: -6.862565956107105, timestamp: 2022-08-20 17:10:59.331173\n",
      "resetting env. episode 2821, reward total was -6.0. running mean: -6.853940296546033, timestamp: 2022-08-20 17:11:20.339009\n",
      "resetting env. episode 2822, reward total was -12.0. running mean: -6.905400893580572, timestamp: 2022-08-20 17:11:35.262121\n",
      "resetting env. episode 2823, reward total was -5.0. running mean: -6.886346884644767, timestamp: 2022-08-20 17:12:03.481689\n",
      "resetting env. episode 2824, reward total was -11.0. running mean: -6.92748341579832, timestamp: 2022-08-20 17:12:20.480250\n",
      "resetting env. episode 2825, reward total was -1.0. running mean: -6.868208581640336, timestamp: 2022-08-20 17:12:47.506012\n",
      "resetting env. episode 2826, reward total was -1.0. running mean: -6.809526495823932, timestamp: 2022-08-20 17:13:10.379435\n",
      "resetting env. episode 2827, reward total was -17.0. running mean: -6.911431230865692, timestamp: 2022-08-20 17:13:20.895326\n",
      "resetting env. episode 2828, reward total was -9.0. running mean: -6.932316918557035, timestamp: 2022-08-20 17:13:35.267911\n",
      "resetting env. episode 2829, reward total was -10.0. running mean: -6.962993749371464, timestamp: 2022-08-20 17:13:45.880540\n",
      "resetting env. episode 2830, reward total was -13.0. running mean: -7.02336381187775, timestamp: 2022-08-20 17:13:58.090905\n",
      "resetting env. episode 2831, reward total was -11.0. running mean: -7.063130173758973, timestamp: 2022-08-20 17:14:10.379057\n",
      "resetting env. episode 2832, reward total was 1.0. running mean: -6.982498872021383, timestamp: 2022-08-20 17:14:26.542850\n",
      "resetting env. episode 2833, reward total was -9.0. running mean: -7.002673883301169, timestamp: 2022-08-20 17:14:38.822032\n",
      "resetting env. episode 2834, reward total was -7.0. running mean: -7.002647144468157, timestamp: 2022-08-20 17:14:53.268415\n",
      "resetting env. episode 2835, reward total was -11.0. running mean: -7.0426206730234755, timestamp: 2022-08-20 17:15:07.077501\n",
      "resetting env. episode 2836, reward total was -6.0. running mean: -7.03219446629324, timestamp: 2022-08-20 17:15:21.632597\n",
      "resetting env. episode 2837, reward total was -3.0. running mean: -6.991872521630308, timestamp: 2022-08-20 17:15:36.920732\n",
      "resetting env. episode 2838, reward total was -11.0. running mean: -7.031953796414005, timestamp: 2022-08-20 17:15:46.639751\n",
      "resetting env. episode 2839, reward total was -9.0. running mean: -7.051634258449864, timestamp: 2022-08-20 17:15:59.001707\n",
      "resetting env. episode 2840, reward total was -7.0. running mean: -7.051117915865365, timestamp: 2022-08-20 17:16:17.879803\n",
      "resetting env. episode 2841, reward total was -12.0. running mean: -7.100606736706712, timestamp: 2022-08-20 17:16:35.445108\n",
      "resetting env. episode 2842, reward total was -11.0. running mean: -7.139600669339645, timestamp: 2022-08-20 17:16:51.298728\n",
      "resetting env. episode 2843, reward total was 2.0. running mean: -7.048204662646249, timestamp: 2022-08-20 17:17:09.860116\n",
      "resetting env. episode 2844, reward total was -7.0. running mean: -7.047722616019787, timestamp: 2022-08-20 17:17:28.542177\n",
      "resetting env. episode 2845, reward total was 1.0. running mean: -6.967245389859589, timestamp: 2022-08-20 17:17:48.399106\n",
      "resetting env. episode 2846, reward total was -11.0. running mean: -7.007572935960994, timestamp: 2022-08-20 17:18:01.949883\n",
      "resetting env. episode 2847, reward total was -9.0. running mean: -7.027497206601383, timestamp: 2022-08-20 17:18:14.386645\n",
      "resetting env. episode 2848, reward total was 6.0. running mean: -6.89722223453537, timestamp: 2022-08-20 17:18:29.995939\n",
      "resetting env. episode 2849, reward total was 1.0. running mean: -6.818250012190016, timestamp: 2022-08-20 17:18:54.858460\n",
      "resetting env. episode 2850, reward total was -3.0. running mean: -6.780067512068116, timestamp: 2022-08-20 17:19:11.090069\n",
      "resetting env. episode 2851, reward total was -13.0. running mean: -6.842266836947434, timestamp: 2022-08-20 17:19:27.131189\n",
      "resetting env. episode 2852, reward total was 6.0. running mean: -6.71384416857796, timestamp: 2022-08-20 17:19:45.848164\n",
      "resetting env. episode 2853, reward total was -9.0. running mean: -6.736705726892181, timestamp: 2022-08-20 17:20:01.164234\n",
      "resetting env. episode 2854, reward total was -6.0. running mean: -6.729338669623258, timestamp: 2022-08-20 17:20:15.795112\n",
      "resetting env. episode 2855, reward total was -9.0. running mean: -6.752045282927026, timestamp: 2022-08-20 17:20:33.175652\n",
      "resetting env. episode 2856, reward total was 5.0. running mean: -6.634524830097756, timestamp: 2022-08-20 17:20:50.924212\n",
      "resetting env. episode 2857, reward total was -6.0. running mean: -6.628179581796778, timestamp: 2022-08-20 17:21:12.168426\n",
      "resetting env. episode 2858, reward total was -15.0. running mean: -6.7118977859788105, timestamp: 2022-08-20 17:21:25.080909\n",
      "resetting env. episode 2859, reward total was -17.0. running mean: -6.814778808119022, timestamp: 2022-08-20 17:21:33.880387\n",
      "resetting env. episode 2860, reward total was -6.0. running mean: -6.806631020037831, timestamp: 2022-08-20 17:21:50.381291\n",
      "resetting env. episode 2861, reward total was -14.0. running mean: -6.878564709837453, timestamp: 2022-08-20 17:22:06.290755\n",
      "resetting env. episode 2862, reward total was -13.0. running mean: -6.939779062739078, timestamp: 2022-08-20 17:22:18.311658\n",
      "resetting env. episode 2863, reward total was -3.0. running mean: -6.900381272111687, timestamp: 2022-08-20 17:22:37.146278\n",
      "resetting env. episode 2864, reward total was -12.0. running mean: -6.951377459390571, timestamp: 2022-08-20 17:22:51.245598\n",
      "resetting env. episode 2865, reward total was -13.0. running mean: -7.011863684796665, timestamp: 2022-08-20 17:23:05.478548\n",
      "resetting env. episode 2866, reward total was -6.0. running mean: -7.001745047948697, timestamp: 2022-08-20 17:23:23.190204\n",
      "resetting env. episode 2867, reward total was -16.0. running mean: -7.091727597469211, timestamp: 2022-08-20 17:23:39.089706\n",
      "resetting env. episode 2868, reward total was -10.0. running mean: -7.120810321494518, timestamp: 2022-08-20 17:23:54.321987\n",
      "resetting env. episode 2869, reward total was -4.0. running mean: -7.089602218279572, timestamp: 2022-08-20 17:24:14.060255\n",
      "resetting env. episode 2870, reward total was -13.0. running mean: -7.148706196096777, timestamp: 2022-08-20 17:24:28.164526\n",
      "resetting env. episode 2871, reward total was -15.0. running mean: -7.227219134135809, timestamp: 2022-08-20 17:24:42.378532\n",
      "resetting env. episode 2872, reward total was -4.0. running mean: -7.194946942794451, timestamp: 2022-08-20 17:24:59.316257\n",
      "resetting env. episode 2873, reward total was -3.0. running mean: -7.152997473366507, timestamp: 2022-08-20 17:25:14.718092\n",
      "resetting env. episode 2874, reward total was -6.0. running mean: -7.141467498632841, timestamp: 2022-08-20 17:25:32.879543\n",
      "resetting env. episode 2875, reward total was -10.0. running mean: -7.170052823646512, timestamp: 2022-08-20 17:25:49.675651\n",
      "resetting env. episode 2876, reward total was -11.0. running mean: -7.208352295410047, timestamp: 2022-08-20 17:26:03.931541\n",
      "resetting env. episode 2877, reward total was -15.0. running mean: -7.286268772455947, timestamp: 2022-08-20 17:26:16.220695\n",
      "resetting env. episode 2878, reward total was -15.0. running mean: -7.363406084731388, timestamp: 2022-08-20 17:26:30.932378\n",
      "resetting env. episode 2879, reward total was -9.0. running mean: -7.379772023884073, timestamp: 2022-08-20 17:26:48.149347\n",
      "resetting env. episode 2880, reward total was -5.0. running mean: -7.355974303645232, timestamp: 2022-08-20 17:27:03.531231\n",
      "resetting env. episode 2881, reward total was -6.0. running mean: -7.34241456060878, timestamp: 2022-08-20 17:27:19.115573\n",
      "resetting env. episode 2882, reward total was -4.0. running mean: -7.308990415002691, timestamp: 2022-08-20 17:27:36.529045\n",
      "resetting env. episode 2883, reward total was -12.0. running mean: -7.355900510852664, timestamp: 2022-08-20 17:27:56.605363\n",
      "resetting env. episode 2884, reward total was 1.0. running mean: -7.272341505744138, timestamp: 2022-08-20 17:28:16.241878\n",
      "resetting env. episode 2885, reward total was -1.0. running mean: -7.209618090686696, timestamp: 2022-08-20 17:28:35.002728\n",
      "resetting env. episode 2886, reward total was -11.0. running mean: -7.247521909779829, timestamp: 2022-08-20 17:28:51.695108\n",
      "resetting env. episode 2887, reward total was -4.0. running mean: -7.215046690682031, timestamp: 2022-08-20 17:29:10.288417\n",
      "resetting env. episode 2888, reward total was 1.0. running mean: -7.1328962237752105, timestamp: 2022-08-20 17:29:28.444880\n",
      "resetting env. episode 2889, reward total was -5.0. running mean: -7.111567261537458, timestamp: 2022-08-20 17:29:45.465380\n",
      "resetting env. episode 2890, reward total was -9.0. running mean: -7.130451588922083, timestamp: 2022-08-20 17:30:03.079298\n",
      "resetting env. episode 2891, reward total was -12.0. running mean: -7.179147073032863, timestamp: 2022-08-20 17:30:18.915966\n",
      "resetting env. episode 2892, reward total was -6.0. running mean: -7.167355602302534, timestamp: 2022-08-20 17:30:32.714089\n",
      "resetting env. episode 2893, reward total was -1.0. running mean: -7.105682046279508, timestamp: 2022-08-20 17:30:48.467007\n",
      "resetting env. episode 2894, reward total was -7.0. running mean: -7.104625225816713, timestamp: 2022-08-20 17:31:03.004120\n",
      "resetting env. episode 2895, reward total was 7.0. running mean: -6.963578973558546, timestamp: 2022-08-20 17:31:19.271637\n",
      "resetting env. episode 2896, reward total was -12.0. running mean: -7.013943183822961, timestamp: 2022-08-20 17:31:33.265236\n",
      "resetting env. episode 2897, reward total was -6.0. running mean: -7.003803751984731, timestamp: 2022-08-20 17:31:51.087593\n",
      "resetting env. episode 2898, reward total was -13.0. running mean: -7.063765714464884, timestamp: 2022-08-20 17:32:06.445544\n",
      "resetting env. episode 2899, reward total was -9.0. running mean: -7.083128057320235, timestamp: 2022-08-20 17:32:21.943115\n",
      "resetting env. episode 2900, reward total was -2.0. running mean: -7.032296776747033, timestamp: 2022-08-20 17:32:39.871194\n",
      "resetting env. episode 2901, reward total was -9.0. running mean: -7.051973808979562, timestamp: 2022-08-20 17:32:54.338573\n",
      "resetting env. episode 2902, reward total was -11.0. running mean: -7.091454070889767, timestamp: 2022-08-20 17:33:07.723757\n",
      "resetting env. episode 2903, reward total was -11.0. running mean: -7.13053953018087, timestamp: 2022-08-20 17:33:22.817397\n",
      "resetting env. episode 2904, reward total was 3.0. running mean: -7.029234134879061, timestamp: 2022-08-20 17:33:38.926338\n",
      "resetting env. episode 2905, reward total was -7.0. running mean: -7.0289417935302705, timestamp: 2022-08-20 17:33:53.514363\n",
      "resetting env. episode 2906, reward total was -1.0. running mean: -6.968652375594967, timestamp: 2022-08-20 17:34:12.775860\n",
      "resetting env. episode 2907, reward total was -11.0. running mean: -7.008965851839018, timestamp: 2022-08-20 17:34:29.106236\n",
      "resetting env. episode 2908, reward total was -12.0. running mean: -7.058876193320628, timestamp: 2022-08-20 17:34:44.822200\n",
      "resetting env. episode 2909, reward total was 2.0. running mean: -6.968287431387422, timestamp: 2022-08-20 17:35:00.387592\n",
      "resetting env. episode 2910, reward total was -5.0. running mean: -6.948604557073548, timestamp: 2022-08-20 17:35:17.320353\n",
      "resetting env. episode 2911, reward total was -11.0. running mean: -6.989118511502813, timestamp: 2022-08-20 17:35:31.458539\n",
      "resetting env. episode 2912, reward total was -5.0. running mean: -6.969227326387784, timestamp: 2022-08-20 17:35:48.968734\n",
      "resetting env. episode 2913, reward total was -9.0. running mean: -6.989535053123906, timestamp: 2022-08-20 17:36:07.937259\n",
      "resetting env. episode 2914, reward total was -10.0. running mean: -7.0196397025926665, timestamp: 2022-08-20 17:36:23.807834\n",
      "resetting env. episode 2915, reward total was -10.0. running mean: -7.049443305566739, timestamp: 2022-08-20 17:36:41.065695\n",
      "resetting env. episode 2916, reward total was 2.0. running mean: -6.958948872511072, timestamp: 2022-08-20 17:36:57.710204\n",
      "resetting env. episode 2917, reward total was -3.0. running mean: -6.919359383785961, timestamp: 2022-08-20 17:37:11.149281\n",
      "resetting env. episode 2918, reward total was 3.0. running mean: -6.820165789948101, timestamp: 2022-08-20 17:37:39.641652\n",
      "resetting env. episode 2919, reward total was -5.0. running mean: -6.80196413204862, timestamp: 2022-08-20 17:37:58.762539\n",
      "resetting env. episode 2920, reward total was -8.0. running mean: -6.813944490728134, timestamp: 2022-08-20 17:38:13.500153\n",
      "resetting env. episode 2921, reward total was -9.0. running mean: -6.835805045820853, timestamp: 2022-08-20 17:38:29.774651\n",
      "resetting env. episode 2922, reward total was -5.0. running mean: -6.817446995362644, timestamp: 2022-08-20 17:38:47.254921\n",
      "resetting env. episode 2923, reward total was -9.0. running mean: -6.839272525409018, timestamp: 2022-08-20 17:39:06.248165\n",
      "resetting env. episode 2924, reward total was -7.0. running mean: -6.840879800154928, timestamp: 2022-08-20 17:39:22.208489\n",
      "resetting env. episode 2925, reward total was -9.0. running mean: -6.862471002153379, timestamp: 2022-08-20 17:39:38.161850\n",
      "resetting env. episode 2926, reward total was -1.0. running mean: -6.803846292131845, timestamp: 2022-08-20 17:39:57.366510\n",
      "resetting env. episode 2927, reward total was -7.0. running mean: -6.805807829210527, timestamp: 2022-08-20 17:40:14.089807\n",
      "resetting env. episode 2928, reward total was -6.0. running mean: -6.797749750918421, timestamp: 2022-08-20 17:40:27.751295\n",
      "resetting env. episode 2929, reward total was -13.0. running mean: -6.859772253409237, timestamp: 2022-08-20 17:40:41.672108\n",
      "resetting env. episode 2930, reward total was -10.0. running mean: -6.891174530875144, timestamp: 2022-08-20 17:40:54.786029\n",
      "resetting env. episode 2931, reward total was 4.0. running mean: -6.782262785566393, timestamp: 2022-08-20 17:41:12.195494\n",
      "resetting env. episode 2932, reward total was -12.0. running mean: -6.834440157710729, timestamp: 2022-08-20 17:41:27.264212\n",
      "resetting env. episode 2933, reward total was -6.0. running mean: -6.826095756133621, timestamp: 2022-08-20 17:41:43.819574\n",
      "resetting env. episode 2934, reward total was -15.0. running mean: -6.907834798572285, timestamp: 2022-08-20 17:41:59.267284\n",
      "resetting env. episode 2935, reward total was -14.0. running mean: -6.978756450586562, timestamp: 2022-08-20 17:42:10.006575\n",
      "resetting env. episode 2936, reward total was -11.0. running mean: -7.018968886080696, timestamp: 2022-08-20 17:42:25.612858\n",
      "resetting env. episode 2937, reward total was -7.0. running mean: -7.01877919721989, timestamp: 2022-08-20 17:42:42.149683\n",
      "resetting env. episode 2938, reward total was -6.0. running mean: -7.0085914052476905, timestamp: 2022-08-20 17:42:56.407493\n",
      "resetting env. episode 2939, reward total was -13.0. running mean: -7.068505491195213, timestamp: 2022-08-20 17:43:09.847602\n",
      "resetting env. episode 2940, reward total was -1.0. running mean: -7.007820436283261, timestamp: 2022-08-20 17:43:31.160596\n",
      "resetting env. episode 2941, reward total was -3.0. running mean: -6.967742231920428, timestamp: 2022-08-20 17:43:46.830718\n",
      "resetting env. episode 2942, reward total was -15.0. running mean: -7.048064809601224, timestamp: 2022-08-20 17:44:00.389468\n",
      "resetting env. episode 2943, reward total was -13.0. running mean: -7.107584161505212, timestamp: 2022-08-20 17:44:16.732780\n",
      "resetting env. episode 2944, reward total was -7.0. running mean: -7.10650831989016, timestamp: 2022-08-20 17:44:35.069769\n",
      "resetting env. episode 2945, reward total was -8.0. running mean: -7.1154432366912586, timestamp: 2022-08-20 17:44:50.858562\n",
      "resetting env. episode 2946, reward total was -6.0. running mean: -7.104288804324345, timestamp: 2022-08-20 17:45:08.982132\n",
      "resetting env. episode 2947, reward total was -5.0. running mean: -7.0832459162811015, timestamp: 2022-08-20 17:45:28.576743\n",
      "resetting env. episode 2948, reward total was -4.0. running mean: -7.05241345711829, timestamp: 2022-08-20 17:45:51.877462\n",
      "resetting env. episode 2949, reward total was -12.0. running mean: -7.101889322547107, timestamp: 2022-08-20 17:46:07.631348\n",
      "resetting env. episode 2950, reward total was -9.0. running mean: -7.120870429321636, timestamp: 2022-08-20 17:46:23.005255\n",
      "resetting env. episode 2951, reward total was -7.0. running mean: -7.11966172502842, timestamp: 2022-08-20 17:46:38.936680\n",
      "resetting env. episode 2952, reward total was -15.0. running mean: -7.198465107778136, timestamp: 2022-08-20 17:46:50.574561\n",
      "resetting env. episode 2953, reward total was -9.0. running mean: -7.216480456700355, timestamp: 2022-08-20 17:47:04.690829\n",
      "resetting env. episode 2954, reward total was -8.0. running mean: -7.224315652133351, timestamp: 2022-08-20 17:47:21.473967\n",
      "resetting env. episode 2955, reward total was 2.0. running mean: -7.132072495612017, timestamp: 2022-08-20 17:47:37.762548\n",
      "resetting env. episode 2956, reward total was -12.0. running mean: -7.180751770655897, timestamp: 2022-08-20 17:47:52.482195\n",
      "resetting env. episode 2957, reward total was -3.0. running mean: -7.138944252949338, timestamp: 2022-08-20 17:48:11.087450\n",
      "resetting env. episode 2958, reward total was -7.0. running mean: -7.137554810419845, timestamp: 2022-08-20 17:48:32.910114\n",
      "resetting env. episode 2959, reward total was 6.0. running mean: -7.006179262315647, timestamp: 2022-08-20 17:48:48.426641\n",
      "resetting env. episode 2960, reward total was 5.0. running mean: -6.886117469692491, timestamp: 2022-08-20 17:49:06.866352\n",
      "resetting env. episode 2961, reward total was -1.0. running mean: -6.827256294995566, timestamp: 2022-08-20 17:49:26.459026\n",
      "resetting env. episode 2962, reward total was -11.0. running mean: -6.868983732045611, timestamp: 2022-08-20 17:49:41.605496\n",
      "resetting env. episode 2963, reward total was -13.0. running mean: -6.930293894725154, timestamp: 2022-08-20 17:49:55.379680\n",
      "resetting env. episode 2964, reward total was -4.0. running mean: -6.900990955777902, timestamp: 2022-08-20 17:50:17.474616\n",
      "resetting env. episode 2965, reward total was -11.0. running mean: -6.941981046220124, timestamp: 2022-08-20 17:50:30.421117\n",
      "resetting env. episode 2966, reward total was -4.0. running mean: -6.912561235757923, timestamp: 2022-08-20 17:50:49.050306\n",
      "resetting env. episode 2967, reward total was -8.0. running mean: -6.923435623400343, timestamp: 2022-08-20 17:51:02.792579\n",
      "resetting env. episode 2968, reward total was -6.0. running mean: -6.914201267166339, timestamp: 2022-08-20 17:51:21.446709\n",
      "resetting env. episode 2969, reward total was -2.0. running mean: -6.865059254494676, timestamp: 2022-08-20 17:51:39.509488\n",
      "resetting env. episode 2970, reward total was 3.0. running mean: -6.766408661949728, timestamp: 2022-08-20 17:52:03.255964\n",
      "resetting env. episode 2971, reward total was -10.0. running mean: -6.798744575330231, timestamp: 2022-08-20 17:52:18.892160\n",
      "resetting env. episode 2972, reward total was -12.0. running mean: -6.8507571295769285, timestamp: 2022-08-20 17:52:32.870795\n",
      "resetting env. episode 2973, reward total was -3.0. running mean: -6.8122495582811595, timestamp: 2022-08-20 17:52:50.908578\n",
      "resetting env. episode 2974, reward total was -14.0. running mean: -6.884127062698347, timestamp: 2022-08-20 17:53:08.213327\n",
      "resetting env. episode 2975, reward total was -9.0. running mean: -6.905285792071363, timestamp: 2022-08-20 17:53:23.889420\n",
      "resetting env. episode 2976, reward total was -9.0. running mean: -6.92623293415065, timestamp: 2022-08-20 17:53:42.041933\n",
      "resetting env. episode 2977, reward total was -1.0. running mean: -6.8669706048091435, timestamp: 2022-08-20 17:53:57.970323\n",
      "resetting env. episode 2978, reward total was -3.0. running mean: -6.828300898761052, timestamp: 2022-08-20 17:54:18.246125\n",
      "resetting env. episode 2979, reward total was -16.0. running mean: -6.920017889773441, timestamp: 2022-08-20 17:54:29.722475\n",
      "resetting env. episode 2980, reward total was -1.0. running mean: -6.860817710875707, timestamp: 2022-08-20 17:54:52.219315\n",
      "resetting env. episode 2981, reward total was 6.0. running mean: -6.73220953376695, timestamp: 2022-08-20 17:55:10.343867\n",
      "resetting env. episode 2982, reward total was -12.0. running mean: -6.784887438429281, timestamp: 2022-08-20 17:55:27.738372\n",
      "resetting env. episode 2983, reward total was -1.0. running mean: -6.727038564044988, timestamp: 2022-08-20 17:55:43.652834\n",
      "resetting env. episode 2984, reward total was -4.0. running mean: -6.699768178404538, timestamp: 2022-08-20 17:55:59.886442\n",
      "resetting env. episode 2985, reward total was -11.0. running mean: -6.742770496620492, timestamp: 2022-08-20 17:56:11.974129\n",
      "resetting env. episode 2986, reward total was -13.0. running mean: -6.8053427916542875, timestamp: 2022-08-20 17:56:24.243334\n",
      "resetting env. episode 2987, reward total was -15.0. running mean: -6.887289363737745, timestamp: 2022-08-20 17:56:36.047781\n",
      "resetting env. episode 2988, reward total was 10.0. running mean: -6.718416470100368, timestamp: 2022-08-20 17:56:48.930351\n",
      "resetting env. episode 2989, reward total was 5.0. running mean: -6.601232305399364, timestamp: 2022-08-20 17:57:14.803848\n",
      "resetting env. episode 2990, reward total was -2.0. running mean: -6.55521998234537, timestamp: 2022-08-20 17:57:40.017455\n",
      "resetting env. episode 2991, reward total was -6.0. running mean: -6.549667782521915, timestamp: 2022-08-20 17:58:00.853771\n",
      "resetting env. episode 2992, reward total was -5.0. running mean: -6.5341711046966955, timestamp: 2022-08-20 17:58:18.334040\n",
      "resetting env. episode 2993, reward total was -9.0. running mean: -6.558829393649728, timestamp: 2022-08-20 17:58:32.688662\n",
      "resetting env. episode 2994, reward total was -7.0. running mean: -6.563241099713231, timestamp: 2022-08-20 17:58:49.801919\n",
      "resetting env. episode 2995, reward total was -9.0. running mean: -6.587608688716099, timestamp: 2022-08-20 17:59:05.481012\n",
      "resetting env. episode 2996, reward total was -17.0. running mean: -6.691732601828938, timestamp: 2022-08-20 17:59:22.337948\n",
      "resetting env. episode 2997, reward total was 5.0. running mean: -6.574815275810648, timestamp: 2022-08-20 17:59:40.286999\n",
      "resetting env. episode 2998, reward total was -8.0. running mean: -6.5890671230525415, timestamp: 2022-08-20 17:59:57.016421\n",
      "resetting env. episode 2999, reward total was -15.0. running mean: -6.673176451822016, timestamp: 2022-08-20 18:00:11.952494\n",
      "resetting env. episode 3000, reward total was -9.0. running mean: -6.696444687303796, timestamp: 2022-08-20 18:00:30.909821\n",
      "resetting env. episode 3001, reward total was -17.0. running mean: -6.799480240430758, timestamp: 2022-08-20 18:00:42.182688\n",
      "resetting env. episode 3002, reward total was 1.0. running mean: -6.72148543802645, timestamp: 2022-08-20 18:01:02.658962\n",
      "resetting env. episode 3003, reward total was -9.0. running mean: -6.744270583646186, timestamp: 2022-08-20 18:01:19.098014\n",
      "resetting env. episode 3004, reward total was -9.0. running mean: -6.766827877809724, timestamp: 2022-08-20 18:01:34.222587\n",
      "resetting env. episode 3005, reward total was -8.0. running mean: -6.779159599031627, timestamp: 2022-08-20 18:01:47.914986\n",
      "resetting env. episode 3006, reward total was -9.0. running mean: -6.80136800304131, timestamp: 2022-08-20 18:02:02.721408\n",
      "resetting env. episode 3007, reward total was -7.0. running mean: -6.803354323010898, timestamp: 2022-08-20 18:02:15.576049\n",
      "resetting env. episode 3008, reward total was -13.0. running mean: -6.865320779780789, timestamp: 2022-08-20 18:02:30.167049\n",
      "resetting env. episode 3009, reward total was -11.0. running mean: -6.9066675719829815, timestamp: 2022-08-20 18:02:43.719824\n",
      "resetting env. episode 3010, reward total was -6.0. running mean: -6.897600896263151, timestamp: 2022-08-20 18:03:01.306839\n",
      "resetting env. episode 3011, reward total was -11.0. running mean: -6.93862488730052, timestamp: 2022-08-20 18:03:17.123531\n",
      "resetting env. episode 3012, reward total was 1.0. running mean: -6.859238638427515, timestamp: 2022-08-20 18:03:37.367420\n",
      "resetting env. episode 3013, reward total was -2.0. running mean: -6.810646252043239, timestamp: 2022-08-20 18:03:59.372599\n",
      "resetting env. episode 3014, reward total was -3.0. running mean: -6.772539789522806, timestamp: 2022-08-20 18:04:16.331272\n",
      "resetting env. episode 3015, reward total was -15.0. running mean: -6.8548143916275786, timestamp: 2022-08-20 18:04:28.511713\n",
      "resetting env. episode 3016, reward total was -7.0. running mean: -6.856266247711303, timestamp: 2022-08-20 18:04:43.265277\n",
      "resetting env. episode 3017, reward total was -10.0. running mean: -6.887703585234189, timestamp: 2022-08-20 18:04:58.817720\n",
      "resetting env. episode 3018, reward total was -7.0. running mean: -6.8888265493818475, timestamp: 2022-08-20 18:05:15.683619\n",
      "resetting env. episode 3019, reward total was -12.0. running mean: -6.939938283888029, timestamp: 2022-08-20 18:05:30.465109\n",
      "resetting env. episode 3020, reward total was 9.0. running mean: -6.780538901049149, timestamp: 2022-08-20 18:05:46.435422\n",
      "resetting env. episode 3021, reward total was -1.0. running mean: -6.7227335120386575, timestamp: 2022-08-20 18:06:04.299220\n",
      "resetting env. episode 3022, reward total was -8.0. running mean: -6.735506176918271, timestamp: 2022-08-20 18:06:24.157137\n",
      "resetting env. episode 3023, reward total was -6.0. running mean: -6.728151115149088, timestamp: 2022-08-20 18:06:41.355180\n",
      "resetting env. episode 3024, reward total was -1.0. running mean: -6.6708696039975965, timestamp: 2022-08-20 18:07:05.137624\n",
      "resetting env. episode 3025, reward total was -3.0. running mean: -6.634160907957621, timestamp: 2022-08-20 18:07:23.774806\n",
      "resetting env. episode 3026, reward total was -2.0. running mean: -6.587819298878044, timestamp: 2022-08-20 18:07:42.017068\n",
      "resetting env. episode 3027, reward total was -15.0. running mean: -6.671941105889264, timestamp: 2022-08-20 18:07:56.671897\n",
      "resetting env. episode 3028, reward total was -1.0. running mean: -6.615221694830371, timestamp: 2022-08-20 18:08:14.498247\n",
      "resetting env. episode 3029, reward total was 6.0. running mean: -6.489069477882068, timestamp: 2022-08-20 18:08:33.450586\n",
      "resetting env. episode 3030, reward total was 1.0. running mean: -6.414178783103248, timestamp: 2022-08-20 18:08:54.502321\n",
      "resetting env. episode 3031, reward total was -9.0. running mean: -6.440036995272215, timestamp: 2022-08-20 18:09:09.289791\n",
      "resetting env. episode 3032, reward total was -3.0. running mean: -6.405636625319493, timestamp: 2022-08-20 18:09:27.433290\n",
      "resetting env. episode 3033, reward total was -8.0. running mean: -6.421580259066299, timestamp: 2022-08-20 18:09:43.576144\n",
      "resetting env. episode 3034, reward total was -4.0. running mean: -6.397364456475636, timestamp: 2022-08-20 18:10:02.000892\n",
      "resetting env. episode 3035, reward total was -1.0. running mean: -6.343390811910879, timestamp: 2022-08-20 18:10:20.734825\n",
      "resetting env. episode 3036, reward total was -11.0. running mean: -6.389956903791771, timestamp: 2022-08-20 18:10:38.296870\n",
      "resetting env. episode 3037, reward total was -5.0. running mean: -6.376057334753853, timestamp: 2022-08-20 18:10:56.079340\n",
      "resetting env. episode 3038, reward total was -2.0. running mean: -6.332296761406314, timestamp: 2022-08-20 18:11:15.020708\n",
      "resetting env. episode 3039, reward total was 1.0. running mean: -6.258973793792252, timestamp: 2022-08-20 18:11:34.651253\n",
      "resetting env. episode 3040, reward total was -1.0. running mean: -6.206384055854329, timestamp: 2022-08-20 18:11:52.431708\n",
      "resetting env. episode 3041, reward total was 5.0. running mean: -6.094320215295786, timestamp: 2022-08-20 18:12:09.003426\n",
      "resetting env. episode 3042, reward total was -10.0. running mean: -6.133377013142828, timestamp: 2022-08-20 18:12:23.186504\n",
      "resetting env. episode 3043, reward total was -10.0. running mean: -6.172043243011399, timestamp: 2022-08-20 18:12:37.142198\n",
      "resetting env. episode 3044, reward total was -10.0. running mean: -6.210322810581284, timestamp: 2022-08-20 18:12:52.747488\n",
      "resetting env. episode 3045, reward total was -6.0. running mean: -6.208219582475471, timestamp: 2022-08-20 18:13:10.580816\n",
      "resetting env. episode 3046, reward total was -7.0. running mean: -6.216137386650717, timestamp: 2022-08-20 18:13:28.206704\n",
      "resetting env. episode 3047, reward total was -9.0. running mean: -6.243976012784209, timestamp: 2022-08-20 18:13:40.503838\n",
      "resetting env. episode 3048, reward total was -9.0. running mean: -6.271536252656367, timestamp: 2022-08-20 18:13:54.925283\n",
      "resetting env. episode 3049, reward total was 5.0. running mean: -6.158820890129804, timestamp: 2022-08-20 18:14:11.223716\n",
      "resetting env. episode 3050, reward total was -13.0. running mean: -6.2272326812285055, timestamp: 2022-08-20 18:14:27.212992\n",
      "resetting env. episode 3051, reward total was -6.0. running mean: -6.22496035441622, timestamp: 2022-08-20 18:14:43.361813\n",
      "resetting env. episode 3052, reward total was -4.0. running mean: -6.202710750872058, timestamp: 2022-08-20 18:15:00.440170\n",
      "resetting env. episode 3053, reward total was -7.0. running mean: -6.210683643363337, timestamp: 2022-08-20 18:15:17.836664\n",
      "resetting env. episode 3054, reward total was -3.0. running mean: -6.178576806929704, timestamp: 2022-08-20 18:15:40.533992\n",
      "resetting env. episode 3055, reward total was 8.0. running mean: -6.0367910388604065, timestamp: 2022-08-20 18:15:56.307827\n",
      "resetting env. episode 3056, reward total was -3.0. running mean: -6.006423128471803, timestamp: 2022-08-20 18:16:17.408426\n",
      "resetting env. episode 3057, reward total was 7.0. running mean: -5.876358897187084, timestamp: 2022-08-20 18:16:30.864462\n",
      "resetting env. episode 3058, reward total was 7.0. running mean: -5.747595308215213, timestamp: 2022-08-20 18:16:49.334087\n",
      "resetting env. episode 3059, reward total was -13.0. running mean: -5.820119355133061, timestamp: 2022-08-20 18:17:03.007541\n",
      "resetting env. episode 3060, reward total was 5.0. running mean: -5.71191816158173, timestamp: 2022-08-20 18:17:30.848124\n",
      "resetting env. episode 3061, reward total was -6.0. running mean: -5.714798979965912, timestamp: 2022-08-20 18:17:47.488619\n",
      "resetting env. episode 3062, reward total was -6.0. running mean: -5.717650990166253, timestamp: 2022-08-20 18:18:05.922358\n",
      "resetting env. episode 3063, reward total was 1.0. running mean: -5.650474480264591, timestamp: 2022-08-20 18:18:24.365050\n",
      "resetting env. episode 3064, reward total was 4.0. running mean: -5.553969735461945, timestamp: 2022-08-20 18:18:39.595337\n",
      "resetting env. episode 3065, reward total was 1.0. running mean: -5.488430038107325, timestamp: 2022-08-20 18:18:59.662700\n",
      "resetting env. episode 3066, reward total was -11.0. running mean: -5.5435457377262525, timestamp: 2022-08-20 18:19:12.809557\n",
      "resetting env. episode 3067, reward total was -3.0. running mean: -5.51811028034899, timestamp: 2022-08-20 18:19:31.331050\n",
      "resetting env. episode 3068, reward total was -15.0. running mean: -5.6129291775455, timestamp: 2022-08-20 18:19:45.760477\n",
      "resetting env. episode 3069, reward total was -9.0. running mean: -5.646799885770045, timestamp: 2022-08-20 18:20:05.116740\n",
      "resetting env. episode 3070, reward total was -9.0. running mean: -5.680331886912345, timestamp: 2022-08-20 18:20:25.245933\n",
      "resetting env. episode 3071, reward total was -1.0. running mean: -5.633528568043221, timestamp: 2022-08-20 18:20:43.367523\n",
      "resetting env. episode 3072, reward total was -9.0. running mean: -5.667193282362789, timestamp: 2022-08-20 18:21:01.095112\n",
      "resetting env. episode 3073, reward total was -13.0. running mean: -5.740521349539161, timestamp: 2022-08-20 18:21:15.756919\n",
      "resetting env. episode 3074, reward total was -3.0. running mean: -5.71311613604377, timestamp: 2022-08-20 18:21:32.574963\n",
      "resetting env. episode 3075, reward total was -4.0. running mean: -5.695984974683332, timestamp: 2022-08-20 18:21:48.521339\n",
      "resetting env. episode 3076, reward total was 6.0. running mean: -5.579025124936499, timestamp: 2022-08-20 18:22:06.647885\n",
      "resetting env. episode 3077, reward total was -4.0. running mean: -5.563234873687134, timestamp: 2022-08-20 18:22:24.114201\n",
      "resetting env. episode 3078, reward total was -13.0. running mean: -5.637602524950262, timestamp: 2022-08-20 18:22:39.439252\n",
      "resetting env. episode 3079, reward total was -7.0. running mean: -5.65122649970076, timestamp: 2022-08-20 18:22:55.340782\n",
      "resetting env. episode 3080, reward total was -9.0. running mean: -5.684714234703752, timestamp: 2022-08-20 18:23:08.845631\n",
      "resetting env. episode 3081, reward total was -7.0. running mean: -5.697867092356715, timestamp: 2022-08-20 18:23:29.411660\n",
      "resetting env. episode 3082, reward total was -1.0. running mean: -5.650888421433147, timestamp: 2022-08-20 18:23:48.578426\n",
      "resetting env. episode 3083, reward total was -12.0. running mean: -5.714379537218816, timestamp: 2022-08-20 18:24:04.806048\n",
      "resetting env. episode 3084, reward total was -9.0. running mean: -5.7472357418466276, timestamp: 2022-08-20 18:24:28.096796\n",
      "resetting env. episode 3085, reward total was -2.0. running mean: -5.709763384428161, timestamp: 2022-08-20 18:24:50.228634\n",
      "resetting env. episode 3086, reward total was -3.0. running mean: -5.68266575058388, timestamp: 2022-08-20 18:25:10.810621\n",
      "resetting env. episode 3087, reward total was -8.0. running mean: -5.7058390930780405, timestamp: 2022-08-20 18:25:26.754009\n",
      "resetting env. episode 3088, reward total was -12.0. running mean: -5.76878070214726, timestamp: 2022-08-20 18:25:39.950736\n",
      "resetting env. episode 3089, reward total was -4.0. running mean: -5.751092895125788, timestamp: 2022-08-20 18:25:59.936314\n",
      "resetting env. episode 3090, reward total was 6.0. running mean: -5.63358196617453, timestamp: 2022-08-20 18:26:15.125743\n",
      "resetting env. episode 3091, reward total was -11.0. running mean: -5.687246146512785, timestamp: 2022-08-20 18:26:29.546203\n",
      "resetting env. episode 3092, reward total was -3.0. running mean: -5.660373685047658, timestamp: 2022-08-20 18:26:48.555390\n",
      "resetting env. episode 3093, reward total was -5.0. running mean: -5.653769948197181, timestamp: 2022-08-20 18:27:03.793659\n",
      "resetting env. episode 3094, reward total was 1.0. running mean: -5.5872322487152095, timestamp: 2022-08-20 18:27:24.532223\n",
      "resetting env. episode 3095, reward total was -8.0. running mean: -5.611359926228057, timestamp: 2022-08-20 18:27:40.517496\n",
      "resetting env. episode 3096, reward total was -3.0. running mean: -5.585246326965777, timestamp: 2022-08-20 18:27:59.206540\n",
      "resetting env. episode 3097, reward total was -3.0. running mean: -5.559393863696119, timestamp: 2022-08-20 18:28:22.170161\n",
      "resetting env. episode 3098, reward total was -1.0. running mean: -5.513799925059158, timestamp: 2022-08-20 18:28:40.453287\n",
      "resetting env. episode 3099, reward total was -2.0. running mean: -5.478661925808566, timestamp: 2022-08-20 18:29:01.472103\n",
      "resetting env. episode 3100, reward total was -7.0. running mean: -5.49387530655048, timestamp: 2022-08-20 18:29:20.961009\n",
      "resetting env. episode 3101, reward total was -9.0. running mean: -5.528936553484975, timestamp: 2022-08-20 18:29:37.234509\n",
      "resetting env. episode 3102, reward total was -11.0. running mean: -5.583647187950126, timestamp: 2022-08-20 18:29:53.198837\n",
      "resetting env. episode 3103, reward total was -13.0. running mean: -5.657810716070625, timestamp: 2022-08-20 18:30:09.150201\n",
      "resetting env. episode 3104, reward total was 5.0. running mean: -5.551232608909919, timestamp: 2022-08-20 18:30:23.619523\n",
      "resetting env. episode 3105, reward total was -12.0. running mean: -5.61572028282082, timestamp: 2022-08-20 18:30:35.865789\n",
      "resetting env. episode 3106, reward total was -1.0. running mean: -5.569563079992611, timestamp: 2022-08-20 18:30:57.451106\n",
      "resetting env. episode 3107, reward total was -10.0. running mean: -5.613867449192685, timestamp: 2022-08-20 18:31:13.105249\n",
      "resetting env. episode 3108, reward total was 3.0. running mean: -5.5277287747007575, timestamp: 2022-08-20 18:31:29.285996\n",
      "resetting env. episode 3109, reward total was -11.0. running mean: -5.58245148695375, timestamp: 2022-08-20 18:31:42.749010\n",
      "resetting env. episode 3110, reward total was -2.0. running mean: -5.546626972084212, timestamp: 2022-08-20 18:32:00.359937\n",
      "resetting env. episode 3111, reward total was -11.0. running mean: -5.60116070236337, timestamp: 2022-08-20 18:32:15.298010\n",
      "resetting env. episode 3112, reward total was 1.0. running mean: -5.535149095339737, timestamp: 2022-08-20 18:32:37.587425\n",
      "resetting env. episode 3113, reward total was -9.0. running mean: -5.5697976043863395, timestamp: 2022-08-20 18:32:57.514178\n",
      "resetting env. episode 3114, reward total was -5.0. running mean: -5.5640996283424755, timestamp: 2022-08-20 18:33:12.934941\n",
      "resetting env. episode 3115, reward total was -5.0. running mean: -5.55845863205905, timestamp: 2022-08-20 18:33:28.484379\n",
      "resetting env. episode 3116, reward total was -9.0. running mean: -5.592874045738459, timestamp: 2022-08-20 18:33:43.174123\n",
      "resetting env. episode 3117, reward total was -10.0. running mean: -5.636945305281074, timestamp: 2022-08-20 18:33:58.000481\n",
      "resetting env. episode 3118, reward total was -9.0. running mean: -5.670575852228263, timestamp: 2022-08-20 18:34:14.427585\n",
      "resetting env. episode 3119, reward total was -4.0. running mean: -5.653870093705981, timestamp: 2022-08-20 18:34:31.886915\n",
      "resetting env. episode 3120, reward total was 10.0. running mean: -5.497331392768921, timestamp: 2022-08-20 18:34:43.525793\n",
      "resetting env. episode 3121, reward total was -7.0. running mean: -5.512358078841232, timestamp: 2022-08-20 18:34:58.175645\n",
      "resetting env. episode 3122, reward total was -8.0. running mean: -5.53723449805282, timestamp: 2022-08-20 18:35:16.207434\n",
      "resetting env. episode 3123, reward total was -9.0. running mean: -5.571862153072292, timestamp: 2022-08-20 18:35:35.062036\n",
      "resetting env. episode 3124, reward total was -1.0. running mean: -5.526143531541569, timestamp: 2022-08-20 18:35:55.274012\n",
      "resetting env. episode 3125, reward total was -11.0. running mean: -5.580882096226154, timestamp: 2022-08-20 18:36:10.185151\n",
      "resetting env. episode 3126, reward total was -6.0. running mean: -5.585073275263892, timestamp: 2022-08-20 18:36:28.941024\n",
      "resetting env. episode 3127, reward total was 4.0. running mean: -5.489222542511253, timestamp: 2022-08-20 18:36:46.268700\n",
      "resetting env. episode 3128, reward total was -8.0. running mean: -5.51433031708614, timestamp: 2022-08-20 18:37:00.137646\n",
      "resetting env. episode 3129, reward total was -9.0. running mean: -5.549187013915279, timestamp: 2022-08-20 18:37:12.832698\n",
      "resetting env. episode 3130, reward total was -7.0. running mean: -5.563695143776126, timestamp: 2022-08-20 18:37:26.420374\n",
      "resetting env. episode 3131, reward total was -15.0. running mean: -5.658058192338365, timestamp: 2022-08-20 18:37:39.868430\n",
      "resetting env. episode 3132, reward total was -9.0. running mean: -5.691477610414982, timestamp: 2022-08-20 18:37:53.212790\n",
      "resetting env. episode 3133, reward total was -5.0. running mean: -5.684562834310832, timestamp: 2022-08-20 18:38:09.981935\n",
      "resetting env. episode 3134, reward total was -4.0. running mean: -5.667717205967723, timestamp: 2022-08-20 18:38:28.590212\n",
      "resetting env. episode 3135, reward total was 9.0. running mean: -5.521040033908046, timestamp: 2022-08-20 18:38:43.187176\n",
      "resetting env. episode 3136, reward total was -5.0. running mean: -5.515829633568965, timestamp: 2022-08-20 18:39:05.571354\n",
      "resetting env. episode 3137, reward total was -2.0. running mean: -5.480671337233275, timestamp: 2022-08-20 18:39:25.101142\n",
      "resetting env. episode 3138, reward total was -13.0. running mean: -5.555864623860942, timestamp: 2022-08-20 18:39:38.309833\n",
      "resetting env. episode 3139, reward total was -4.0. running mean: -5.5403059776223325, timestamp: 2022-08-20 18:39:57.578342\n",
      "resetting env. episode 3140, reward total was -9.0. running mean: -5.574902917846109, timestamp: 2022-08-20 18:40:13.791993\n",
      "resetting env. episode 3141, reward total was -4.0. running mean: -5.559153888667648, timestamp: 2022-08-20 18:40:35.835074\n",
      "resetting env. episode 3142, reward total was 4.0. running mean: -5.463562349780972, timestamp: 2022-08-20 18:40:55.189340\n",
      "resetting env. episode 3143, reward total was -17.0. running mean: -5.578926726283162, timestamp: 2022-08-20 18:41:10.356793\n",
      "resetting env. episode 3144, reward total was -5.0. running mean: -5.57313745902033, timestamp: 2022-08-20 18:41:29.327095\n",
      "resetting env. episode 3145, reward total was -5.0. running mean: -5.567406084430127, timestamp: 2022-08-20 18:41:46.723584\n",
      "resetting env. episode 3146, reward total was -11.0. running mean: -5.621732023585826, timestamp: 2022-08-20 18:42:02.424616\n",
      "resetting env. episode 3147, reward total was -10.0. running mean: -5.6655147033499675, timestamp: 2022-08-20 18:42:19.965727\n",
      "resetting env. episode 3148, reward total was -7.0. running mean: -5.678859556316468, timestamp: 2022-08-20 18:42:35.832319\n",
      "resetting env. episode 3149, reward total was -6.0. running mean: -5.682070960753302, timestamp: 2022-08-20 18:42:55.376075\n",
      "resetting env. episode 3150, reward total was -7.0. running mean: -5.69525025114577, timestamp: 2022-08-20 18:43:16.415838\n",
      "resetting env. episode 3151, reward total was -7.0. running mean: -5.708297748634312, timestamp: 2022-08-20 18:43:33.241871\n",
      "resetting env. episode 3152, reward total was -6.0. running mean: -5.711214771147969, timestamp: 2022-08-20 18:43:47.614963\n",
      "resetting env. episode 3153, reward total was -12.0. running mean: -5.774102623436489, timestamp: 2022-08-20 18:44:04.026089\n",
      "resetting env. episode 3154, reward total was -1.0. running mean: -5.726361597202124, timestamp: 2022-08-20 18:44:24.270977\n",
      "resetting env. episode 3155, reward total was -6.0. running mean: -5.729097981230102, timestamp: 2022-08-20 18:44:41.043142\n",
      "resetting env. episode 3156, reward total was -7.0. running mean: -5.7418070014178015, timestamp: 2022-08-20 18:44:57.862189\n",
      "resetting env. episode 3157, reward total was -10.0. running mean: -5.784388931403623, timestamp: 2022-08-20 18:45:14.815870\n",
      "resetting env. episode 3158, reward total was 2.0. running mean: -5.706545042089587, timestamp: 2022-08-20 18:45:35.228307\n",
      "resetting env. episode 3159, reward total was -9.0. running mean: -5.739479591668691, timestamp: 2022-08-20 18:45:51.731193\n",
      "resetting env. episode 3160, reward total was -10.0. running mean: -5.782084795752004, timestamp: 2022-08-20 18:46:09.795910\n",
      "resetting env. episode 3161, reward total was -1.0. running mean: -5.734263947794484, timestamp: 2022-08-20 18:46:29.793455\n",
      "resetting env. episode 3162, reward total was -6.0. running mean: -5.736921308316539, timestamp: 2022-08-20 18:46:49.102843\n",
      "resetting env. episode 3163, reward total was 6.0. running mean: -5.619552095233374, timestamp: 2022-08-20 18:47:08.093080\n",
      "resetting env. episode 3164, reward total was -9.0. running mean: -5.65335657428104, timestamp: 2022-08-20 18:47:25.346973\n",
      "resetting env. episode 3165, reward total was 3.0. running mean: -5.566823008538229, timestamp: 2022-08-20 18:47:43.758745\n",
      "resetting env. episode 3166, reward total was -14.0. running mean: -5.651154778452846, timestamp: 2022-08-20 18:47:57.192836\n",
      "resetting env. episode 3167, reward total was -3.0. running mean: -5.624643230668318, timestamp: 2022-08-20 18:48:17.915443\n",
      "resetting env. episode 3168, reward total was 4.0. running mean: -5.5283967983616344, timestamp: 2022-08-20 18:48:33.652383\n",
      "resetting env. episode 3169, reward total was -8.0. running mean: -5.553112830378018, timestamp: 2022-08-20 18:48:46.858079\n",
      "resetting env. episode 3170, reward total was -12.0. running mean: -5.617581702074238, timestamp: 2022-08-20 18:49:02.485307\n",
      "resetting env. episode 3171, reward total was 7.0. running mean: -5.491405885053496, timestamp: 2022-08-20 18:49:19.002161\n",
      "resetting env. episode 3172, reward total was -3.0. running mean: -5.466491826202961, timestamp: 2022-08-20 18:49:39.045583\n",
      "resetting env. episode 3173, reward total was -11.0. running mean: -5.521826907940931, timestamp: 2022-08-20 18:49:53.589706\n",
      "resetting env. episode 3174, reward total was -7.0. running mean: -5.536608638861522, timestamp: 2022-08-20 18:50:11.628489\n",
      "resetting env. episode 3175, reward total was -5.0. running mean: -5.531242552472906, timestamp: 2022-08-20 18:50:29.455838\n",
      "resetting env. episode 3176, reward total was -13.0. running mean: -5.605930126948177, timestamp: 2022-08-20 18:50:44.945431\n",
      "resetting env. episode 3177, reward total was 3.0. running mean: -5.519870825678695, timestamp: 2022-08-20 18:51:02.879497\n",
      "resetting env. episode 3178, reward total was -11.0. running mean: -5.574672117421908, timestamp: 2022-08-20 18:51:16.815244\n",
      "resetting env. episode 3179, reward total was -19.0. running mean: -5.70892539624769, timestamp: 2022-08-20 18:51:27.932530\n",
      "resetting env. episode 3180, reward total was -11.0. running mean: -5.761836142285213, timestamp: 2022-08-20 18:51:45.879554\n",
      "resetting env. episode 3181, reward total was 8.0. running mean: -5.62421778086236, timestamp: 2022-08-20 18:51:59.494163\n",
      "resetting env. episode 3182, reward total was -11.0. running mean: -5.677975603053737, timestamp: 2022-08-20 18:52:12.605118\n",
      "resetting env. episode 3183, reward total was 1.0. running mean: -5.6111958470231995, timestamp: 2022-08-20 18:52:32.063107\n",
      "resetting env. episode 3184, reward total was -9.0. running mean: -5.645083888552968, timestamp: 2022-08-20 18:52:47.483889\n",
      "resetting env. episode 3185, reward total was -1.0. running mean: -5.598633049667438, timestamp: 2022-08-20 18:53:05.480781\n",
      "resetting env. episode 3186, reward total was -2.0. running mean: -5.562646719170763, timestamp: 2022-08-20 18:53:26.004920\n",
      "resetting env. episode 3187, reward total was -12.0. running mean: -5.627020251979055, timestamp: 2022-08-20 18:53:42.078954\n",
      "resetting env. episode 3188, reward total was -10.0. running mean: -5.670750049459264, timestamp: 2022-08-20 18:53:57.915621\n",
      "resetting env. episode 3189, reward total was -3.0. running mean: -5.644042548964672, timestamp: 2022-08-20 18:54:16.238643\n",
      "resetting env. episode 3190, reward total was -11.0. running mean: -5.697602123475026, timestamp: 2022-08-20 18:54:32.420464\n",
      "resetting env. episode 3191, reward total was -5.0. running mean: -5.690626102240275, timestamp: 2022-08-20 18:54:49.974541\n",
      "resetting env. episode 3192, reward total was -11.0. running mean: -5.743719841217873, timestamp: 2022-08-20 18:55:04.438879\n",
      "resetting env. episode 3193, reward total was -1.0. running mean: -5.696282642805694, timestamp: 2022-08-20 18:55:21.956566\n",
      "resetting env. episode 3194, reward total was -7.0. running mean: -5.709319816377637, timestamp: 2022-08-20 18:55:40.976725\n",
      "resetting env. episode 3195, reward total was -4.0. running mean: -5.692226618213861, timestamp: 2022-08-20 18:55:58.290446\n",
      "resetting env. episode 3196, reward total was 3.0. running mean: -5.605304352031722, timestamp: 2022-08-20 18:56:15.804637\n",
      "resetting env. episode 3197, reward total was -13.0. running mean: -5.679251308511405, timestamp: 2022-08-20 18:56:31.642295\n",
      "resetting env. episode 3198, reward total was -9.0. running mean: -5.712458795426291, timestamp: 2022-08-20 18:56:50.009199\n",
      "resetting env. episode 3199, reward total was -3.0. running mean: -5.685334207472028, timestamp: 2022-08-20 18:57:08.551635\n",
      "resetting env. episode 3200, reward total was -8.0. running mean: -5.708480865397308, timestamp: 2022-08-20 18:57:29.133621\n",
      "resetting env. episode 3201, reward total was -7.0. running mean: -5.721396056743336, timestamp: 2022-08-20 18:57:46.202005\n",
      "resetting env. episode 3202, reward total was 6.0. running mean: -5.604182096175903, timestamp: 2022-08-20 18:58:01.518061\n",
      "resetting env. episode 3203, reward total was 2.0. running mean: -5.5281402752141435, timestamp: 2022-08-20 18:58:21.251869\n",
      "resetting env. episode 3204, reward total was -11.0. running mean: -5.582858872462002, timestamp: 2022-08-20 18:58:35.901712\n",
      "resetting env. episode 3205, reward total was -11.0. running mean: -5.637030283737382, timestamp: 2022-08-20 18:58:50.414918\n",
      "resetting env. episode 3206, reward total was -13.0. running mean: -5.710659980900008, timestamp: 2022-08-20 18:59:05.817746\n",
      "resetting env. episode 3207, reward total was -10.0. running mean: -5.753553381091008, timestamp: 2022-08-20 18:59:23.256131\n",
      "resetting env. episode 3208, reward total was -9.0. running mean: -5.786017847280098, timestamp: 2022-08-20 18:59:37.941877\n",
      "resetting env. episode 3209, reward total was 14.0. running mean: -5.588157668807297, timestamp: 2022-08-20 18:59:49.678507\n",
      "resetting env. episode 3210, reward total was 6.0. running mean: -5.472276092119225, timestamp: 2022-08-20 19:00:05.018502\n",
      "resetting env. episode 3211, reward total was -17.0. running mean: -5.587553331198032, timestamp: 2022-08-20 19:00:16.725211\n",
      "resetting env. episode 3212, reward total was 6.0. running mean: -5.471677797886052, timestamp: 2022-08-20 19:00:32.474124\n",
      "resetting env. episode 3213, reward total was 1.0. running mean: -5.406961019907191, timestamp: 2022-08-20 19:00:53.442067\n",
      "resetting env. episode 3214, reward total was -8.0. running mean: -5.43289140970812, timestamp: 2022-08-20 19:01:10.506452\n",
      "resetting env. episode 3215, reward total was 3.0. running mean: -5.348562495611038, timestamp: 2022-08-20 19:01:28.811521\n",
      "resetting env. episode 3216, reward total was -5.0. running mean: -5.345076870654927, timestamp: 2022-08-20 19:01:47.270183\n",
      "resetting env. episode 3217, reward total was -2.0. running mean: -5.311626101948377, timestamp: 2022-08-20 19:02:06.942598\n",
      "resetting env. episode 3218, reward total was -5.0. running mean: -5.3085098409288936, timestamp: 2022-08-20 19:02:28.754324\n",
      "resetting env. episode 3219, reward total was -2.0. running mean: -5.275424742519604, timestamp: 2022-08-20 19:02:47.093274\n",
      "resetting env. episode 3220, reward total was -2.0. running mean: -5.242670495094408, timestamp: 2022-08-20 19:03:05.550937\n",
      "resetting env. episode 3221, reward total was -1.0. running mean: -5.200243790143463, timestamp: 2022-08-20 19:03:30.277841\n",
      "resetting env. episode 3222, reward total was -3.0. running mean: -5.178241352242028, timestamp: 2022-08-20 19:03:47.958581\n",
      "resetting env. episode 3223, reward total was -13.0. running mean: -5.256458938719608, timestamp: 2022-08-20 19:04:00.753382\n",
      "resetting env. episode 3224, reward total was -10.0. running mean: -5.3038943493324116, timestamp: 2022-08-20 19:04:16.665850\n",
      "resetting env. episode 3225, reward total was -4.0. running mean: -5.290855405839087, timestamp: 2022-08-20 19:04:30.902794\n",
      "resetting env. episode 3226, reward total was -10.0. running mean: -5.337946851780696, timestamp: 2022-08-20 19:04:44.360823\n",
      "resetting env. episode 3227, reward total was -1.0. running mean: -5.294567383262889, timestamp: 2022-08-20 19:05:03.705133\n",
      "resetting env. episode 3228, reward total was -13.0. running mean: -5.37162170943026, timestamp: 2022-08-20 19:05:18.763881\n",
      "resetting env. episode 3229, reward total was -13.0. running mean: -5.4479054923359564, timestamp: 2022-08-20 19:05:32.153086\n",
      "resetting env. episode 3230, reward total was -14.0. running mean: -5.533426437412596, timestamp: 2022-08-20 19:05:47.815248\n",
      "resetting env. episode 3231, reward total was -12.0. running mean: -5.59809217303847, timestamp: 2022-08-20 19:06:01.355032\n",
      "resetting env. episode 3232, reward total was -8.0. running mean: -5.622111251308086, timestamp: 2022-08-20 19:06:16.713974\n",
      "resetting env. episode 3233, reward total was -3.0. running mean: -5.5958901387950055, timestamp: 2022-08-20 19:06:37.352911\n",
      "resetting env. episode 3234, reward total was -1.0. running mean: -5.549931237407055, timestamp: 2022-08-20 19:07:01.010674\n",
      "resetting env. episode 3235, reward total was 6.0. running mean: -5.4344319250329844, timestamp: 2022-08-20 19:07:19.956051\n",
      "resetting env. episode 3236, reward total was -1.0. running mean: -5.390087605782655, timestamp: 2022-08-20 19:07:36.483855\n",
      "resetting env. episode 3237, reward total was -6.0. running mean: -5.396186729724827, timestamp: 2022-08-20 19:07:57.477747\n",
      "resetting env. episode 3238, reward total was -3.0. running mean: -5.372224862427579, timestamp: 2022-08-20 19:08:16.081583\n",
      "resetting env. episode 3239, reward total was -2.0. running mean: -5.338502613803303, timestamp: 2022-08-20 19:08:35.479732\n",
      "resetting env. episode 3240, reward total was 9.0. running mean: -5.195117587665271, timestamp: 2022-08-20 19:08:50.031835\n",
      "resetting env. episode 3241, reward total was -1.0. running mean: -5.153166411788618, timestamp: 2022-08-20 19:09:13.095189\n",
      "resetting env. episode 3242, reward total was 4.0. running mean: -5.061634747670732, timestamp: 2022-08-20 19:09:29.991039\n",
      "resetting env. episode 3243, reward total was -3.0. running mean: -5.041018400194025, timestamp: 2022-08-20 19:09:46.664466\n",
      "resetting env. episode 3244, reward total was -11.0. running mean: -5.100608216192085, timestamp: 2022-08-20 19:10:02.112693\n",
      "resetting env. episode 3245, reward total was -1.0. running mean: -5.059602134030164, timestamp: 2022-08-20 19:10:21.198677\n",
      "resetting env. episode 3246, reward total was -4.0. running mean: -5.049006112689862, timestamp: 2022-08-20 19:10:42.751067\n",
      "resetting env. episode 3247, reward total was -11.0. running mean: -5.1085160515629635, timestamp: 2022-08-20 19:11:03.490140\n",
      "resetting env. episode 3248, reward total was -9.0. running mean: -5.147430891047334, timestamp: 2022-08-20 19:11:25.136281\n",
      "resetting env. episode 3249, reward total was 4.0. running mean: -5.055956582136861, timestamp: 2022-08-20 19:11:48.919708\n",
      "resetting env. episode 3250, reward total was -9.0. running mean: -5.095397016315492, timestamp: 2022-08-20 19:12:10.613720\n",
      "resetting env. episode 3251, reward total was -5.0. running mean: -5.094443046152337, timestamp: 2022-08-20 19:12:28.947727\n",
      "resetting env. episode 3252, reward total was -1.0. running mean: -5.053498615690813, timestamp: 2022-08-20 19:12:54.481464\n",
      "resetting env. episode 3253, reward total was -13.0. running mean: -5.132963629533905, timestamp: 2022-08-20 19:13:15.438446\n",
      "resetting env. episode 3254, reward total was 1.0. running mean: -5.071633993238566, timestamp: 2022-08-20 19:13:39.163031\n",
      "resetting env. episode 3255, reward total was -10.0. running mean: -5.12091765330618, timestamp: 2022-08-20 19:14:03.116998\n",
      "resetting env. episode 3256, reward total was -2.0. running mean: -5.089708476773118, timestamp: 2022-08-20 19:14:26.407742\n",
      "resetting env. episode 3257, reward total was 1.0. running mean: -5.0288113920053865, timestamp: 2022-08-20 19:14:45.830827\n",
      "resetting env. episode 3258, reward total was 3.0. running mean: -4.948523278085332, timestamp: 2022-08-20 19:15:09.095678\n",
      "resetting env. episode 3259, reward total was -5.0. running mean: -4.949038045304479, timestamp: 2022-08-20 19:15:28.785045\n",
      "resetting env. episode 3260, reward total was -1.0. running mean: -4.909547664851434, timestamp: 2022-08-20 19:15:54.356703\n",
      "resetting env. episode 3261, reward total was -11.0. running mean: -4.97045218820292, timestamp: 2022-08-20 19:16:13.949321\n",
      "resetting env. episode 3262, reward total was -5.0. running mean: -4.97074766632089, timestamp: 2022-08-20 19:16:38.969442\n",
      "resetting env. episode 3263, reward total was -4.0. running mean: -4.961040189657681, timestamp: 2022-08-20 19:17:01.603940\n",
      "resetting env. episode 3264, reward total was -3.0. running mean: -4.941429787761105, timestamp: 2022-08-20 19:17:22.925946\n",
      "resetting env. episode 3265, reward total was -3.0. running mean: -4.922015489883494, timestamp: 2022-08-20 19:17:46.937775\n",
      "resetting env. episode 3266, reward total was -8.0. running mean: -4.952795334984659, timestamp: 2022-08-20 19:18:07.648413\n",
      "resetting env. episode 3267, reward total was -7.0. running mean: -4.9732673816348125, timestamp: 2022-08-20 19:18:26.999679\n",
      "resetting env. episode 3268, reward total was 4.0. running mean: -4.883534707818464, timestamp: 2022-08-20 19:18:47.523822\n",
      "resetting env. episode 3269, reward total was 2.0. running mean: -4.81469936074028, timestamp: 2022-08-20 19:19:12.249737\n",
      "resetting env. episode 3270, reward total was -1.0. running mean: -4.776552367132878, timestamp: 2022-08-20 19:19:42.029126\n",
      "resetting env. episode 3271, reward total was -5.0. running mean: -4.778786843461549, timestamp: 2022-08-20 19:20:04.278652\n",
      "resetting env. episode 3272, reward total was -5.0. running mean: -4.780998975026933, timestamp: 2022-08-20 19:20:26.359628\n",
      "resetting env. episode 3273, reward total was -3.0. running mean: -4.763188985276664, timestamp: 2022-08-20 19:20:52.071903\n",
      "resetting env. episode 3274, reward total was -5.0. running mean: -4.7655570954238975, timestamp: 2022-08-20 19:21:10.256805\n",
      "resetting env. episode 3275, reward total was -4.0. running mean: -4.757901524469658, timestamp: 2022-08-20 19:21:34.812167\n",
      "resetting env. episode 3276, reward total was -3.0. running mean: -4.740322509224962, timestamp: 2022-08-20 19:21:54.522491\n",
      "resetting env. episode 3277, reward total was -10.0. running mean: -4.7929192841327115, timestamp: 2022-08-20 19:22:12.052621\n",
      "resetting env. episode 3278, reward total was -8.0. running mean: -4.824990091291385, timestamp: 2022-08-20 19:22:28.861693\n",
      "resetting env. episode 3279, reward total was -6.0. running mean: -4.83674019037847, timestamp: 2022-08-20 19:22:48.713629\n",
      "resetting env. episode 3280, reward total was -7.0. running mean: -4.858372788474686, timestamp: 2022-08-20 19:23:09.524002\n",
      "resetting env. episode 3281, reward total was -7.0. running mean: -4.879789060589939, timestamp: 2022-08-20 19:23:25.837402\n",
      "resetting env. episode 3282, reward total was -2.0. running mean: -4.850991169984039, timestamp: 2022-08-20 19:23:45.471914\n",
      "resetting env. episode 3283, reward total was -6.0. running mean: -4.862481258284198, timestamp: 2022-08-20 19:24:05.386681\n",
      "resetting env. episode 3284, reward total was -11.0. running mean: -4.9238564457013565, timestamp: 2022-08-20 19:24:22.135911\n",
      "resetting env. episode 3285, reward total was -7.0. running mean: -4.944617881244343, timestamp: 2022-08-20 19:24:44.712562\n",
      "resetting env. episode 3286, reward total was -7.0. running mean: -4.9651717024319, timestamp: 2022-08-20 19:25:02.975747\n",
      "resetting env. episode 3287, reward total was -5.0. running mean: -4.965519985407581, timestamp: 2022-08-20 19:25:23.088985\n",
      "resetting env. episode 3288, reward total was -2.0. running mean: -4.9358647855535045, timestamp: 2022-08-20 19:25:44.135728\n",
      "resetting env. episode 3289, reward total was -7.0. running mean: -4.9565061376979695, timestamp: 2022-08-20 19:26:01.145257\n",
      "resetting env. episode 3290, reward total was -13.0. running mean: -5.03694107632099, timestamp: 2022-08-20 19:26:16.350616\n",
      "resetting env. episode 3291, reward total was -4.0. running mean: -5.02657166555778, timestamp: 2022-08-20 19:26:33.391081\n",
      "resetting env. episode 3292, reward total was -10.0. running mean: -5.076305948902202, timestamp: 2022-08-20 19:26:48.461784\n",
      "resetting env. episode 3293, reward total was -13.0. running mean: -5.155542889413179, timestamp: 2022-08-20 19:27:01.979675\n",
      "resetting env. episode 3294, reward total was 1.0. running mean: -5.093987460519047, timestamp: 2022-08-20 19:27:16.046047\n",
      "resetting env. episode 3295, reward total was 14.0. running mean: -4.903047585913857, timestamp: 2022-08-20 19:27:28.370139\n",
      "resetting env. episode 3296, reward total was -3.0. running mean: -4.884017110054718, timestamp: 2022-08-20 19:27:51.590037\n",
      "resetting env. episode 3297, reward total was -9.0. running mean: -4.925176938954171, timestamp: 2022-08-20 19:28:03.158121\n",
      "resetting env. episode 3298, reward total was -3.0. running mean: -4.90592516956463, timestamp: 2022-08-20 19:28:23.699209\n",
      "resetting env. episode 3299, reward total was -16.0. running mean: -5.016865917868984, timestamp: 2022-08-20 19:28:44.187458\n",
      "resetting env. episode 3300, reward total was -3.0. running mean: -4.996697258690294, timestamp: 2022-08-20 19:29:07.696604\n",
      "resetting env. episode 3301, reward total was -13.0. running mean: -5.076730286103391, timestamp: 2022-08-20 19:29:19.735435\n",
      "resetting env. episode 3302, reward total was -1.0. running mean: -5.035962983242357, timestamp: 2022-08-20 19:29:40.164834\n",
      "resetting env. episode 3303, reward total was -3.0. running mean: -5.015603353409934, timestamp: 2022-08-20 19:30:00.730856\n",
      "resetting env. episode 3304, reward total was -13.0. running mean: -5.095447319875834, timestamp: 2022-08-20 19:30:14.633693\n",
      "resetting env. episode 3305, reward total was -11.0. running mean: -5.154492846677076, timestamp: 2022-08-20 19:30:29.679465\n",
      "resetting env. episode 3306, reward total was -7.0. running mean: -5.172947918210306, timestamp: 2022-08-20 19:30:46.504515\n",
      "resetting env. episode 3307, reward total was -12.0. running mean: -5.241218439028203, timestamp: 2022-08-20 19:31:01.185254\n",
      "resetting env. episode 3308, reward total was -21.0. running mean: -5.398806254637921, timestamp: 2022-08-20 19:31:10.774618\n",
      "resetting env. episode 3309, reward total was -10.0. running mean: -5.4448181920915415, timestamp: 2022-08-20 19:31:24.375275\n",
      "resetting env. episode 3310, reward total was -1.0. running mean: -5.400370010170626, timestamp: 2022-08-20 19:31:45.949595\n",
      "resetting env. episode 3311, reward total was 1.0. running mean: -5.33636631006892, timestamp: 2022-08-20 19:32:05.917238\n",
      "resetting env. episode 3312, reward total was -9.0. running mean: -5.373002646968231, timestamp: 2022-08-20 19:32:23.771504\n",
      "resetting env. episode 3313, reward total was -5.0. running mean: -5.369272620498548, timestamp: 2022-08-20 19:32:46.750086\n",
      "resetting env. episode 3314, reward total was -14.0. running mean: -5.455579894293562, timestamp: 2022-08-20 19:33:00.050523\n",
      "resetting env. episode 3315, reward total was -7.0. running mean: -5.471024095350627, timestamp: 2022-08-20 19:33:16.890510\n",
      "resetting env. episode 3316, reward total was -5.0. running mean: -5.466313854397121, timestamp: 2022-08-20 19:33:34.069597\n",
      "resetting env. episode 3317, reward total was -8.0. running mean: -5.491650715853149, timestamp: 2022-08-20 19:33:48.438195\n",
      "resetting env. episode 3318, reward total was -7.0. running mean: -5.506734208694618, timestamp: 2022-08-20 19:34:04.405513\n",
      "resetting env. episode 3319, reward total was -4.0. running mean: -5.491666866607671, timestamp: 2022-08-20 19:34:24.829908\n",
      "resetting env. episode 3320, reward total was -14.0. running mean: -5.576750197941594, timestamp: 2022-08-20 19:34:41.159263\n",
      "resetting env. episode 3321, reward total was -1.0. running mean: -5.530982695962178, timestamp: 2022-08-20 19:35:00.833670\n",
      "resetting env. episode 3322, reward total was -11.0. running mean: -5.585672869002557, timestamp: 2022-08-20 19:35:14.960909\n",
      "resetting env. episode 3323, reward total was 2.0. running mean: -5.509816140312531, timestamp: 2022-08-20 19:35:40.340069\n",
      "resetting env. episode 3324, reward total was -8.0. running mean: -5.534717978909406, timestamp: 2022-08-20 19:35:59.829975\n",
      "resetting env. episode 3325, reward total was -9.0. running mean: -5.569370799120311, timestamp: 2022-08-20 19:36:14.399029\n",
      "resetting env. episode 3326, reward total was -8.0. running mean: -5.593677091129108, timestamp: 2022-08-20 19:36:34.388627\n",
      "resetting env. episode 3327, reward total was -17.0. running mean: -5.707740320217817, timestamp: 2022-08-20 19:36:48.133864\n",
      "resetting env. episode 3328, reward total was 4.0. running mean: -5.610662917015639, timestamp: 2022-08-20 19:37:04.468356\n",
      "resetting env. episode 3329, reward total was -3.0. running mean: -5.584556287845483, timestamp: 2022-08-20 19:37:25.180622\n",
      "resetting env. episode 3330, reward total was -11.0. running mean: -5.638710724967028, timestamp: 2022-08-20 19:37:42.026593\n",
      "resetting env. episode 3331, reward total was -8.0. running mean: -5.662323617717358, timestamp: 2022-08-20 19:38:04.185966\n",
      "resetting env. episode 3332, reward total was -9.0. running mean: -5.695700381540184, timestamp: 2022-08-20 19:38:24.054013\n",
      "resetting env. episode 3333, reward total was -10.0. running mean: -5.738743377724782, timestamp: 2022-08-20 19:38:36.298271\n",
      "resetting env. episode 3334, reward total was -7.0. running mean: -5.751355943947534, timestamp: 2022-08-20 19:38:52.164871\n",
      "resetting env. episode 3335, reward total was -7.0. running mean: -5.763842384508059, timestamp: 2022-08-20 19:39:05.306732\n",
      "resetting env. episode 3336, reward total was -7.0. running mean: -5.776203960662978, timestamp: 2022-08-20 19:39:23.637736\n",
      "resetting env. episode 3337, reward total was -5.0. running mean: -5.768441921056349, timestamp: 2022-08-20 19:39:41.516943\n",
      "resetting env. episode 3338, reward total was -8.0. running mean: -5.7907575018457855, timestamp: 2022-08-20 19:39:58.885091\n",
      "resetting env. episode 3339, reward total was -10.0. running mean: -5.832849926827327, timestamp: 2022-08-20 19:40:14.245040\n",
      "resetting env. episode 3340, reward total was -11.0. running mean: -5.884521427559054, timestamp: 2022-08-20 19:40:32.067433\n",
      "resetting env. episode 3341, reward total was -10.0. running mean: -5.925676213283463, timestamp: 2022-08-20 19:40:48.979200\n",
      "resetting env. episode 3342, reward total was -5.0. running mean: -5.916419451150628, timestamp: 2022-08-20 19:41:09.403595\n",
      "resetting env. episode 3343, reward total was -4.0. running mean: -5.897255256639122, timestamp: 2022-08-20 19:41:28.898502\n",
      "resetting env. episode 3344, reward total was -5.0. running mean: -5.88828270407273, timestamp: 2022-08-20 19:41:46.895381\n",
      "resetting env. episode 3345, reward total was -9.0. running mean: -5.919399877032003, timestamp: 2022-08-20 19:42:03.254653\n",
      "resetting env. episode 3346, reward total was -4.0. running mean: -5.900205878261683, timestamp: 2022-08-20 19:42:23.267158\n",
      "resetting env. episode 3347, reward total was -9.0. running mean: -5.931203819479066, timestamp: 2022-08-20 19:42:40.497106\n",
      "resetting env. episode 3348, reward total was -3.0. running mean: -5.9018917812842755, timestamp: 2022-08-20 19:42:59.042532\n",
      "resetting env. episode 3349, reward total was -10.0. running mean: -5.9428728634714325, timestamp: 2022-08-20 19:43:12.213327\n",
      "resetting env. episode 3350, reward total was 2.0. running mean: -5.863444134836719, timestamp: 2022-08-20 19:43:32.963861\n",
      "resetting env. episode 3351, reward total was -1.0. running mean: -5.8148096934883515, timestamp: 2022-08-20 19:43:54.950097\n",
      "resetting env. episode 3352, reward total was -1.0. running mean: -5.766661596553468, timestamp: 2022-08-20 19:44:11.238566\n",
      "resetting env. episode 3353, reward total was -8.0. running mean: -5.788994980587933, timestamp: 2022-08-20 19:44:28.559256\n",
      "resetting env. episode 3354, reward total was -12.0. running mean: -5.851105030782054, timestamp: 2022-08-20 19:44:43.239014\n",
      "resetting env. episode 3355, reward total was -10.0. running mean: -5.892593980474233, timestamp: 2022-08-20 19:45:01.851268\n",
      "resetting env. episode 3356, reward total was -6.0. running mean: -5.89366804066949, timestamp: 2022-08-20 19:45:17.225168\n",
      "resetting env. episode 3357, reward total was -5.0. running mean: -5.884731360262795, timestamp: 2022-08-20 19:45:38.046512\n",
      "resetting env. episode 3358, reward total was -14.0. running mean: -5.965884046660166, timestamp: 2022-08-20 19:45:53.938034\n",
      "resetting env. episode 3359, reward total was -3.0. running mean: -5.936225206193565, timestamp: 2022-08-20 19:46:14.582866\n",
      "resetting env. episode 3360, reward total was 8.0. running mean: -5.796862954131629, timestamp: 2022-08-20 19:46:32.725368\n",
      "resetting env. episode 3361, reward total was -7.0. running mean: -5.808894324590312, timestamp: 2022-08-20 19:46:51.914095\n",
      "resetting env. episode 3362, reward total was -8.0. running mean: -5.830805381344409, timestamp: 2022-08-20 19:47:11.885697\n",
      "resetting env. episode 3363, reward total was 5.0. running mean: -5.722497327530966, timestamp: 2022-08-20 19:47:33.064085\n",
      "resetting env. episode 3364, reward total was -3.0. running mean: -5.695272354255656, timestamp: 2022-08-20 19:47:53.777715\n",
      "resetting env. episode 3365, reward total was -1.0. running mean: -5.648319630713099, timestamp: 2022-08-20 19:48:14.594089\n",
      "resetting env. episode 3366, reward total was -8.0. running mean: -5.671836434405969, timestamp: 2022-08-20 19:48:33.993220\n",
      "resetting env. episode 3367, reward total was -1.0. running mean: -5.625118070061909, timestamp: 2022-08-20 19:48:54.037643\n",
      "resetting env. episode 3368, reward total was -4.0. running mean: -5.608866889361289, timestamp: 2022-08-20 19:49:12.422524\n",
      "resetting env. episode 3369, reward total was -4.0. running mean: -5.592778220467676, timestamp: 2022-08-20 19:49:30.615868\n",
      "resetting env. episode 3370, reward total was -10.0. running mean: -5.636850438262999, timestamp: 2022-08-20 19:49:46.715832\n",
      "resetting env. episode 3371, reward total was -9.0. running mean: -5.670481933880369, timestamp: 2022-08-20 19:50:05.304145\n",
      "resetting env. episode 3372, reward total was -8.0. running mean: -5.693777114541565, timestamp: 2022-08-20 19:50:23.239240\n",
      "resetting env. episode 3373, reward total was -5.0. running mean: -5.68683934339615, timestamp: 2022-08-20 19:50:43.374386\n",
      "resetting env. episode 3374, reward total was -6.0. running mean: -5.689970949962188, timestamp: 2022-08-20 19:51:05.235955\n",
      "resetting env. episode 3375, reward total was -3.0. running mean: -5.663071240462566, timestamp: 2022-08-20 19:51:27.431621\n",
      "resetting env. episode 3376, reward total was -6.0. running mean: -5.666440528057939, timestamp: 2022-08-20 19:51:50.041182\n",
      "resetting env. episode 3377, reward total was -2.0. running mean: -5.6297761227773595, timestamp: 2022-08-20 19:52:11.248502\n",
      "resetting env. episode 3378, reward total was -10.0. running mean: -5.673478361549585, timestamp: 2022-08-20 19:52:27.367412\n",
      "resetting env. episode 3379, reward total was -10.0. running mean: -5.716743577934089, timestamp: 2022-08-20 19:52:47.975338\n",
      "resetting env. episode 3380, reward total was -16.0. running mean: -5.819576142154748, timestamp: 2022-08-20 19:53:00.001181\n",
      "resetting env. episode 3381, reward total was -9.0. running mean: -5.8513803807332, timestamp: 2022-08-20 19:53:17.680924\n",
      "resetting env. episode 3382, reward total was -11.0. running mean: -5.902866576925868, timestamp: 2022-08-20 19:53:33.998305\n",
      "resetting env. episode 3383, reward total was -9.0. running mean: -5.933837911156609, timestamp: 2022-08-20 19:53:52.955633\n",
      "resetting env. episode 3384, reward total was -13.0. running mean: -6.004499532045043, timestamp: 2022-08-20 19:54:07.995458\n",
      "resetting env. episode 3385, reward total was -1.0. running mean: -5.954454536724592, timestamp: 2022-08-20 19:54:23.384298\n",
      "resetting env. episode 3386, reward total was -9.0. running mean: -5.984909991357346, timestamp: 2022-08-20 19:54:39.728607\n",
      "resetting env. episode 3387, reward total was -13.0. running mean: -6.055060891443772, timestamp: 2022-08-20 19:54:52.829589\n",
      "resetting env. episode 3388, reward total was -10.0. running mean: -6.094510282529334, timestamp: 2022-08-20 19:55:11.097761\n",
      "resetting env. episode 3389, reward total was -9.0. running mean: -6.12356517970404, timestamp: 2022-08-20 19:55:27.669461\n",
      "resetting env. episode 3390, reward total was -2.0. running mean: -6.0823295279069995, timestamp: 2022-08-20 19:55:48.621462\n",
      "resetting env. episode 3391, reward total was -5.0. running mean: -6.07150623262793, timestamp: 2022-08-20 19:56:05.739701\n",
      "resetting env. episode 3392, reward total was 1.0. running mean: -6.0007911703016505, timestamp: 2022-08-20 19:56:25.017174\n",
      "resetting env. episode 3393, reward total was 1.0. running mean: -5.930783258598634, timestamp: 2022-08-20 19:56:50.492104\n",
      "resetting env. episode 3394, reward total was -3.0. running mean: -5.901475426012648, timestamp: 2022-08-20 19:57:11.226654\n",
      "resetting env. episode 3395, reward total was -10.0. running mean: -5.942460671752521, timestamp: 2022-08-20 19:57:26.878819\n",
      "resetting env. episode 3396, reward total was 1.0. running mean: -5.873036065034996, timestamp: 2022-08-20 19:57:46.708829\n",
      "resetting env. episode 3397, reward total was -3.0. running mean: -5.844305704384646, timestamp: 2022-08-20 19:58:05.264217\n",
      "resetting env. episode 3398, reward total was -12.0. running mean: -5.9058626473408, timestamp: 2022-08-20 19:58:20.478545\n",
      "resetting env. episode 3399, reward total was -10.0. running mean: -5.946804020867392, timestamp: 2022-08-20 19:58:36.899651\n",
      "resetting env. episode 3400, reward total was -4.0. running mean: -5.927335980658718, timestamp: 2022-08-20 19:58:56.666813\n",
      "resetting env. episode 3401, reward total was -1.0. running mean: -5.8780626208521305, timestamp: 2022-08-20 19:59:19.000118\n",
      "resetting env. episode 3402, reward total was -7.0. running mean: -5.88928199464361, timestamp: 2022-08-20 19:59:36.767640\n",
      "resetting env. episode 3403, reward total was -9.0. running mean: -5.920389174697173, timestamp: 2022-08-20 19:59:54.585996\n",
      "resetting env. episode 3404, reward total was -9.0. running mean: -5.951185282950202, timestamp: 2022-08-20 20:00:11.092882\n",
      "resetting env. episode 3405, reward total was 6.0. running mean: -5.8316734301207, timestamp: 2022-08-20 20:00:26.681203\n",
      "resetting env. episode 3406, reward total was 3.0. running mean: -5.743356695819493, timestamp: 2022-08-20 20:00:45.984606\n",
      "resetting env. episode 3407, reward total was 4.0. running mean: -5.645923128861298, timestamp: 2022-08-20 20:01:05.751776\n",
      "resetting env. episode 3408, reward total was 4.0. running mean: -5.549463897572685, timestamp: 2022-08-20 20:01:21.491693\n",
      "resetting env. episode 3409, reward total was 1.0. running mean: -5.483969258596958, timestamp: 2022-08-20 20:01:38.979950\n",
      "resetting env. episode 3410, reward total was -13.0. running mean: -5.559129566010989, timestamp: 2022-08-20 20:01:54.612180\n",
      "resetting env. episode 3411, reward total was 2.0. running mean: -5.483538270350879, timestamp: 2022-08-20 20:02:16.955441\n",
      "resetting env. episode 3412, reward total was -7.0. running mean: -5.4987028876473705, timestamp: 2022-08-20 20:02:37.319016\n",
      "resetting env. episode 3413, reward total was -11.0. running mean: -5.5537158587708975, timestamp: 2022-08-20 20:02:54.734457\n",
      "resetting env. episode 3414, reward total was -4.0. running mean: -5.538178700183188, timestamp: 2022-08-20 20:03:14.376952\n",
      "resetting env. episode 3415, reward total was -4.0. running mean: -5.522796913181356, timestamp: 2022-08-20 20:03:36.999483\n",
      "resetting env. episode 3416, reward total was 4.0. running mean: -5.4275689440495425, timestamp: 2022-08-20 20:03:56.041582\n",
      "resetting env. episode 3417, reward total was -11.0. running mean: -5.483293254609047, timestamp: 2022-08-20 20:04:11.786496\n",
      "resetting env. episode 3418, reward total was -3.0. running mean: -5.458460322062957, timestamp: 2022-08-20 20:04:31.126799\n",
      "resetting env. episode 3419, reward total was -7.0. running mean: -5.473875718842328, timestamp: 2022-08-20 20:04:49.460792\n",
      "resetting env. episode 3420, reward total was 1.0. running mean: -5.409136961653905, timestamp: 2022-08-20 20:05:11.850944\n",
      "resetting env. episode 3421, reward total was -7.0. running mean: -5.425045592037367, timestamp: 2022-08-20 20:05:30.601824\n",
      "resetting env. episode 3422, reward total was -6.0. running mean: -5.430795136116992, timestamp: 2022-08-20 20:05:45.910912\n",
      "resetting env. episode 3423, reward total was -1.0. running mean: -5.386487184755822, timestamp: 2022-08-20 20:06:10.720587\n",
      "resetting env. episode 3424, reward total was -12.0. running mean: -5.452622312908264, timestamp: 2022-08-20 20:06:26.811575\n",
      "resetting env. episode 3425, reward total was 4.0. running mean: -5.358096089779181, timestamp: 2022-08-20 20:06:46.455069\n",
      "resetting env. episode 3426, reward total was -10.0. running mean: -5.404515128881389, timestamp: 2022-08-20 20:07:02.280771\n",
      "resetting env. episode 3427, reward total was -4.0. running mean: -5.390469977592575, timestamp: 2022-08-20 20:07:22.450853\n",
      "resetting env. episode 3428, reward total was -9.0. running mean: -5.42656527781665, timestamp: 2022-08-20 20:07:43.597325\n",
      "resetting env. episode 3429, reward total was -6.0. running mean: -5.432299625038483, timestamp: 2022-08-20 20:08:03.855195\n",
      "resetting env. episode 3430, reward total was 6.0. running mean: -5.317976628788098, timestamp: 2022-08-20 20:08:19.490382\n",
      "resetting env. episode 3431, reward total was -6.0. running mean: -5.324796862500216, timestamp: 2022-08-20 20:08:44.098619\n",
      "resetting env. episode 3432, reward total was -1.0. running mean: -5.281548893875214, timestamp: 2022-08-20 20:09:08.725778\n",
      "resetting env. episode 3433, reward total was -3.0. running mean: -5.258733404936462, timestamp: 2022-08-20 20:09:28.270535\n",
      "resetting env. episode 3434, reward total was -10.0. running mean: -5.306146070887096, timestamp: 2022-08-20 20:09:47.341556\n",
      "resetting env. episode 3435, reward total was 10.0. running mean: -5.153084610178226, timestamp: 2022-08-20 20:10:02.816216\n",
      "resetting env. episode 3436, reward total was -12.0. running mean: -5.221553764076444, timestamp: 2022-08-20 20:10:16.298158\n",
      "resetting env. episode 3437, reward total was -17.0. running mean: -5.33933822643568, timestamp: 2022-08-20 20:10:29.386174\n",
      "resetting env. episode 3438, reward total was -6.0. running mean: -5.345944844171322, timestamp: 2022-08-20 20:10:46.952218\n",
      "resetting env. episode 3439, reward total was -11.0. running mean: -5.4024853957296095, timestamp: 2022-08-20 20:11:00.748340\n",
      "resetting env. episode 3440, reward total was -9.0. running mean: -5.438460541772313, timestamp: 2022-08-20 20:11:19.492239\n",
      "resetting env. episode 3441, reward total was -1.0. running mean: -5.39407593635459, timestamp: 2022-08-20 20:11:38.565256\n",
      "resetting env. episode 3442, reward total was -5.0. running mean: -5.390135176991044, timestamp: 2022-08-20 20:11:57.513613\n",
      "resetting env. episode 3443, reward total was -3.0. running mean: -5.3662338252211335, timestamp: 2022-08-20 20:12:16.054057\n",
      "resetting env. episode 3444, reward total was -5.0. running mean: -5.362571486968922, timestamp: 2022-08-20 20:12:33.526345\n",
      "resetting env. episode 3445, reward total was -3.0. running mean: -5.338945772099233, timestamp: 2022-08-20 20:12:58.278185\n",
      "resetting env. episode 3446, reward total was -12.0. running mean: -5.405556314378241, timestamp: 2022-08-20 20:13:13.768777\n",
      "resetting env. episode 3447, reward total was -13.0. running mean: -5.481500751234458, timestamp: 2022-08-20 20:13:31.729776\n",
      "resetting env. episode 3448, reward total was -11.0. running mean: -5.5366857437221135, timestamp: 2022-08-20 20:13:48.519891\n",
      "resetting env. episode 3449, reward total was -4.0. running mean: -5.5213188862848925, timestamp: 2022-08-20 20:14:09.448975\n",
      "resetting env. episode 3450, reward total was -8.0. running mean: -5.5461056974220435, timestamp: 2022-08-20 20:14:27.320176\n",
      "resetting env. episode 3451, reward total was -5.0. running mean: -5.540644640447823, timestamp: 2022-08-20 20:14:47.576029\n",
      "resetting env. episode 3452, reward total was -7.0. running mean: -5.555238194043345, timestamp: 2022-08-20 20:15:04.431973\n",
      "resetting env. episode 3453, reward total was -11.0. running mean: -5.609685812102912, timestamp: 2022-08-20 20:15:35.795221\n",
      "resetting env. episode 3454, reward total was -11.0. running mean: -5.663588953981883, timestamp: 2022-08-20 20:15:56.492895\n",
      "resetting env. episode 3455, reward total was -3.0. running mean: -5.636953064442064, timestamp: 2022-08-20 20:16:15.169971\n",
      "resetting env. episode 3456, reward total was -7.0. running mean: -5.6505835337976436, timestamp: 2022-08-20 20:16:29.045884\n",
      "resetting env. episode 3457, reward total was -3.0. running mean: -5.624077698459668, timestamp: 2022-08-20 20:16:48.046093\n",
      "resetting env. episode 3458, reward total was 1.0. running mean: -5.557836921475071, timestamp: 2022-08-20 20:17:09.578568\n",
      "resetting env. episode 3459, reward total was 2.0. running mean: -5.482258552260321, timestamp: 2022-08-20 20:17:28.203752\n",
      "resetting env. episode 3460, reward total was -1.0. running mean: -5.437435966737717, timestamp: 2022-08-20 20:17:46.220636\n",
      "resetting env. episode 3461, reward total was 2.0. running mean: -5.363061607070341, timestamp: 2022-08-20 20:18:04.326196\n",
      "resetting env. episode 3462, reward total was -7.0. running mean: -5.379430990999637, timestamp: 2022-08-20 20:18:23.651544\n",
      "resetting env. episode 3463, reward total was -9.0. running mean: -5.415636681089641, timestamp: 2022-08-20 20:18:41.058012\n",
      "resetting env. episode 3464, reward total was 3.0. running mean: -5.331480314278744, timestamp: 2022-08-20 20:19:03.340454\n",
      "resetting env. episode 3465, reward total was -7.0. running mean: -5.348165511135957, timestamp: 2022-08-20 20:19:19.463354\n",
      "resetting env. episode 3466, reward total was -1.0. running mean: -5.304683856024598, timestamp: 2022-08-20 20:19:37.587935\n",
      "resetting env. episode 3467, reward total was -1.0. running mean: -5.261637017464351, timestamp: 2022-08-20 20:19:59.674898\n",
      "resetting env. episode 3468, reward total was 8.0. running mean: -5.129020647289708, timestamp: 2022-08-20 20:20:15.024839\n",
      "resetting env. episode 3469, reward total was -10.0. running mean: -5.17773044081681, timestamp: 2022-08-20 20:20:36.491609\n",
      "resetting env. episode 3470, reward total was -5.0. running mean: -5.1759531364086415, timestamp: 2022-08-20 20:20:59.722645\n",
      "resetting env. episode 3471, reward total was -7.0. running mean: -5.194193605044555, timestamp: 2022-08-20 20:21:33.630334\n",
      "resetting env. episode 3472, reward total was -2.0. running mean: -5.162251668994109, timestamp: 2022-08-20 20:21:53.184675\n",
      "resetting env. episode 3473, reward total was -5.0. running mean: -5.160629152304168, timestamp: 2022-08-20 20:22:17.296187\n",
      "resetting env. episode 3474, reward total was -14.0. running mean: -5.249022860781126, timestamp: 2022-08-20 20:22:28.947047\n",
      "resetting env. episode 3475, reward total was -11.0. running mean: -5.306532632173314, timestamp: 2022-08-20 20:22:44.749847\n",
      "resetting env. episode 3476, reward total was -10.0. running mean: -5.353467305851581, timestamp: 2022-08-20 20:23:04.200028\n",
      "resetting env. episode 3477, reward total was -5.0. running mean: -5.349932632793065, timestamp: 2022-08-20 20:23:25.335527\n",
      "resetting env. episode 3478, reward total was -7.0. running mean: -5.366433306465135, timestamp: 2022-08-20 20:23:50.383569\n",
      "resetting env. episode 3479, reward total was -2.0. running mean: -5.332768973400483, timestamp: 2022-08-20 20:24:19.872759\n",
      "resetting env. episode 3480, reward total was -1.0. running mean: -5.289441283666478, timestamp: 2022-08-20 20:24:54.459353\n",
      "resetting env. episode 3481, reward total was -7.0. running mean: -5.306546870829814, timestamp: 2022-08-20 20:25:24.414290\n",
      "resetting env. episode 3482, reward total was 2.0. running mean: -5.233481402121516, timestamp: 2022-08-20 20:25:47.765163\n",
      "resetting env. episode 3483, reward total was 4.0. running mean: -5.141146588100301, timestamp: 2022-08-20 20:26:14.651300\n",
      "resetting env. episode 3484, reward total was -11.0. running mean: -5.199735122219298, timestamp: 2022-08-20 20:26:29.419823\n",
      "resetting env. episode 3485, reward total was -13.0. running mean: -5.277737770997105, timestamp: 2022-08-20 20:26:58.590849\n",
      "resetting env. episode 3486, reward total was 2.0. running mean: -5.204960393287135, timestamp: 2022-08-20 20:27:19.769236\n",
      "resetting env. episode 3487, reward total was -7.0. running mean: -5.222910789354264, timestamp: 2022-08-20 20:27:39.908424\n",
      "resetting env. episode 3488, reward total was -3.0. running mean: -5.200681681460722, timestamp: 2022-08-20 20:28:05.381663\n",
      "resetting env. episode 3489, reward total was -11.0. running mean: -5.258674864646115, timestamp: 2022-08-20 20:28:23.256881\n",
      "resetting env. episode 3490, reward total was -7.0. running mean: -5.276088115999654, timestamp: 2022-08-20 20:28:42.929296\n",
      "resetting env. episode 3491, reward total was -13.0. running mean: -5.353327234839657, timestamp: 2022-08-20 20:28:57.247040\n",
      "resetting env. episode 3492, reward total was -13.0. running mean: -5.42979396249126, timestamp: 2022-08-20 20:29:16.930532\n",
      "resetting env. episode 3493, reward total was 4.0. running mean: -5.335496022866347, timestamp: 2022-08-20 20:29:42.997369\n",
      "resetting env. episode 3494, reward total was -5.0. running mean: -5.3321410626376835, timestamp: 2022-08-20 20:30:07.666426\n",
      "resetting env. episode 3495, reward total was -9.0. running mean: -5.3688196520113065, timestamp: 2022-08-20 20:30:24.566279\n",
      "resetting env. episode 3496, reward total was 1.0. running mean: -5.3051314554911935, timestamp: 2022-08-20 20:30:44.671511\n",
      "resetting env. episode 3497, reward total was -6.0. running mean: -5.312080140936281, timestamp: 2022-08-20 20:31:02.653475\n",
      "resetting env. episode 3498, reward total was -1.0. running mean: -5.268959339526918, timestamp: 2022-08-20 20:31:27.696506\n",
      "resetting env. episode 3499, reward total was -11.0. running mean: -5.32626974613165, timestamp: 2022-08-20 20:31:43.017562\n",
      "resetting env. episode 3500, reward total was -9.0. running mean: -5.363007048670333, timestamp: 2022-08-20 20:31:58.707617\n",
      "resetting env. episode 3501, reward total was 7.0. running mean: -5.239376978183629, timestamp: 2022-08-20 20:32:13.085185\n",
      "resetting env. episode 3502, reward total was 1.0. running mean: -5.1769832084017935, timestamp: 2022-08-20 20:32:34.819087\n",
      "resetting env. episode 3503, reward total was 4.0. running mean: -5.085213376317776, timestamp: 2022-08-20 20:32:50.158086\n",
      "resetting env. episode 3504, reward total was -15.0. running mean: -5.184361242554598, timestamp: 2022-08-20 20:33:05.456195\n",
      "resetting env. episode 3505, reward total was -11.0. running mean: -5.242517630129052, timestamp: 2022-08-20 20:33:20.900913\n",
      "resetting env. episode 3506, reward total was -11.0. running mean: -5.300092453827762, timestamp: 2022-08-20 20:33:36.480267\n",
      "resetting env. episode 3507, reward total was -15.0. running mean: -5.397091529289485, timestamp: 2022-08-20 20:33:49.668019\n",
      "resetting env. episode 3508, reward total was 3.0. running mean: -5.313120613996589, timestamp: 2022-08-20 20:34:08.328143\n",
      "resetting env. episode 3509, reward total was -9.0. running mean: -5.349989407856623, timestamp: 2022-08-20 20:34:27.613590\n",
      "resetting env. episode 3510, reward total was -5.0. running mean: -5.346489513778057, timestamp: 2022-08-20 20:34:51.751068\n",
      "resetting env. episode 3511, reward total was -3.0. running mean: -5.323024618640276, timestamp: 2022-08-20 20:35:05.914220\n",
      "resetting env. episode 3512, reward total was 4.0. running mean: -5.229794372453873, timestamp: 2022-08-20 20:35:22.393164\n",
      "resetting env. episode 3513, reward total was -2.0. running mean: -5.197496428729334, timestamp: 2022-08-20 20:35:42.991105\n",
      "resetting env. episode 3514, reward total was -2.0. running mean: -5.16552146444204, timestamp: 2022-08-20 20:36:03.861317\n",
      "resetting env. episode 3515, reward total was 10.0. running mean: -5.01386624979762, timestamp: 2022-08-20 20:36:16.925395\n",
      "resetting env. episode 3516, reward total was -12.0. running mean: -5.083727587299644, timestamp: 2022-08-20 20:36:33.515067\n",
      "resetting env. episode 3517, reward total was -1.0. running mean: -5.042890311426647, timestamp: 2022-08-20 20:36:53.078758\n",
      "resetting env. episode 3518, reward total was -4.0. running mean: -5.032461408312381, timestamp: 2022-08-20 20:37:11.878509\n",
      "resetting env. episode 3519, reward total was -11.0. running mean: -5.0921367942292575, timestamp: 2022-08-20 20:37:27.991436\n",
      "resetting env. episode 3520, reward total was -14.0. running mean: -5.181215426286965, timestamp: 2022-08-20 20:37:40.583781\n",
      "resetting env. episode 3521, reward total was -11.0. running mean: -5.239403272024095, timestamp: 2022-08-20 20:37:55.546784\n",
      "resetting env. episode 3522, reward total was 5.0. running mean: -5.137009239303854, timestamp: 2022-08-20 20:38:11.189967\n",
      "resetting env. episode 3523, reward total was 2.0. running mean: -5.065639146910816, timestamp: 2022-08-20 20:38:32.553862\n",
      "resetting env. episode 3524, reward total was -11.0. running mean: -5.1249827554417084, timestamp: 2022-08-20 20:38:45.447425\n",
      "resetting env. episode 3525, reward total was -7.0. running mean: -5.143732927887291, timestamp: 2022-08-20 20:39:04.759776\n",
      "resetting env. episode 3526, reward total was -14.0. running mean: -5.2322955986084185, timestamp: 2022-08-20 20:39:19.095456\n",
      "resetting env. episode 3527, reward total was -2.0. running mean: -5.199972642622334, timestamp: 2022-08-20 20:39:36.093022\n",
      "resetting env. episode 3528, reward total was -1.0. running mean: -5.15797291619611, timestamp: 2022-08-20 20:39:58.594191\n",
      "resetting env. episode 3529, reward total was -6.0. running mean: -5.166393187034148, timestamp: 2022-08-20 20:40:18.746324\n",
      "resetting env. episode 3530, reward total was -1.0. running mean: -5.124729255163807, timestamp: 2022-08-20 20:40:35.037785\n",
      "resetting env. episode 3531, reward total was -9.0. running mean: -5.163481962612169, timestamp: 2022-08-20 20:40:50.492467\n",
      "resetting env. episode 3532, reward total was 7.0. running mean: -5.041847142986047, timestamp: 2022-08-20 20:41:07.571814\n",
      "resetting env. episode 3533, reward total was -6.0. running mean: -5.051428671556186, timestamp: 2022-08-20 20:41:24.945379\n",
      "resetting env. episode 3534, reward total was -11.0. running mean: -5.110914384840624, timestamp: 2022-08-20 20:41:42.090559\n",
      "resetting env. episode 3535, reward total was -6.0. running mean: -5.119805240992218, timestamp: 2022-08-20 20:41:55.594462\n",
      "resetting env. episode 3536, reward total was -4.0. running mean: -5.108607188582296, timestamp: 2022-08-20 20:42:15.393527\n",
      "resetting env. episode 3537, reward total was -14.0. running mean: -5.197521116696472, timestamp: 2022-08-20 20:42:26.213608\n",
      "resetting env. episode 3538, reward total was -16.0. running mean: -5.305545905529508, timestamp: 2022-08-20 20:42:37.060617\n",
      "resetting env. episode 3539, reward total was -1.0. running mean: -5.262490446474213, timestamp: 2022-08-20 20:43:00.567776\n",
      "resetting env. episode 3540, reward total was 3.0. running mean: -5.179865542009471, timestamp: 2022-08-20 20:43:18.010152\n",
      "resetting env. episode 3541, reward total was -7.0. running mean: -5.198066886589376, timestamp: 2022-08-20 20:43:35.267026\n",
      "resetting env. episode 3542, reward total was 4.0. running mean: -5.106086217723482, timestamp: 2022-08-20 20:43:52.009274\n",
      "resetting env. episode 3543, reward total was -10.0. running mean: -5.155025355546247, timestamp: 2022-08-20 20:44:13.613525\n",
      "resetting env. episode 3544, reward total was -8.0. running mean: -5.183475101990784, timestamp: 2022-08-20 20:44:32.577268\n",
      "resetting env. episode 3545, reward total was -4.0. running mean: -5.171640350970876, timestamp: 2022-08-20 20:45:02.208904\n",
      "resetting env. episode 3546, reward total was -1.0. running mean: -5.129923947461167, timestamp: 2022-08-20 20:45:29.700542\n",
      "resetting env. episode 3547, reward total was -7.0. running mean: -5.1486247079865555, timestamp: 2022-08-20 20:45:59.416274\n",
      "resetting env. episode 3548, reward total was 7.0. running mean: -5.02713846090669, timestamp: 2022-08-20 20:46:25.768424\n",
      "resetting env. episode 3549, reward total was -5.0. running mean: -5.026867076297623, timestamp: 2022-08-20 20:47:04.929893\n",
      "resetting env. episode 3550, reward total was -3.0. running mean: -5.006598405534646, timestamp: 2022-08-20 20:47:40.013266\n",
      "resetting env. episode 3551, reward total was 5.0. running mean: -4.9065324214793, timestamp: 2022-08-20 20:48:14.166406\n",
      "resetting env. episode 3552, reward total was -12.0. running mean: -4.9774670972645065, timestamp: 2022-08-20 20:48:38.918079\n",
      "resetting env. episode 3553, reward total was 1.0. running mean: -4.917692426291862, timestamp: 2022-08-20 20:49:07.610499\n",
      "resetting env. episode 3554, reward total was -1.0. running mean: -4.878515502028943, timestamp: 2022-08-20 20:49:29.337337\n",
      "resetting env. episode 3555, reward total was -5.0. running mean: -4.879730347008653, timestamp: 2022-08-20 20:49:56.375413\n",
      "resetting env. episode 3556, reward total was -8.0. running mean: -4.9109330435385665, timestamp: 2022-08-20 20:50:10.558224\n",
      "resetting env. episode 3557, reward total was 3.0. running mean: -4.831823713103181, timestamp: 2022-08-20 20:50:28.189180\n",
      "resetting env. episode 3558, reward total was -9.0. running mean: -4.873505475972149, timestamp: 2022-08-20 20:50:43.187065\n",
      "resetting env. episode 3559, reward total was -9.0. running mean: -4.914770421212427, timestamp: 2022-08-20 20:50:58.107114\n",
      "resetting env. episode 3560, reward total was -11.0. running mean: -4.975622717000302, timestamp: 2022-08-20 20:51:11.659769\n",
      "resetting env. episode 3561, reward total was -9.0. running mean: -5.015866489830299, timestamp: 2022-08-20 20:51:27.670970\n",
      "resetting env. episode 3562, reward total was -7.0. running mean: -5.035707824931996, timestamp: 2022-08-20 20:51:42.285904\n",
      "resetting env. episode 3563, reward total was 8.0. running mean: -4.905350746682676, timestamp: 2022-08-20 20:51:57.509214\n",
      "resetting env. episode 3564, reward total was -12.0. running mean: -4.9762972392158495, timestamp: 2022-08-20 20:52:14.065958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__expired_functions__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'float'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27432\\2029763627.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'hist1_last_1e3.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'memit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hist1,hist_2 = train_model(env, model, total_episodes=8000)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhist1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'% s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2349\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2350\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2351\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2352\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\decorator.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\memory_profiler.py\u001b[0m in \u001b[0;36mmemit\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m             tmp = memory_usage((_func_exec, (stmt, self.shell.user_ns)),\n\u001b[0m\u001b[0;32m   1112\u001b[0m                                \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m                                \u001b[0mmax_usage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[1;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mreturned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# finish timing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\memory_profiler.py\u001b[0m in \u001b[0;36m_func_exec\u001b[1;34m(stmt, ns)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;31m# helper for magic_memit, just a function proxy for the exec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[1;31m# statement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27432\\2417372194.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(env, model, total_episodes)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# preprocess the observation, set input to network to be difference image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mcur_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepro\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_x\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprev_x\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprev_x\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mprev_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27432\\1818632268.py\u001b[0m in \u001b[0;36mprepro\u001b[1;34m(I)\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m109\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m# erase background (background type 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mI\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# everything else (paddles, ball) just set to 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpolicy_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;31m# that always raises an exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__expired_functions__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_name = 'hist1_last_1e3.csv'\n",
    "%memit hist1,hist_2 = train_model(env, model, total_episodes=8000)\n",
    "np.savetxt(file_name, hist1, delimiter =\",\", fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27432\\1247669137.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mplot_graph2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'hist_2' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "def plot_graph2(data):\n",
    "    for line in data:\n",
    "        xs.append(float(line[0]))\n",
    "        ys.append(float(line[1]))\n",
    "        ax1.clear()\n",
    "        \n",
    "    ax1.plot(xs, ys)\n",
    "    plt.pause(1)\n",
    "    plt.show()\n",
    "\n",
    "plot_graph2(hist_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27432\\3261209029.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mplot_graph2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'hist_2' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('hist1_8000_1e3.csv', delimiter=',')\n",
    "\n",
    "data = [list(row) for row in df.values]\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "def plot_graph2(data):\n",
    "    for line in data:\n",
    "        xs.append(float(line[0]))\n",
    "        ys.append(float(line[2]))\n",
    "        ax1.clear()\n",
    "        \n",
    "    ax1.plot(xs, ys)\n",
    "    plt.pause(1)\n",
    "    plt.show()\n",
    "\n",
    "plot_graph2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHYCDYwhlVLV",
    "outputId": "44023870-3ed0-46f5-84e7-df559e94ffc0"
   },
   "outputs": [],
   "source": [
    "#%time hist2 = train_model(env, model, total_episodes=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "8fheN9DRlWXQ",
    "outputId": "23490ad9-3ac8-4899-824b-44400caa8afd"
   },
   "outputs": [],
   "source": [
    "#play_game(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AxOcQhIsKow",
    "outputId": "68d93a54-196d-4d91-a6f6-731aa0de23c4"
   },
   "outputs": [],
   "source": [
    "#%time hist3 = train_model(env, model, total_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "w2NblmwDsL3y",
    "outputId": "f3dc32bf-ab84-418d-b830-407510690d12"
   },
   "outputs": [],
   "source": [
    "#play_game(env, model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "H=200 le-4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "83a8a37af132a2fa5bd73ffbd7034c1e5f9f9b0bef7ee17d2a911228df5d8f19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
